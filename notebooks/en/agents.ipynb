{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venezianof/booksum/blob/main/notebooks/en/agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ9M2l_AnrKy"
      },
      "source": [
        "# Build an agent with tool-calling superpowers ğŸ¦¸ using smolagents\n",
        "_Authored by: [Aymeric Roucher](https://huggingface.co/m-ric)_\n",
        "\n",
        "This notebook demonstrates how you can use [**smolagents**](https://huggingface.co/docs/smolagents/index) to build awesome **agents**!\n",
        "\n",
        "What are **agents**? Agents are systems that are powered by an LLM and enable the LLM (with careful prompting and output parsing) to use specific *tools* to solve problems.\n",
        "\n",
        "These *tools* are basically functions that the LLM couldn't perform well by itself: for instance for a text-generation LLM like [Llama-3-70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct), this could be an image generation tool, a web search tool, a calculator...\n",
        "\n",
        "What is **smolagents**? It's an library that provides building blocks to build your own agents! Learn more about it in the [documentation](https://huggingface.co/docs/smolagents/index).\n",
        "\n",
        "Let's see how to use it, and which use cases it can solve.\n",
        "\n",
        "Run the line below to install required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Yl_P9wTXnrK1"
      },
      "outputs": [],
      "source": [
        "!pip install smolagents datasets langchain sentence-transformers faiss-cpu duckduckgo-search openai langchain-community --upgrade -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kww1RLXKnrK4"
      },
      "source": [
        "Let's login in order to call the HF Inference API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387,
          "referenced_widgets": [
            "ea832e7f1f2e4066997ac07f8faf1317",
            "6a1f328452064359b1cde05151770505",
            "73c712ece49847b6ada2839027b157f3",
            "39c4950ef6de48bc930e2eb9f680ed1f",
            "7146d399e0804fd8a637a50d9d8c877e",
            "a153bdd3e9bf4e1388e25dfb8f79d9fc",
            "156ce781ae4e41cfa5551ba989a1fe3a",
            "59dd85f100574e089a2deac4fac4cbfc",
            "a123b071d1df4d1489774a03519d296e",
            "036526c7fcf540f7988d986578e62549",
            "5d98ba5cfaa84fe896e5d95c8e934e56",
            "fc18bfd606454f09a5aa2f54db8f7e4f",
            "db238b996dbb4b81847190769ba53da4",
            "d3a657fde1b3429d87568ad78bb682af",
            "2a32b53b81c640699fba9b2779790ac6",
            "1005914d7b2f4a9eae5955b9f538e8d8",
            "2bd27454561f4df0a7a0608f51fe8fe2"
          ]
        },
        "id": "NlO3tLDPnrK5",
        "outputId": "31bf880f-538e-441e-a095-55758108b4f8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea832e7f1f2e4066997ac07f8faf1317"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-dMDUgpnrK6"
      },
      "source": [
        "## 1. ğŸï¸ Multimodal + ğŸŒ Web-browsing assistant\n",
        "\n",
        "For this use case, we want to show an agent that browses the web and is able to generate images.\n",
        "\n",
        "To build it, we simply need to have two tools ready: image generation and web search.\n",
        "- For image generation, we load a tool from the Hub that uses the HF Inference API (Serverless) to generate images using Stable Diffusion.\n",
        "- For the web search, we use a built-in tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "MZauzUI3nrK7",
        "outputId": "fddb25f4-2074-44c9-8df7-57aebc6fcdcc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2mâ•­â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•®\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mGenerate me a photo of the car that James bond drove in the latest movie.\u001b[0m                                       \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ•°â”€\u001b[0m\u001b[38;2;212;183;2m InferenceClientModel - google/gemma-7b-it \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">Generate me a photo of the car that James bond drove in the latest movie.</span>                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•°â”€ InferenceClientModel - google/gemma-7b-it â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1;37mStep 1\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mError in generating model output:\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating model output:</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 0.01 seconds]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 0.01 seconds]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AgentGenerationError",
          "evalue": "Error in generating model output:\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_step_stream\u001b[0;34m(self, memory_step)\u001b[0m\n\u001b[1;32m   1653\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m                 chat_message: ChatMessage = self.model.generate(\n\u001b[0m\u001b[1;32m   1655\u001b[0m                     \u001b[0minput_messages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/smolagents/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop_sequences, response_format, tools_to_call_from, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_rate_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretryer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_completion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcompletion_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1539\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/smolagents/utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0;31m# Get the provider helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m         provider_helper = get_provider_helper(\n\u001b[0m\u001b[1;32m    879\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/inference/_providers/__init__.py\u001b[0m in \u001b[0;36mget_provider_helper\u001b[0;34m(provider, task, model)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mprovider_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fetch_inference_provider_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mprovider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprovider_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAgentGenerationError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2370795474.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Run it!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m result = agent.run(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"Generate me a photo of the car that James bond drove in the latest movie.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, task, stream, reset, images, additional_args, max_steps, return_full_result)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0mrun_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# Outputs are returned only at the end. We only look at the last step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_run_stream\u001b[0;34m(self, task, max_steps, images)\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAgentGenerationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0;31m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAgentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;31m# Other AgentError types are caused by the Model, so we should log them and iterate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_run_stream\u001b[0;34m(self, task, max_steps, images)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Step {self.step_number}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogLevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m                     \u001b[0;31m# Yield all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_step_stream\u001b[0;34m(self, memory_step)\u001b[0m\n\u001b[1;32m   1675\u001b[0m             \u001b[0mmemory_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAgentGenerationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error in generating model output:\\n{e}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1679\u001b[0m         \u001b[0;31m### Parse output ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAgentGenerationError\u001b[0m: Error in generating model output:\n"
          ]
        }
      ],
      "source": [
        "from smolagents import load_tool, CodeAgent, InferenceClientModel, DuckDuckGoSearchTool\n",
        "\n",
        "# Import tool from Hub\n",
        "image_generation_tool = load_tool(\"m-ric/text-to-image\", trust_remote_code=True)\n",
        "\n",
        "\n",
        "search_tool = DuckDuckGoSearchTool()\n",
        "\n",
        "# Using google/gemma-7b-it again, as the login might have been the issue\n",
        "model = InferenceClientModel(\"google/gemma-7b-it\")\n",
        "# Initialize the agent with both tools\n",
        "agent = CodeAgent(\n",
        "    tools=[image_generation_tool, search_tool], model=model\n",
        ")\n",
        "\n",
        "# Run it!\n",
        "result = agent.run(\n",
        "    \"Generate me a photo of the car that James bond drove in the latest movie.\",\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76bd4454"
      },
      "source": [
        "!pip install ddgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrkogaMynrK-"
      },
      "source": [
        "![Image of an Aston Martin DB5](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/agents_db5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGAcSvDRnrLA"
      },
      "source": [
        "## 2. ğŸ“šğŸ’¬ RAG with Iterative query refinement & Source selection\n",
        "\n",
        "Quick definition: Retrieval-Augmented-Generation (RAG) is ___â€œusing an LLM to answer a user query, but basing the answer on information retrieved from a knowledge baseâ€.___\n",
        "\n",
        "This method has many advantages over using a vanilla or fine-tuned LLM: to name a few, it allows to ground the answer on true facts and reduce confabulations, it allows to provide the LLM with domain-specific knowledge, and it allows fine-grained control of access to information from the knowledge base.\n",
        "\n",
        "- Now letâ€™s say we want to perform RAG, but with the additional constraint that some parameters must be dynamically generated. For example, depending on the user query we could want to restrict the search to specific subsets of the knowledge base, or we could want to adjust the number of documents retrieved. The difficulty is: **how to dynamically adjust these parameters based on the user query?**\n",
        "\n",
        "- A frequent failure case of RAG is when the retrieval based on the user query does not return any relevant supporting documents. **Is there a way to iterate by re-calling the retriever with a modified query in case the previous results were not relevant?**\n",
        "\n",
        "\n",
        "ğŸ”§ Well, we can solve the points above in a simple way: we will **give our agent control over the retriever's parameters!**\n",
        "\n",
        "â¡ï¸ Let's show how to do this. We first load a knowledge base on which we want to perform RAG: this dataset is a compilation of the documentation pages for many `huggingface` packages, stored as markdown.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y65MomB8nrLB"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95VwM42WnrLD"
      },
      "source": [
        "Now we prepare the knowledge base by processing the dataset and storing it into a vector database to be used by the retriever. We are going to use LangChain, since it features excellent utilities for vector databases:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3aaDPg7nrLE"
      },
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "source_docs = [\n",
        "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
        "    for doc in knowledge_base\n",
        "]\n",
        "\n",
        "docs_processed = RecursiveCharacterTextSplitter(chunk_size=500).split_documents(\n",
        "    source_docs\n",
        ")[:1000]\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
        "vectordb = FAISS.from_documents(documents=docs_processed, embedding=embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KFWp2lsnrLG"
      },
      "source": [
        "Now that we have the database ready, letâ€™s build a RAG system that answers user queries based on it!\n",
        "\n",
        "We want our system to select only from the most relevant sources of information, depending on the query.\n",
        "\n",
        "Our documentation pages come from the following sources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOcLhkoZnrLH"
      },
      "outputs": [],
      "source": [
        "all_sources = list(set([doc.metadata[\"source\"] for doc in docs_processed]))\n",
        "print(all_sources)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9-rxGLjnrLI"
      },
      "source": [
        "ğŸ‘‰ Now let's build a `RetrieverTool` that our agent can leverage to retrieve information from the knowledge base.\n",
        "\n",
        "Since we need to add a vectordb as an attribute of the tool, we cannot simply use the [simple tool constructor](https://huggingface.co/docs/transformers/main/en/agents#create-a-new-tool) with a `@tool` decorator: so we will follow the advanced setup highlighted in the [advanced agents documentation](https://huggingface.co/docs/transformers/main/en/agents_advanced#directly-define-a-tool-by-subclassing-tool-and-share-it-to-the-hub)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZRCm2gqnrLI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from smolagents import Tool\n",
        "from langchain_core.vectorstores import VectorStore\n",
        "\n",
        "\n",
        "class RetrieverTool(Tool):\n",
        "    name = \"retriever\"\n",
        "    description = \"Retrieves some documents from the knowledge base that have the closest embeddings to the input query.\"\n",
        "    inputs = {\n",
        "        \"query\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
        "        },\n",
        "        \"source\": {\"type\": \"string\", \"description\": \"\"},\n",
        "        \"number_of_documents\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"the number of documents to retrieve. Stay under 10 to avoid drowning in docs\",\n",
        "        },\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, vectordb: VectorStore, all_sources: str, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vectordb = vectordb\n",
        "        self.inputs[\"source\"][\"description\"] = (\n",
        "            f\"The source of the documents to search, as a str representation of a list. Possible values in the list are: {all_sources}. If this argument is not provided, all sources will be searched.\".replace(\n",
        "                \"'\", \"`\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, query: str, source: str = None, number_of_documents=7) -> str:\n",
        "        assert isinstance(query, str), \"Your search query must be a string\"\n",
        "        number_of_documents = int(number_of_documents)\n",
        "\n",
        "        if source:\n",
        "            if isinstance(source, str) and \"[\" not in str(\n",
        "                source\n",
        "            ):  # if the source is not representing a list\n",
        "                source = [source]\n",
        "            source = json.loads(str(source).replace(\"'\", '\"'))\n",
        "\n",
        "        docs = self.vectordb.similarity_search(\n",
        "            query,\n",
        "            filter=({\"source\": source} if source else None),\n",
        "            k=number_of_documents,\n",
        "        )\n",
        "\n",
        "        if len(docs) == 0:\n",
        "            return \"No documents found with this filtering. Try removing the source filter.\"\n",
        "        return \"Retrieved documents:\\n\\n\" + \"\\n===Document===\\n\".join(\n",
        "            [doc.page_content for doc in docs]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5Ey3yXenrLK"
      },
      "source": [
        "### Optional: Share your Retriever tool to Hub\n",
        "\n",
        "To share your tool to the Hub, first copy-paste the code in the RetrieverTool definition cell to a new file named for instance `retriever.py`.\n",
        "\n",
        "When the tool is loaded from a separate file, you can then push it to the Hub using the code below (make sure to login with a `write` access token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5Z2vr8TnrLL"
      },
      "outputs": [],
      "source": [
        "share_to_hub = True\n",
        "\n",
        "if share_to_hub:\n",
        "    from huggingface_hub import login\n",
        "    from retriever import RetrieverTool\n",
        "\n",
        "    login(\"your_token\")\n",
        "\n",
        "    tool = RetrieverTool(vectordb, all_sources)\n",
        "\n",
        "    tool.push_to_hub(repo_id=\"m-ric/retriever-tool\")\n",
        "\n",
        "    # Loading the tool\n",
        "    from smolagents import load_tool\n",
        "\n",
        "    retriever_tool = load_tool(\n",
        "        \"m-ric/retriever-tool\", vectordb=vectordb, all_sources=all_sources\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2No8mwNnrLL"
      },
      "source": [
        "### Run the agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6rRpmurnrLM"
      },
      "outputs": [],
      "source": [
        "from smolagents import InferenceClientModel, ToolCallingAgent\n",
        "\n",
        "model = InferenceClientModel(\"Qwen/Qwen2.5-72B-Instruct\")\n",
        "\n",
        "retriever_tool = RetrieverTool(vectordb=vectordb, all_sources=all_sources)\n",
        "agent = ToolCallingAgent(tools=[retriever_tool], model=model, verbose=0)\n",
        "\n",
        "agent_output = agent.run(\"Please show me a LORA finetuning script\")\n",
        "\n",
        "print(\"Final output:\")\n",
        "print(agent_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnTGYJZ6nrLM"
      },
      "source": [
        "What happened here? First, the agent launched the retriever with specific sources in mind (`['transformers', 'blog']`).\n",
        "\n",
        "But this retrieval did not yield enough results â‡’ no problem! The agent could iterate on previous results, so it just re-ran its retrieval with less restrictive search parameters.\n",
        "Thus the research was successful!\n",
        "\n",
        "Note that **using an LLM agent** that calls a retriever as a tool and can dynamically modify the query and other retrieval parameters **is a more general formulation of RAG**, which also covers many RAG improvement techniques like iterative query refinement.\n",
        "\n",
        "## 3. ğŸ’» Debug Python code\n",
        "Since the CodeAgent has a built-in Python code interpreter, we can use it to debug our faulty Python script!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhJx3Q9gnrLN"
      },
      "outputs": [],
      "source": [
        "from smolagents import CodeAgent\n",
        "\n",
        "agent = CodeAgent(tools=[], model=InferenceClientModel(\"Qwen/Qwen2.5-72B-Instruct\"))\n",
        "\n",
        "code = \"\"\"\n",
        "numbers=[0, 1, 2]\n",
        "\n",
        "for i in range(4):\n",
        "    print(numbers(i))\n",
        "\"\"\"\n",
        "\n",
        "final_answer = agent.run(\n",
        "    \"I have some code that creates a bug: please debug it, then run it to make sure it works and return the final code\",\n",
        "    additional_args=dict(code=code)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-a3NqhBnrLN"
      },
      "source": [
        "As you can see, the agent tried the given code, gets an error, analyses the error, corrects the code and returns it after veryfing that it works!\n",
        "\n",
        "And the final code is the corrected code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwxRaVHgnrLO"
      },
      "outputs": [],
      "source": [
        "print(final_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ANARs95nrLO"
      },
      "source": [
        "## â¡ï¸ Conclusion\n",
        "\n",
        "The use cases above should give you a glimpse into the possibilities of our Agents framework!\n",
        "\n",
        "For more advanced usage, read the [documentation](https://huggingface.co/docs/smolagents/index).\n",
        "\n",
        "All feedback is welcome, it will help us improve the framework! ğŸš€"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "test2",
      "language": "python",
      "name": "test2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ea832e7f1f2e4066997ac07f8faf1317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a1f328452064359b1cde05151770505",
              "IPY_MODEL_73c712ece49847b6ada2839027b157f3",
              "IPY_MODEL_39c4950ef6de48bc930e2eb9f680ed1f",
              "IPY_MODEL_7146d399e0804fd8a637a50d9d8c877e",
              "IPY_MODEL_a153bdd3e9bf4e1388e25dfb8f79d9fc"
            ],
            "layout": "IPY_MODEL_156ce781ae4e41cfa5551ba989a1fe3a"
          }
        },
        "6a1f328452064359b1cde05151770505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59dd85f100574e089a2deac4fac4cbfc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a123b071d1df4d1489774a03519d296e",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "73c712ece49847b6ada2839027b157f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_036526c7fcf540f7988d986578e62549",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5d98ba5cfaa84fe896e5d95c8e934e56",
            "value": ""
          }
        },
        "39c4950ef6de48bc930e2eb9f680ed1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_fc18bfd606454f09a5aa2f54db8f7e4f",
            "style": "IPY_MODEL_db238b996dbb4b81847190769ba53da4",
            "value": true
          }
        },
        "7146d399e0804fd8a637a50d9d8c877e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_d3a657fde1b3429d87568ad78bb682af",
            "style": "IPY_MODEL_2a32b53b81c640699fba9b2779790ac6",
            "tooltip": ""
          }
        },
        "a153bdd3e9bf4e1388e25dfb8f79d9fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1005914d7b2f4a9eae5955b9f538e8d8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2bd27454561f4df0a7a0608f51fe8fe2",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "156ce781ae4e41cfa5551ba989a1fe3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "59dd85f100574e089a2deac4fac4cbfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a123b071d1df4d1489774a03519d296e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "036526c7fcf540f7988d986578e62549": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d98ba5cfaa84fe896e5d95c8e934e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc18bfd606454f09a5aa2f54db8f7e4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db238b996dbb4b81847190769ba53da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3a657fde1b3429d87568ad78bb682af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a32b53b81c640699fba9b2779790ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "1005914d7b2f4a9eae5955b9f538e8d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bd27454561f4df0a7a0608f51fe8fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}