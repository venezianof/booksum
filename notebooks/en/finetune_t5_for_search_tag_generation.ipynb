{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venezianof/booksum/blob/main/notebooks/en/finetune_t5_for_search_tag_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPMM9V_78GFD"
      },
      "source": [
        "### üîñ GitHub Tag Generator with T5 + PEFT (LoRA)\n",
        "\n",
        "**_Authored by: [Zamal Babar](https://huggingface.co/zamal)_**\n",
        "\n",
        "In this notebook, we walk through a complete **end-to-end implementation** of a lightweight, fast, and open-source **GitHub tag generator** using **T5-small** fine-tuned on a custom dataset with **PEFT (LoRA)**. This tool can automatically generate relevant tags from a GitHub repository description or summary ‚Äî useful for improving discoverability and organizing repos more intelligently.\n",
        "\n",
        "---\n",
        "\n",
        "#### üí° Use Case\n",
        "\n",
        "Imagine you're building a tool that helps users explore GitHub repositories more effectively. Instead of relying on manually written or sometimes missing tags, we train a model that **automatically generates descriptive tags** for any GitHub project. This could help:\n",
        "\n",
        "- Improve search functionality  \n",
        "- Automatically tag new repos  \n",
        "- Build better filters for discovery  \n",
        "\n",
        "---\n",
        "\n",
        "#### üì¶ Dataset\n",
        "\n",
        "We use a dataset of GitHub project descriptions and their associated tags. Each training example contains:\n",
        "\n",
        "- `\"input\"`: A natural language description of a GitHub repository  \n",
        "- `\"target\"`: A comma-separated list of relevant tags  \n",
        "\n",
        "The dataset was initially loaded from a local `.jsonl` file, but is now also available on the Hugging Face Hub here:  \n",
        "‚û°Ô∏è [`zamal/github-meta-data`](https://huggingface.co/datasets/zamal/github-meta-data)\n",
        "\n",
        "---\n",
        "\n",
        "#### üß† Model Architecture\n",
        "\n",
        "We fine-tuned the [`T5-small`](https://huggingface.co/t5-small) model for this task ‚Äî a lightweight encoder-decoder transformer that's well-suited for text-to-text generation tasks.  \n",
        "To make fine-tuning faster and more efficient, we used the ü§ó `peft` library with **LoRA (Low-Rank Adaptation)** to update only a subset of model parameters.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚úÖ What This Notebook Covers\n",
        "\n",
        "This notebook includes:\n",
        "\n",
        "- ‚úÖ Loading and preprocessing a custom dataset  \n",
        "- ‚úÖ Setting up a T5-small model with LoRA  \n",
        "- ‚úÖ Training the model using the Hugging Face `Trainer`  \n",
        "- ‚úÖ Monitoring progress with **Weights & Biases**  \n",
        "- ‚úÖ Saving and pushing the model to the Hugging Face Hub  \n",
        "- ‚úÖ Performing inference and postprocessing for clean, deduplicated tags  \n",
        "\n",
        "---\n",
        "\n",
        "#### üîç Final Outcome\n",
        "\n",
        "By the end of this notebook, you‚Äôll have:\n",
        "\n",
        "- üöÄ A fully trained and hosted GitHub tag generator  \n",
        "- üîÅ A deployable and shareable model on Hugging Face Hub  \n",
        "- üß† An inference function to use your model anywhere with just a few lines of code  \n",
        "\n",
        "Let‚Äôs dive in! üéØ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJuMmRMA0dn7"
      },
      "source": [
        "We begin by:\n",
        "\n",
        "- Importing essential libraries for model training (`transformers`, `datasets`, `peft`)\n",
        "- Loading the T5 tokenizer\n",
        "- Setting the Hugging Face token (stored securely in Colab‚Äôs `userdata`)\n",
        "\n",
        "Make sure you've stored your `HUGGINGFACE_TOKEN` in your Colab's secrets before running this cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8d05a2e"
      },
      "source": [
        "import os\n",
        "\n",
        "hf_token = os.environ.get('HUGGINGFACE_TOKEN')\n",
        "if hf_token:\n",
        "    print(f\"Hugging Face Token loaded: {hf_token[:5]}...{hf_token[-5:]}\")\n",
        "else:\n",
        "    print(\"Hugging Face Token NOT loaded. Please check your Colab secrets.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MtPZJc1valkQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "113541cb-59ea-4308-b33d-f9d081078429"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret HUGGINGFACE_TOKEN does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3385685560.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HUGGINGFACE_TOKEN'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HUGGINGFACE_TOKEN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret HUGGINGFACE_TOKEN does not exist."
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['HUGGINGFACE_TOKEN'] = userdata.get('HUGGINGFACE_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b975781",
        "outputId": "95903333-d36d-4ae0-cafb-bf91c5cfc8bd"
      },
      "source": [
        "import os\n",
        "\n",
        "hf_token = os.environ.get('HUGGINGFACE_TOKEN')\n",
        "if hf_token:\n",
        "    print(f\"Hugging Face Token loaded successfully!\")\n",
        "    print(f\"First 5 chars: {hf_token[:5]}..., Last 5 chars: ...{hf_token[-5:]}\")\n",
        "else:\n",
        "    print(\"Hugging Face Token NOT loaded. Please ensure it's set in Colab secrets and the cell to load it has been executed.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face Token NOT loaded. Please ensure it's set in Colab secrets and the cell to load it has been executed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riGl54N7WcT0"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1aA3zapaiG9"
      },
      "outputs": [],
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEkZsjic03QT"
      },
      "source": [
        "#### üì¶ Load and Prepare the Dataset\n",
        "\n",
        "We now load our training data from a local JSONL file that contains repository descriptions and their corresponding tags.\n",
        "\n",
        "Each line in the file is a JSON object with two fields:\n",
        "- `input`: a short repository description\n",
        "- `target`: the tags (comma-separated)\n",
        "\n",
        "We split this dataset into training and validation sets using a 90/10 ratio.\n",
        "\n",
        "üîÅ _Note_: When this notebook was initially run, the dataset was loaded locally from a file. However, the same dataset is now also available on the Hugging Face Hub here: [zamal/github-meta-data](https://huggingface.co/datasets/zamal/github-meta-data). Feel free to load it directly using `load_dataset(\"zamal/github-meta-data\")` in your workflow as shown below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXKk3i7X1tjA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Load existing dataset with only a \"train\" split\n",
        "dataset = load_dataset(\"zamal/github-meta-data\")  # returns DatasetDict\n",
        "\n",
        "# Split the train set into train and validation\n",
        "split = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "# Wrap into a new DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    \"train\": split[\"train\"],\n",
        "    \"validation\": split[\"test\"]\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lhSaOh2aqaQ"
      },
      "outputs": [],
      "source": [
        "print(len(dataset_dict[\"train\"]))\n",
        "print(len(dataset_dict[\"validation\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZItTU0X095U"
      },
      "source": [
        "#### üî§ Load the Tokenizer\n",
        "\n",
        "We load the tokenizer associated with the `t5-small` model. T5 expects input and output text to be tokenized in a specific way, and this tokenizer ensures compatibility during training and inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AVpIq-Oatp5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoOUigZB1Rlc"
      },
      "source": [
        "#### üßπ Preprocessing the Dataset\n",
        "\n",
        "Next, we define a preprocessing function to tokenize both the inputs and the targets using the T5 tokenizer.\n",
        "- The inputs are padded and truncated to a maximum length of 128 tokens.\n",
        "- The target labels (i.e., tags) are also tokenized with a shorter maximum length of 64 tokens.\n",
        "\n",
        "We then map this preprocessing function across our training and validation datasets and format the output for PyTorch compatibility. This prepares the dataset for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLSUbEz5a1WZ"
      },
      "outputs": [],
      "source": [
        "def preprocess(batch):\n",
        "    inputs = batch[\"input\"]\n",
        "    targets = batch[\"target\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=64, truncation=True, padding=\"max_length\").input_ids\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl9B_UY4a3BO"
      },
      "outputs": [],
      "source": [
        "tokenized = dataset_dict.map(preprocess, batched=True, remove_columns=dataset_dict[\"train\"].column_names)\n",
        "tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff_7ZVe91iw1"
      },
      "source": [
        "#### Loading the Base T5 Model\n",
        "\n",
        "We load the base T5 model (`t5-small`) for conditional generation. This model serves as the backbone for our tag generation task, where the goal is to generate relevant tags given a description of a GitHub repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4z_zU91a5In"
      },
      "outputs": [],
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI9VIDxr1qz8"
      },
      "source": [
        "#### üîß Preparing the LoRA Configuration\n",
        "\n",
        "We configure LoRA (Low-Rank Adaptation) to fine-tune the T5 model efficiently. LoRA injects trainable low-rank matrices into attention layers, significantly reducing the number of trainable parameters while maintaining performance.\n",
        "\n",
        "In this setup:\n",
        "- `r=16` defines the rank of the update matrices.\n",
        "- `lora_alpha=32` scales the updates.\n",
        "- We apply LoRA to the `\"q\"` and `\"v\"` attention projection modules.\n",
        "- The task type is set to `\"SEQ_2_SEQ_LM\"` since we're working on a sequence-to-sequence task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5psto7La6dJ"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],  # Adjust based on model architecture\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yypZi1C11fs"
      },
      "source": [
        "#### üîå Injecting LoRA into the Base T5 Model\n",
        "\n",
        "Now that we've defined our LoRA configuration, we apply it to the base T5 model using `get_peft_model()`. This wraps the original model with the LoRA adapters, allowing us to fine-tune only a small number of parameters instead of the entire model‚Äîmaking training faster and more memory-efficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiUvcLqKa8OE"
      },
      "outputs": [],
      "source": [
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znvYN-rK2KTK"
      },
      "source": [
        "#### üõ†Ô∏è TrainingArguments Configuration\n",
        "\n",
        "We use the `TrainingArguments` class to define the hyperparameters and training behavior for our model. Here's a breakdown of each parameter:\n",
        "\n",
        "- **`output_dir=\"./t5_tag_generator\"`**  \n",
        "  Directory to save model checkpoints and training logs.\n",
        "\n",
        "- **`per_device_train_batch_size=8`**  \n",
        "  Number of training samples per GPU/TPU core (or CPU) in each training step.\n",
        "\n",
        "- **`per_device_eval_batch_size=8`**  \n",
        "  Number of evaluation samples per GPU/TPU core (or CPU) in each evaluation step.\n",
        "\n",
        "- **`learning_rate=1e-4`**  \n",
        "  Initial learning rate. A good starting point for T5 models with LoRA.\n",
        "\n",
        "- **`num_train_epochs=25`**  \n",
        "  Total number of training epochs. This is relatively high to ensure convergence for our use case.\n",
        "\n",
        "- **`logging_steps=10`**  \n",
        "  How often (in steps) to log training metrics to the console and W&B.\n",
        "\n",
        "- **`eval_strategy=\"steps\"`**  \n",
        "  Run evaluation every `eval_steps` instead of after every epoch.\n",
        "\n",
        "- **`eval_steps=50`**  \n",
        "  Evaluate the model every 50 steps to monitor progress during training.\n",
        "\n",
        "- **`save_steps=50`**  \n",
        "  Save model checkpoints every 50 steps for redundancy and safe restoration.\n",
        "\n",
        "- **`save_total_limit=2`**  \n",
        "  Keep only the 2 most recent model checkpoints to save disk space.\n",
        "\n",
        "- **`fp16=True`**  \n",
        "  Enable mixed precision training (faster and memory-efficient on supported GPUs).\n",
        "\n",
        "- **`push_to_hub=True`**  \n",
        "  Automatically push the trained model to the Hugging Face Hub.\n",
        "\n",
        "- **`hub_model_id=\"zamal/github-tag-generatorr\"`**  \n",
        "  The model repo name on Hugging Face under your username. This is where checkpoints and final model weights will be pushed.\n",
        "\n",
        "- **`hub_token=os.environ['HUGGINGFACE_TOKEN']`**  \n",
        "  Token to authenticate your Hugging Face account. We securely retrieve this from the environment.\n",
        "\n",
        "This setup ensures a balance between training efficiency, frequent monitoring, and safe saving of model progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyApAXU2a9s9"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./t5_tag_generator\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=25,\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"zamal/github-tag-generatorr\",  # Replace with your Hugging Face username\n",
        "    hub_token=os.environ['HUGGINGFACE_TOKEN']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl4o-tdj2nbU"
      },
      "source": [
        "#### üß† Initialize the Trainer\n",
        "\n",
        "We now configure the `Trainer`, which abstracts away the training loop, evaluation steps, logging, and saving. It handles all of it for us using the parameters we've defined earlier.\n",
        "\n",
        "We also pass in the `DataCollatorForSeq2Seq`, which ensures proper padding and batching during training and evaluation for sequence-to-sequence tasks like ours.\n",
        "\n",
        "#### ‚ö†Ô∏è Warnings Explained:\n",
        "\n",
        "- **`FutureWarning: 'tokenizer' is deprecated...`**  \n",
        "  As of Transformers v5.0.0, the `tokenizer` argument in `Trainer` is deprecated. Instead, Hugging Face recommends using the `processing_class`, which refers to a processor that combines tokenization and potentially feature extraction. For now, it's safe to ignore this, but it's good practice to track deprecations for future compatibility.\n",
        "\n",
        "- **`No label_names provided for model class 'PeftModelForSeq2SeqLM'`**  \n",
        "  This warning appears because we‚Äôre using a [PEFT (Parameter-Efficient Fine-Tuning)](https://huggingface.co/docs/peft) wrapped model (`PeftModelForSeq2SeqLM`), and the `Trainer` cannot automatically determine the label field names in this case.  \n",
        "  Since we're already formatting our dataset correctly (by explicitly setting `labels` during preprocessing), this warning can be safely ignored as well ‚Äî training will still proceed correctly.\n",
        "\n",
        "Now, we can initialize our `Trainer`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7IW2JTGbANC"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFpan36l3EuD"
      },
      "source": [
        "#### üöÄ Start Training the Tag Generator Model\n",
        "\n",
        "With everything set up ‚Äî the model, tokenizer, dataset, LoRA configuration, training arguments, and the `Trainer` ‚Äî we can now kick off the fine-tuning process by calling `trainer.train()`.\n",
        "\n",
        "This will:\n",
        "- Fine-tune our **T5 model** using the **parameter-efficient LoRA strategy**.\n",
        "- Save checkpoints at regular intervals (`save_steps=50`).\n",
        "- Evaluate on the validation set every 50 steps (`eval_steps=50`).\n",
        "- Log metrics like loss to **Weights & Biases** or the Hugging Face Hub if integrated.\n",
        "\n",
        "Training will take some time depending on the size of your dataset and GPU, but you‚Äôll start to see metrics printed out step-by-step, such as:\n",
        "\n",
        "- `Training Loss`: how well the model is fitting the training data.\n",
        "- `Validation Loss`: how well the model performs on unseen data.\n",
        "\n",
        "Let‚Äôs begin the fine-tuning! üëá\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlF3XEqkbB3O"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R59tgxzd3a_0"
      },
      "source": [
        "#### ‚úÖ Training Summary and Observations\n",
        "\n",
        "The training process successfully completed over **25 epochs**, using a LoRA-fine-tuned `T5-small` model to generate tags for GitHub repository descriptions. Here's a quick breakdown of what happened and how to interpret it:\n",
        "\n",
        "#### üîÑ Logging with Weights & Biases (W&B)\n",
        "We logged all training metrics and artifacts using [Weights & Biases](https://wandb.ai/), which offers a convenient UI to monitor model performance in real time. You can view the run at:\n",
        "üëâ [W&B Project Run](https://wandb.ai/zamalbabar9866-fau-erlangen-n-rnberg/huggingface/runs/3uv5wis6)\n",
        "\n",
        "#### üìâ Training & Validation Loss\n",
        "From the logs:\n",
        "- **Training loss** began at 8.9 (random init) and steadily declined to ~1.06.\n",
        "- **Validation loss** also dropped consistently from 7.9 to **0.95**, indicating good generalization and minimal overfitting.\n",
        "  \n",
        "The slight fluctuations (e.g., at steps 850, 1000, 1100) are normal and reflect natural variance in optimization, especially with small batch sizes.\n",
        "\n",
        "#### ‚öôÔ∏è Warnings and Notices\n",
        "- The warning about `past_key_values` being deprecated is safe to ignore for now and expected behavior with the current `transformers` version.\n",
        "- `UserWarning` about tensor creation can be optimized later, but doesn't affect the result.\n",
        "- The `run_name` warning suggests you can optionally decouple logging folder names from output directories.\n",
        "\n",
        "#### üìä Performance Metrics\n",
        "The model completed:\n",
        "- **1725 training steps**\n",
        "- ~**1.9 samples/sec** processing speed\n",
        "- Total training time: **~2 hours**\n",
        "\n",
        "This is solid performance given the setup and confirms that your LoRA fine-tuning pipeline is both stable and efficient.\n",
        "\n",
        "---\n",
        "\n",
        "Next, we‚Äôll save and push this trained model to the Hugging Face Hub so you (or others!) can load and test it anytime. üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avG841Uf37Gk"
      },
      "source": [
        "#### üîç Inference: Generate Tags from Repository Descriptions\n",
        "\n",
        "Now that the model is trained, we define a simple helper function `generate_tags` to run inference. It takes a natural language query describing a repository and generates relevant tags using our fine-tuned T5 model.\n",
        "\n",
        "Below is an example for a query related to image augmentation and no-code tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaKfnd__bN8w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def generate_tags(query, model, tokenizer, max_length=64, num_beams=5):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=max_length,\n",
        "            num_beams=num_beams,\n",
        "            early_stopping=True,\n",
        "            decoder_start_token_id=tokenizer.pad_token_id  # üëà required for T5\n",
        "        )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jURjMtQxbhuZ"
      },
      "outputs": [],
      "source": [
        "generate_tags(\"looking for repositories on image augmentation no code implementations\", model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e24wiEI24LEN"
      },
      "source": [
        "#### üíæ Save Fine-Tuned Model Locally\n",
        "\n",
        "Once the training is complete, we save the fine-tuned model and tokenizer to a local directory. This allows us to reuse or share the model later without needing to retrain it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Es_CcfMfDHL"
      },
      "outputs": [],
      "source": [
        "# Save model, tokenizer, and config to local output directory\n",
        "model_path = \"./t5_tag_generator/final\"\n",
        "\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n",
        "print(\"‚úÖ Model and tokenizer saved locally at:\", model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeI6K7dL4aEl"
      },
      "source": [
        "#### üöÄ Push Model to Hugging Face Hub\n",
        "\n",
        "After saving the model locally, we now push it to the Hugging Face Hub so that others can easily access, test, and load it using `from_pretrained`.\n",
        "\n",
        "‚û°Ô∏è The model is publicly available at: [huggingface.co/zamal/github-tag-generatorr](https://huggingface.co/zamal/github-tag-generatorr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ihk0UoDfDnp"
      },
      "outputs": [],
      "source": [
        "# Push to Hugging Face Hub under your repo\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=model_path,\n",
        "    repo_id=\"zamal/github-tag-generatorr\",  # Your model ID\n",
        "    repo_type=\"model\",\n",
        "    path_in_repo=\"\",  # Root of the repo\n",
        ")\n",
        "\n",
        "print(\"üöÄ Model pushed to Hugging Face Hub: https://huggingface.co/zamal/github-tag-generatorr\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHWzCGlZzCN9"
      },
      "source": [
        "#### üì¶ Load Model Directly from Hugging Face Hub\n",
        "\n",
        "Now that we've pushed our fine-tuned model to the Hugging Face Hub, we can easily load it from anywhere using the `pipeline` utility. This allows us to instantly test or integrate the model into other applications without needing local files.\n",
        "\n",
        "The model is hosted at: [zamal/github-tag-generatorr](https://huggingface.co/zamal/github-tag-generatorr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XlotqmEfHq8"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the model and tokenizer from Hugging Face Hub\n",
        "tag_generator = pipeline(\"text2text-generation\", model=\"zamal/github-tag-generatorr\", tokenizer=\"zamal/github-tag-generatorr\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAkJntiD47i1"
      },
      "source": [
        "### üß† Inference Function with Post-Processing\n",
        "\n",
        "This function wraps the model inference process to generate tags for a given GitHub project description. We prepend the prefix `\"generate tags: \"` (which the model was trained on) and tokenize the input appropriately before calling `model.generate()`.\n",
        "\n",
        "After decoding the generated output, we **deduplicate the tags** using a simple `dict.fromkeys()` trick. This ensures that tags like `\"pytorch, pytorch, pytorch\"` only appear once.\n",
        "\n",
        "We added this logic because the training data included some noisy samples with repeated or inconsistent tags. Since we did not perform extensive data cleaning or multiple training runs to refine the quality, this lightweight fix helps improve the final output. In a production-grade system, we‚Äôd recommend:\n",
        "\n",
        "- more rigorous data preprocessing,\n",
        "- filtering weak labels,\n",
        "- and performing iterative fine-tuning with evaluation and human-in-the-loop review.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnYzrCZ26tWm"
      },
      "outputs": [],
      "source": [
        "def generate_tags(text, model, tokenizer, max_length=64, num_beams=5):\n",
        "    input_text = text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=max_length,\n",
        "            num_beams=num_beams,\n",
        "            early_stopping=True,\n",
        "            decoder_start_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Deduplicate and clean tags\n",
        "    tags = [t.strip().lower() for t in decoded.split(\",\")]\n",
        "    unique_tags = list(dict.fromkeys(tags))  # preserve order + remove duplicates\n",
        "    return \", \".join(unique_tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfHpD2QN5OpL"
      },
      "source": [
        "##### üîç Real-world Examples: Testing on Sample Inputs\n",
        "\n",
        "Now that we've defined our inference function and loaded the model, let's run it on a few example descriptions.\n",
        "\n",
        "Each input represents a short summary of a hypothetical GitHub repository. Our goal is to generate meaningful and concise tags using the fine-tuned T5 model.\n",
        "\n",
        "These test cases demonstrate how well the model generalizes to realistic prompts ‚Äî and thanks to our post-processing, any repetitive or noisy tags are cleaned up before display.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvBSuiot6vYx"
      },
      "outputs": [],
      "source": [
        "inputs = [\n",
        "    \"Need an AI tool to convert customer voice calls into structured CRM record\",\n",
        "    \"How to train a text summarization model using Pegasus or BART\",\n",
        "    \"Fine-tuning BERT for spam detection in emails\"\n",
        "]\n",
        "\n",
        "for text in inputs:\n",
        "    print(f\"üì• Input: {text}\")\n",
        "    print(f\"üè∑Ô∏è Tags: {generate_tags(text, model, tokenizer)}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF1VJ2T_b7eM"
      },
      "source": [
        "##### üîç Inference Examples Using Real Hugging Face Projects\n",
        "\n",
        "To follow the same format as our training data, we rephrase descriptive statements into query-style inputs ‚Äî just like users would naturally search for repositories. This aligns with our fine-tuning data, which was based on natural language search queries mapped to relevant tags.\n",
        "\n",
        "Below are some meta and practical examples, including:\n",
        "- Hugging Face‚Äôs own popular repositories (e.g., Transformers, Datasets, Diffusers)\n",
        "- Styled as realistic queries for better inference consistency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUrLuZHDaLSQ"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Load the model and tokenizer from the Hugging Face Hub\n",
        "tag_generator = pipeline(\"text2text-generation\", model=\"zamal/github-tag-generatorr\", tokenizer=\"zamal/github-tag-generatorr\")\n",
        "\n",
        "def clean_and_deduplicate_tags(decoded):\n",
        "    tags = [tag.strip().lower() for tag in decoded.split(\",\")]\n",
        "\n",
        "    # Remove non-informative or overly generic tokens\n",
        "    ignore_list = {\"a\", \"an\", \"the\", \"and\", \"or\", \"of\", \"to\", \"on\", \"in\", \"for\", \"with\", \"etc\", \"from\"}\n",
        "    filtered = [tag for tag in tags if tag not in ignore_list and len(tag) > 1]\n",
        "\n",
        "    # Deduplicate while preserving order\n",
        "    return \", \".join(dict.fromkeys(filtered))\n",
        "\n",
        "def generate_tags_with_pipeline(text):\n",
        "    output = tag_generator(text, max_length=64, num_beams=5, early_stopping=True)\n",
        "    decoded = output[0][\"generated_text\"]\n",
        "    return clean_and_deduplicate_tags(decoded)\n",
        "\n",
        "# ü§ó Realistic repo descriptions for inference (from Hugging Face & this notebook)\n",
        "hf_repos = [\n",
        "    \"Best GitHub repositories with practical notebooks demonstrating real-world AI applications from Hugging Face.\",\n",
        "    \"Best libraries for accessing NLP datasets and evaluation tools in Python.\",\n",
        "    \"Searching for Hugging Face Diffusers repositories for generating images, audio, and other media with pre-trained diffusion models.\"\n",
        "]\n",
        "\n",
        "\n",
        "for repo in hf_repos:\n",
        "    print(f\"üì• Input: {repo}\")\n",
        "    print(f\"üè∑Ô∏è Tags: {generate_tags_with_pipeline(repo)}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}