{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venezianof/booksum/blob/main/notebooks/en/enterprise_dedicated_endpoints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fc45b5-48e6-47b1-95d9-743143bed2ef",
      "metadata": {
        "id": "66fc45b5-48e6-47b1-95d9-743143bed2ef"
      },
      "source": [
        "# Inference Endpoints (dedicated)\n",
        "_Authored by: [Moritz Laurer](https://huggingface.co/MoritzLaurer)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df4e03b4"
      },
      "source": [
        "### Elaborating on 'New Avenues for Intervention'\n",
        "\n",
        "'New Avenues for Intervention' refers to the potential to develop innovative therapeutic strategies for metabolic diseases by leveraging our understanding of how epigenetic mechanisms influence GLP-1. This goes beyond simply targeting the GLP-1 receptor with agonists. Instead, it could involve:\n",
        "\n",
        "*   **Epigenetic Modulator Drugs**: Developing drugs that specifically target enzymes or proteins responsible for epigenetic modifications (like DNA methyltransferases or histone deacetylases) to enhance beneficial GLP-1 production or signaling pathways.\n",
        "*   **Personalized Medicine**: Tailoring treatments based on an individual's unique epigenetic profile related to GLP-1, potentially identifying those who would respond best to epigenetic therapies.\n",
        "*   **Nutrigenomics and Environmental Interventions**: Designing dietary or lifestyle interventions that are known to induce favorable epigenetic changes, thereby naturally modulating GLP-1 activity and improving metabolic health.\n",
        "*   **Gene Editing/Therapy**: Although more futuristic, understanding specific epigenetic marks might even open doors for targeted gene editing approaches to correct dysregulated GLP-1 pathways.\n",
        "\n",
        "The ultimate goal is to find more effective, and potentially more durable, treatments for conditions like type 2 diabetes and obesity by addressing the root epigenetic causes of GLP-1 dysregulation."
      ],
      "id": "df4e03b4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "14c42498",
        "outputId": "466c94a0-600f-49c3-f13f-35f1282b1b6f"
      },
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "import huggingface_hub\n",
        "\n",
        "# Assicurati che l'InferenceClient sia inizializzato\n",
        "client = InferenceClient()\n",
        "\n",
        "# Definizione dei parametri dell'endpoint di Stable Diffusion\n",
        "model_id_sd = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "endpoint_name_sd = \"stable-diffusion-xl-base-1-0-001\"  # Nome dell'endpoint usato in precedenza\n",
        "namespace_sd = \"MoritzLaurer\"  # Il tuo nome utente o organizzazione\n",
        "\n",
        "# Recupera l'endpoint esistente e assicurati che sia pronto\n",
        "print(f\"Recupero dell'endpoint '{endpoint_name_sd}'...\")\n",
        "endpoint = huggingface_hub.get_inference_endpoint(\n",
        "    name=endpoint_name_sd,\n",
        "    namespace=namespace_sd\n",
        ")\n",
        "\n",
        "# Se l'endpoint Ã¨ in pausa o scalato a zero, ripristinalo e attendi\n",
        "if endpoint.status in [\"paused\", \"scaledToZero\"]:\n",
        "    print(\"Ripristino dell'endpoint...\")\n",
        "    endpoint.resume()\n",
        "\n",
        "print(\"Attesa che l'endpoint sia pronto...\")\n",
        "endpoint.wait()\n",
        "print(\"Endpoint pronto!\")\n",
        "\n",
        "prompt_to_generate = \"An astronaut riding a horse on the moon, in a photorealistic style.\"\n",
        "\n",
        "print(f\"Generazione immagine con prompt: {prompt_to_generate}\")\n",
        "\n",
        "image = client.text_to_image(\n",
        "    prompt=prompt_to_generate,\n",
        "    model=endpoint.url, # Usiamo l'URL dell'endpoint di Stable Diffusion\n",
        "    guidance_scale=8,\n",
        "    num_inference_steps=25,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"Immagine generata:\")\n",
        "display(image.resize((image.width // 2, image.height // 2)))"
      ],
      "id": "14c42498",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recupero dell'endpoint 'stable-diffusion-xl-base-1-0-001'...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HfHubHTTPError",
          "evalue": "(Request ID: apXb2T)\n\n403 Forbidden: None.\nCannot access content at: https://api.endpoints.huggingface.cloud/v2/endpoint/MoritzLaurer/stable-diffusion-xl-base-1-0-001.\nMake sure your token has the correct permissions.\nForbidden: You don't have the required permissions to complete this action, missing permissions: inference.endpoints.read",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api.endpoints.huggingface.cloud/v2/endpoint/MoritzLaurer/stable-diffusion-xl-base-1-0-001",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3389649853.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Recupera l'endpoint esistente e assicurati che sia pronto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Recupero dell'endpoint '{endpoint_name_sd}'...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m endpoint = huggingface_hub.get_inference_endpoint(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name_sd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace_sd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mget_inference_endpoint\u001b[0;34m(self, name, namespace, token)\u001b[0m\n\u001b[1;32m   7856\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_hf_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7857\u001b[0m         )\n\u001b[0;32m-> 7858\u001b[0;31m         \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mInferenceEndpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\nMake sure your token has the correct permissions.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             )\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m416\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: apXb2T)\n\n403 Forbidden: None.\nCannot access content at: https://api.endpoints.huggingface.cloud/v2/endpoint/MoritzLaurer/stable-diffusion-xl-base-1-0-001.\nMake sure your token has the correct permissions.\nForbidden: You don't have the required permissions to complete this action, missing permissions: inference.endpoints.read"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59346841"
      },
      "source": [
        "### Therapeutic Implications of Epigenetica del GLP-1\n",
        "\n",
        "Understanding the epigenetic dynamics related to GLP-1 offers significant therapeutic potential:\n",
        "\n",
        "*   **New Avenues for Intervention**: This knowledge could open up new approaches for therapeutic interventions in metabolic diseases.\n",
        "*   **Identifying Epigenetic Targets**: By identifying specific epigenetic targets that can enhance GLP-1 secretion or sensitivity, novel drugs for metabolic disorders (like type 2 diabetes and obesity) could be developed.\n",
        "*   **Understanding Existing Therapies**: It also provides insight into how current GLP-1-based therapies might exert their long-term effects through modifications at the epigenetic level."
      ],
      "id": "59346841"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e479a92c"
      },
      "source": [
        "## Epigenetica del GLP-1: Dinamiche e Implicazioni Terapeu (Epigenetics of GLP-1: Dynamics and Therapeutic Implications)\n",
        "\n",
        "This topic explores the fascinating intersection of epigenetics and Glucagon-Like Peptide-1 (GLP-1). GLP-1 is an incretin hormone known for its crucial role in glucose homeostasis, appetite regulation, and metabolic health. It's widely targeted in the treatment of type 2 diabetes and obesity.\n",
        "\n",
        "**Epigenetics** refers to heritable changes in gene expression that occur without alterations to the underlying DNA sequence. These changes include DNA methylation, histone modifications, and non-coding RNA mechanisms, all of which can influence how and when genes are turned on or off.\n",
        "\n",
        "In the context of GLP-1, 'Epigenetica del GLP-1' investigates:\n",
        "\n",
        "*   **How epigenetic mechanisms regulate GLP-1 production and signaling**: This could involve studying epigenetic marks on the genes encoding GLP-1, its receptor (GLP-1R), or enzymes involved in its synthesis and degradation.\n",
        "*   **The epigenetic impact of GLP-1 on target tissues**: How GLP-1 signaling, in turn, might induce epigenetic changes in pancreatic beta cells, adipose tissue, or the brain, affecting their function and long-term metabolic health.\n",
        "*   **Therapeutic Implications**: Understanding these epigenetic dynamics could open new avenues for therapeutic interventions. For example, identifying specific epigenetic targets that enhance GLP-1 secretion or sensitivity could lead to novel drugs for metabolic disorders. It also considers how existing GLP-1-based therapies might exert their long-term effects through epigenetic modifications.\n",
        "\n",
        "In essence, this field aims to unravel the complex interplay between environmental factors, gene expression regulation, and the physiological actions of GLP-1, with a view to developing more effective and personalized treatments for metabolic diseases."
      ],
      "id": "e479a92c"
    },
    {
      "cell_type": "markdown",
      "id": "7d1e22fa",
      "metadata": {
        "id": "7d1e22fa"
      },
      "source": [
        "Have you ever wanted to create your own machine learning API? That's what we will do in this recipe with the [HF Dedicated Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index). Inference Endpoints enable you to pick any of the hundreds of thousands of models on the HF Hub, create your own API on a deployment platform you control, and on hardware you choose.\n",
        "\n",
        "[Serverless Inference APIs](link-to-recipe) are great for initial testing, but they are limited to a pre-configured selection of popular models and they are rate limited, because the serverless API's hardware is used by many users at the same time. With a Dedicated Inference Endpoint, you can customize the deployment of your model and the hardware is exclusively dedicated to you.\n",
        "\n",
        "In this recipe, we will:\n",
        "- Create an Inference Endpoint via a simple UI and send standard HTTP requests to the Endpoint\n",
        "- Create and manage different Inference Endpoints programmatically with the `huggingface_hub` library\n",
        "- Cover three use-cases: text generation with an LLM, image generation with Stable Diffusion, and reasoning over images with Idefics2."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68cbd86f-c3fb-46f5-8470-37a23dfeac6a",
      "metadata": {
        "id": "68cbd86f-c3fb-46f5-8470-37a23dfeac6a"
      },
      "source": [
        "## Install and login\n",
        "In case you don't have a HF Account, you can create your account [here](https://huggingface.co/join). If you work in a larger team, you can also create a [HF Organization](https://huggingface.co/organizations) and manage all your models, datasets and Endpoints via this organization. Dedicated Inference Endpoints are a paid service and you will therefore need to add a credit card to the [billing settings](https://huggingface.co/settings/billing) of your personal HF account, or of your HF organization.  \n",
        "\n",
        "You can then create a user access token [here](https://huggingface.co/docs/hub/security-tokens). A token with `read` or `write` permissions will work for this guide, but we encourage the use of fine-grained tokens for increased security. For this notebook, you'll need a fine-grained token with `User Permissions > Inference > Make calls to Inference Endpoints & Manage Inference Endpoints` and `Repository permissions > google/gemma-1.1-2b-it & HuggingFaceM4/idefics2-8b-chatty`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf4e7353-d503-4b12-819a-4ea43e1305b3",
      "metadata": {
        "tags": [],
        "id": "cf4e7353-d503-4b12-819a-4ea43e1305b3"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub~=0.23.3\n",
        "!pip install transformers~=4.41.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "908c9c20-ca5e-4ff9-b5b3-9a0c477a21b5",
      "metadata": {
        "tags": [],
        "id": "908c9c20-ca5e-4ff9-b5b3-9a0c477a21b5"
      },
      "outputs": [],
      "source": [
        "# Login to the HF Hub. We recommend using this login method\n",
        "# to avoid the need for explicitly storing your HF token in variables\n",
        "import huggingface_hub\n",
        "huggingface_hub.interpreter_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b0d609d-60eb-42d9-9524-7be8917509e6",
      "metadata": {
        "id": "2b0d609d-60eb-42d9-9524-7be8917509e6"
      },
      "source": [
        "## Creating your first Endpoint\n",
        "\n",
        "With this initial setup out of the way, we can now create our first Endpoint. Navigate to https://ui.endpoints.huggingface.co/ and click on `+ New` next to `Dedicated Endpoints`. You will then see the interface for creating a new Endpoint with the following options (see image below):\n",
        "\n",
        "- **Model Repository**: Here you can insert the identifier of any model on the HF Hub. For this initial demonstration, we use [google/gemma-1.1-2b-it](https://huggingface.co/google/gemma-1.1-2b-it), a small generative LLM (2.5B parameters).\n",
        "- **Endpoint Name**: The Endpoint Name is automatically generated based on the model identifier, but you are free to change the name. Valid Endpoint names must only contain lower-case characters, numbers or hyphens (\"-\") and are between 4 to 32 characters long.\n",
        "- **Instance Configuration**: Here you can choose from a wide range of CPUs or GPUs from all major cloud platforms. You can also adjust the region, for example if you need to host your Endpoint in the EU.\n",
        "- **Automatic Scale-to-Zero**: You can configure your Endpoint to scale to zero GPUs/CPUs after a certain amount of time. Scaled-to-zero Endpoints are not billed anymore. Note that restarting the Endpoint requires the model to be re-loaded into memory (and potentially re-downloaded), which can take several minutes for large models.\n",
        "- **Endpoint Security Level**: The standard security level is `Protected`, which requires an authorized HF token for accessing the Endpoint. `Public` Endpoints are accessible by anyone without token authentification. `Private` Endpoints are only available through an intra-region secured AWS or Azure PrivateLink connection.\n",
        "- **Advanced configuration**: Here you can select some advanced options like the Docker container type. As Gemma is compatible with [Text Generation Inference (TGI)](https://huggingface.co/docs/text-generation-inference/index) containers, the system automatically selects TGI as the container type and other good default values.\n",
        "\n",
        "For this guide, select the options in the image below and click on `Create Endpoint`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b56622c2-334d-4523-b342-c1a9ccae66ad",
      "metadata": {
        "id": "b56622c2-334d-4523-b342-c1a9ccae66ad"
      },
      "source": [
        "<div style=\"display: flex; justify-content: center !important;\">\n",
        "    <img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/enterprise-endpoints-creation.png\">  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcf4d24d"
      },
      "source": [
        "### Executing a Text Generation Model"
      ],
      "id": "bcf4d24d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b12e11c"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a pre-trained text generation model. 'distilgpt2' is a good lightweight option.\n",
        "text_generator = pipeline(\"text-generation\", model=\"distilgpt2\")"
      ],
      "id": "3b12e11c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccbec0db"
      },
      "source": [
        "### Generate Text\n",
        "Now, provide a prompt and let the model generate text based on it."
      ],
      "id": "ccbec0db"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b383a44"
      },
      "source": [
        "prompt = \"Once upon a time, in a land far, far away, a brave knight set out on a quest to\"\n",
        "generated_text = text_generator(prompt, max_length=50, num_return_sequences=1)\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(\"\\nGenerated Text:\")\n",
        "print(generated_text[0]['generated_text'])"
      ],
      "id": "9b383a44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d68ab8ec"
      },
      "source": [
        "You can experiment with different `prompt` values, `max_length` (to control the length of the generated text), and `num_return_sequences` (to get multiple unique generations)."
      ],
      "id": "d68ab8ec"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b34d5f13"
      },
      "source": [
        "### 1. Install the `transformers` library\n",
        "If you don't have it installed, you'll need to run this cell. It's good practice to do this at the beginning of your notebook."
      ],
      "id": "b34d5f13"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc02c591",
        "outputId": "619623d6-c9a7-4294-e92a-deb0efebaf5b"
      },
      "source": [
        "!pip install transformers torch"
      ],
      "id": "cc02c591",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f67a7e65"
      },
      "source": [
        "### 2. Load a pre-trained model and its tokenizer\n",
        "For this example, we'll use a `DistilBERT` model fine-tuned for sentiment analysis."
      ],
      "id": "f67a7e65"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234,
          "referenced_widgets": [
            "f8c748baa19b4daf89578f209336f371",
            "3267df31e79849789ca40adeff9faf1d",
            "cbb1d64ce17a43b7a7d13e5f3669b582",
            "9cfa705013b145458e730fd9df36d796",
            "363e7b2d76984a35be4431fbb59058d5",
            "733f5c3940d841c397ed204b61f7a7ff",
            "b1713b6d8a904fd6ac7cdd2a22a9ea09",
            "b065d38755cc46fe99dac9edadbb43f3",
            "715d626d02074d9794d0a669d042eb80",
            "eedfe1122ce74e92bc655c1a9de8e392",
            "2e67648a60644a9580b3452c75dfc7ba",
            "d0d743bae4574702864da2a7c27f6205",
            "128ec12e639843a6b8a4895648a43e27",
            "8ae9259982b24b028551a91fdadb3f2f",
            "61130cf1fd1845cba40c3930fa2a8967",
            "bde3161a14154d088d3151a34040f7e7",
            "5c0d4c2129ca4c298dd40e327b7152e1",
            "a443b8433883451bbfdd0c54f64d202d",
            "9f03b2aff5fd40adbc754373306d78a2",
            "be84032a40a54ebf96c8a1605392e1a1",
            "c2a7538441824968a19cc0e504bf3a0e",
            "a6d62f4b1b5f4363ac9b4e15d9a3947d",
            "77d083cc1436469c98468bdd2f86b0b4",
            "5622ea92cafc411ab5050bf029422c32",
            "94c98f6427494204afa355e7b2f6de97",
            "c9f0bc36a9954a969b4716cdd493d343",
            "02d044819e9e43179df83b9ccce46c87",
            "51c63c95bdec43ae97c1af477293ef82",
            "4c1364996e0b4b5d94363272ca8c9de7",
            "48907d09fdaf477fa7ee73dfcb494528",
            "8da7f1b7e6344a779e0263dd472c1c0e",
            "1cb04ec504b94fdabcfdfffcd5b5efa5",
            "647b03e18ba64fedb327f31a2dd61e1b",
            "822905e57fc246f9b4fd9c21599ae453",
            "d8f7d66e0daa4017894e2c6ec15b1fc1",
            "09636e93fc1443a5b57b4908053f86ae",
            "2c1c983716a6423f91d4555064d55941",
            "67ed44d68fbf4598bfbca6fc459f5d9a",
            "5a43a66c7c814ca49ac9cfffcffcd1d6",
            "796188bba8944b2f99dbd2d48cafca0f",
            "65f8b45e7c1a446e8747c73b9efc00bd",
            "8f2d50a5219744c7bdcbc8b2595fad5f",
            "a94b315522044a83b97cc56d2c020e6e",
            "417e3bc5e43042fea85ddb581af3ccf9"
          ]
        },
        "id": "73a2fd50",
        "outputId": "69fde2ea-8a9d-4f63-b644-91a830a77679"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# This creates a 'pipeline' for sentiment analysis, abstracting away the model and tokenizer details.\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
      ],
      "id": "73a2fd50",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8c748baa19b4daf89578f209336f371"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0d743bae4574702864da2a7c27f6205"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77d083cc1436469c98468bdd2f86b0b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "822905e57fc246f9b4fd9c21599ae453"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00d6e052"
      },
      "source": [
        "### 3. Prepare your input and perform inference\n",
        "Now, you can pass text to the pipeline to get sentiment predictions."
      ],
      "id": "00d6e052"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ce4a064",
        "outputId": "f6bfdbf2-d35e-4e1e-c6e1-b21aeeebfa30"
      },
      "source": [
        "text_to_analyze = \"This is a fantastic example of running a model in Colab!\"\n",
        "result = sentiment_pipeline(text_to_analyze)\n",
        "\n",
        "print(f\"Text: \\\"{text_to_analyze}\\\"\")\n",
        "print(f\"Sentiment: {result[0]['label']} with score: {result[0]['score']:.2f}\")\n",
        "\n",
        "text_to_analyze_negative = \"This was a terrible experience, I'm very disappointed.\"\n",
        "result_negative = sentiment_pipeline(text_to_analyze_negative)\n",
        "\n",
        "print(f\"\\nText: \\\"{text_to_analyze_negative}\\\"\")\n",
        "print(f\"Sentiment: {result_negative[0]['label']} with score: {result_negative[0]['score']:.2f}\")"
      ],
      "id": "7ce4a064",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: \"This is a fantastic example of running a model in Colab!\"\n",
            "Sentiment: POSITIVE with score: 1.00\n",
            "\n",
            "Text: \"This was a terrible experience, I'm very disappointed.\"\n",
            "Sentiment: NEGATIVE with score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ab5d74c"
      },
      "source": [
        "This is a basic example. You can adapt it to different models and tasks (e.g., text generation, image classification) by changing the model loaded and the pipeline type."
      ],
      "id": "2ab5d74c"
    },
    {
      "cell_type": "markdown",
      "id": "cdb85ae5-316d-4628-aa12-0e1628485c27",
      "metadata": {
        "id": "cdb85ae5-316d-4628-aa12-0e1628485c27"
      },
      "source": [
        "After roughly one minute, your Endpoint will be created and you will see a page similar to the image below.\n",
        "\n",
        "On the Endpoint's `Overview` page, will find the URL for querying the Endpoint, a Playground for testing the model and additional tabs on `Analytics`, `Usage & Cost`, `Logs`and `Settings`.  \n",
        "\n",
        "<div style=\"flex justify-center\">\n",
        "    <img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/enterprise-endpoint-overviewpage.png\">  \n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2612aa56",
      "metadata": {
        "id": "2612aa56"
      },
      "source": [
        "### Creating and managing Endpoints programmatically\n",
        "\n",
        "When moving into production, you don't always want to manually start, stop and modify your Endpoints. The `huggingface_hub` library provides good functionality for managing your Endpoints programmatically. See the docs [here](https://huggingface.co/docs/huggingface_hub/guides/inference_endpoints) and details on all functions [here](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_endpoints). Here are some key functions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c7cd4fd",
      "metadata": {
        "id": "1c7cd4fd"
      },
      "outputs": [],
      "source": [
        "# list all your inference endpoints\n",
        "huggingface_hub.list_inference_endpoints()\n",
        "\n",
        "# get an existing endpoint and check it's status\n",
        "endpoint = huggingface_hub.get_inference_endpoint(\n",
        "    name=\"gemma-1-1-2b-it-yci\",  # the name of the endpoint\n",
        "    namespace=\"MoritzLaurer\"  # your user name or organization name\n",
        ")\n",
        "print(endpoint)\n",
        "\n",
        "# Pause endpoint to stop billing\n",
        "endpoint.pause()\n",
        "\n",
        "# Resume and wait until the endpoint is ready\n",
        "#endpoint.resume()\n",
        "#endpoint.wait()\n",
        "\n",
        "# Update the endpoint to a different GPU\n",
        "# You can find the correct arguments for different hardware types in this table: https://huggingface.co/docs/inference-endpoints/pricing#gpu-instances\n",
        "#endpoint.update(\n",
        "#    instance_size=\"x1\",\n",
        "#    instance_type=\"nvidia-a100\",  # nvidia-a10g\n",
        "#)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b62976e",
      "metadata": {
        "id": "3b62976e"
      },
      "source": [
        "You can also create an inference Endpoint programmatically. Let's recreate the same `gemma` LLM Endpoint as the one created with the UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67ebf5c6",
      "metadata": {
        "id": "67ebf5c6"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import create_inference_endpoint\n",
        "\n",
        "\n",
        "model_id = \"google/gemma-1.1-2b-it\"\n",
        "endpoint_name = \"gemma-1-1-2b-it-001\"  # Valid Endpoint names must only contain lower-case characters, numbers or hyphens (\"-\") and are between 4 to 32 characters long.\n",
        "namespace = \"MoritzLaurer\"  # your user or organization name\n",
        "\n",
        "\n",
        "# check if endpoint with this name already exists from previous tests\n",
        "available_endpoints_names = [endpoint.name for endpoint in huggingface_hub.list_inference_endpoints()]\n",
        "if endpoint_name in available_endpoints_names:\n",
        "    endpoint_exists = True\n",
        "else:\n",
        "    endpoint_exists = False\n",
        "print(\"Does the endpoint already exist?\", endpoint_exists)\n",
        "\n",
        "\n",
        "# create new endpoint\n",
        "if not endpoint_exists:\n",
        "    endpoint = create_inference_endpoint(\n",
        "        endpoint_name,\n",
        "        repository=model_id,\n",
        "        namespace=namespace,\n",
        "        framework=\"pytorch\",\n",
        "        task=\"text-generation\",\n",
        "        # see the available hardware options here: https://huggingface.co/docs/inference-endpoints/pricing#pricing\n",
        "        accelerator=\"gpu\",\n",
        "        vendor=\"aws\",\n",
        "        region=\"us-east-1\",\n",
        "        instance_size=\"x1\",\n",
        "        instance_type=\"nvidia-a10g\",\n",
        "        min_replica=0,\n",
        "        max_replica=1,\n",
        "        type=\"protected\",\n",
        "        # since the LLM is compatible with TGI, we specify that we want to use the latest TGI image\n",
        "        custom_image={\n",
        "            \"health_route\": \"/health\",\n",
        "            \"env\": {\n",
        "                \"MODEL_ID\": \"/repository\"\n",
        "            },\n",
        "            \"url\": \"ghcr.io/huggingface/text-generation-inference:latest\",\n",
        "        },\n",
        "    )\n",
        "    print(\"Waiting for endpoint to be created\")\n",
        "    endpoint.wait()\n",
        "    print(\"Endpoint ready\")\n",
        "\n",
        "# if endpoint with this name already exists, get and resume existing endpoint\n",
        "else:\n",
        "    endpoint = huggingface_hub.get_inference_endpoint(name=endpoint_name, namespace=namespace)\n",
        "    if endpoint.status in [\"paused\", \"scaledToZero\"]:\n",
        "        print(\"Resuming endpoint\")\n",
        "        endpoint.resume()\n",
        "    print(\"Waiting for endpoint to start\")\n",
        "    endpoint.wait()\n",
        "    print(\"Endpoint ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2552904e",
      "metadata": {
        "id": "2552904e"
      },
      "outputs": [],
      "source": [
        "# access the endpoint url for API calls\n",
        "print(endpoint.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70921600-a20f-455e-b53a-561311b6beda",
      "metadata": {
        "tags": [],
        "id": "70921600-a20f-455e-b53a-561311b6beda"
      },
      "source": [
        "## Querying your Endpoint\n",
        "\n",
        "Now let's query this Endpoint like any other LLM API. First copy the Endpoint URL from the interface (or use `endpoint.url`) and assign it to `API_URL` below. We then use the standardised messages format for the text inputs, i.e. a dictionary of user and assistant messages, which you might know from other LLM API services. We then need to apply the chat template to the messages, which LLMs like Gemma, Llama-3 etc. have been trained to expect (see details on in the [docs](https://huggingface.co/docs/transformers/main/en/chat_templating)). For most recent generative LLMs, it is essential to apply this chat template, otherwise the model's performance will degrade without throwing an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ab4e8cd-16ae-4f73-b35a-e571eb83e700",
      "metadata": {
        "tags": [],
        "id": "9ab4e8cd-16ae-4f73-b35a-e571eb83e700"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# paste your endpoint URL here or reuse endpoint.url if you created the endpoint programmatically\n",
        "API_URL = endpoint.url  # or paste link like \"https://dz07884a53qjqb98.us-east-1.aws.endpoints.huggingface.cloud\"\n",
        "HEADERS = {\"Authorization\": f\"Bearer {huggingface_hub.get_token()}\"}\n",
        "\n",
        "# function for standard http requests\n",
        "def query(payload=None, api_url=None):\n",
        "    response = requests.post(api_url, headers=HEADERS, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "# define conversation input in messages format\n",
        "# you can also provide multiple turns between user and assistant\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Please write a short poem about open source for me.\"},\n",
        "    #{\"role\": \"assistant\", \"content\": \"I am not in the mood.\"},\n",
        "    #{\"role\": \"user\", \"content\": \"Can you please do this for me?\"},\n",
        "]\n",
        "\n",
        "# apply the chat template for the respective model\n",
        "model_id = \"google/gemma-1.1-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "messages_with_template = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "print(\"Your text input looks like this, after the chat template has been applied:\\n\")\n",
        "print(messages_with_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9fc7bb2-e00f-414e-8765-fbbfe71ff5e5",
      "metadata": {
        "tags": [],
        "id": "c9fc7bb2-e00f-414e-8765-fbbfe71ff5e5"
      },
      "outputs": [],
      "source": [
        "# send standard http request to endpoint\n",
        "output = query(\n",
        "    payload = {\n",
        "        \"inputs\": messages_with_template,\n",
        "        \"parameters\": {\"temperature\": 0.2, \"max_new_tokens\": 100, \"seed\": 42, \"return_full_text\": False},\n",
        "    },\n",
        "    api_url = API_URL\n",
        ")\n",
        "\n",
        "print(\"The output from your API/Endpoint call:\\n\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e1c5f9c-88a2-473a-b0eb-172659dce626",
      "metadata": {
        "id": "4e1c5f9c-88a2-473a-b0eb-172659dce626"
      },
      "source": [
        "That's it, you've made the first request to your Endpoint - your very own API!\n",
        "\n",
        "If you want the Endpoint to handle the chat template automatically and if your LLM runs on a TGI container, you can also use the [messages API](https://huggingface.co/docs/text-generation-inference/en/messages_api) by appending the `/v1/chat/completions` path to the URL. With the `/v1/chat/completions` path, the [TGI](https://huggingface.co/docs/text-generation-inference/index) container running on the Endpoint applies the chat template automatically and is fully compatible with OpenAI's API structure for easier interoperability. See the [TGI Swagger UI](https://huggingface.github.io/text-generation-inference/#/Text%20Generation%20Inference/chat_completions) for all available parameters. Note that the parameters accepted by the default `/` path and by the `/v1/chat/completions` path are slightly different. Here is the slightly modified code for using the messages API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ed7ca02-78f0-420c-8930-17b08eb323ba",
      "metadata": {
        "tags": [],
        "id": "2ed7ca02-78f0-420c-8930-17b08eb323ba"
      },
      "outputs": [],
      "source": [
        "API_URL_CHAT = API_URL + \"/v1/chat/completions\"\n",
        "\n",
        "output = query(\n",
        "    payload = {\n",
        "        \"messages\": messages,\n",
        "        \"model\": \"tgi\",\n",
        "        \"parameters\": {\"temperature\": 0.2, \"max_tokens\": 100, \"seed\": 42},\n",
        "    },\n",
        "    api_url = API_URL_CHAT\n",
        ")\n",
        "\n",
        "print(\"The output from your API/Endpoint call with the OpenAI-compatible messages API route:\\n\")\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14066377-755d-4292-972e-33a5694e6024",
      "metadata": {
        "id": "14066377-755d-4292-972e-33a5694e6024"
      },
      "source": [
        "### Simplified Endpoint usage with the InferenceClient\n",
        "\n",
        "You can also use the [`InferenceClient`](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client#huggingface_hub.InferenceClient) to easily send requests to your Endpoint. The client is a convenient utility available in the `huggingface_hub` Python library that allows you to easily make calls to both [Dedicated Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) and the [Serverless Inference API](https://huggingface.co/docs/api-inference/index). See the [docs](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client#inference) for details.\n",
        "\n",
        "This is the most succinct way of sending requests to your Endpoint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64e08679-4ea4-422c-b6e5-69e39b50f31e",
      "metadata": {
        "tags": [],
        "id": "64e08679-4ea4-422c-b6e5-69e39b50f31e"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient()\n",
        "\n",
        "output = client.chat_completion(\n",
        "    messages,  # the chat template is applied automatically, if your endpoint uses a TGI container\n",
        "    model=API_URL,\n",
        "    temperature=0.2, max_tokens=100, seed=42,\n",
        ")\n",
        "\n",
        "print(\"The output from your API/Endpoint call with the InferenceClient:\\n\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56fdd862",
      "metadata": {
        "id": "56fdd862"
      },
      "outputs": [],
      "source": [
        "# pause the endpoint to stop billing\n",
        "#endpoint.pause()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f92df9-9ea9-4831-b167-9e7bce5e4179",
      "metadata": {
        "id": "a2f92df9-9ea9-4831-b167-9e7bce5e4179"
      },
      "source": [
        "## Creating Endpoints for a wide variety of models\n",
        "Following the same process, you can create Endpoints for any of the models on the HF Hub. Let's illustrate some other use-cases."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d27fc50f-ae65-4a65-92e8-d5978ca1968a",
      "metadata": {
        "id": "d27fc50f-ae65-4a65-92e8-d5978ca1968a"
      },
      "source": [
        "### Image generation with Stable Diffusion\n",
        "We can create an image generation Endpoint with almost the exact same code as for the LLM. The only difference is that we do not use the TGI container in this case, as TGI is only designed for LLMs (and vision LMs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79996af3-d848-45ab-b346-407f41a83ecf",
      "metadata": {
        "tags": [],
        "id": "79996af3-d848-45ab-b346-407f41a83ecf"
      },
      "outputs": [],
      "source": [
        "!pip install Pillow  # for image processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c71068-c712-4cf3-b6b5-4400529d49b3",
      "metadata": {
        "tags": [],
        "id": "b7c71068-c712-4cf3-b6b5-4400529d49b3"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import create_inference_endpoint\n",
        "\n",
        "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "endpoint_name = \"stable-diffusion-xl-base-1-0-001\"  # Valid Endpoint names must only contain lower-case characters, numbers or hyphens (\"-\") and are between 4 to 32 characters long.\n",
        "namespace = \"MoritzLaurer\"  # your user or organization name\n",
        "task = \"text-to-image\"\n",
        "\n",
        "# check if endpoint with this name already exists from previous tests\n",
        "available_endpoints_names = [endpoint.name for endpoint in huggingface_hub.list_inference_endpoints()]\n",
        "if endpoint_name in available_endpoints_names:\n",
        "    endpoint_exists = True\n",
        "else:\n",
        "    endpoint_exists = False\n",
        "print(\"Does the endpoint already exist?\", endpoint_exists)\n",
        "\n",
        "\n",
        "# create new endpoint\n",
        "if not endpoint_exists:\n",
        "    endpoint = create_inference_endpoint(\n",
        "        endpoint_name,\n",
        "        repository=model_id,\n",
        "        namespace=namespace,\n",
        "        framework=\"pytorch\",\n",
        "        task=task,\n",
        "        # see the available hardware options here: https://huggingface.co/docs/inference-endpoints/pricing#pricing\n",
        "        accelerator=\"gpu\",\n",
        "        vendor=\"aws\",\n",
        "        region=\"us-east-1\",\n",
        "        instance_size=\"x1\",\n",
        "        instance_type=\"nvidia-a100\",\n",
        "        min_replica=0,\n",
        "        max_replica=1,\n",
        "        type=\"protected\",\n",
        "    )\n",
        "    print(\"Waiting for endpoint to be created\")\n",
        "    endpoint.wait()\n",
        "    print(\"Endpoint ready\")\n",
        "\n",
        "# if endpoint with this name already exists, get existing endpoint\n",
        "else:\n",
        "    endpoint = huggingface_hub.get_inference_endpoint(name=endpoint_name, namespace=namespace)\n",
        "    if endpoint.status in [\"paused\", \"scaledToZero\"]:\n",
        "        print(\"Resuming endpoint\")\n",
        "        endpoint.resume()\n",
        "    print(\"Waiting for endpoint to start\")\n",
        "    endpoint.wait()\n",
        "    print(\"Endpoint ready\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af8171e-4cfb-4006-ab17-a8e57838e1cd",
      "metadata": {
        "tags": [],
        "id": "5af8171e-4cfb-4006-ab17-a8e57838e1cd"
      },
      "outputs": [],
      "source": [
        "prompt = \"A whimsical illustration of a fashionably dressed llama proudly holding a worn, vintage cookbook, with a warm cup of tea and a few freshly baked treats scattered around, set against a cozy background of rustic wood and blooming flowers.\"\n",
        "\n",
        "image = client.text_to_image(\n",
        "    prompt=prompt,\n",
        "    model=endpoint.url,  #\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    guidance_scale=8,\n",
        ")\n",
        "\n",
        "print(\"PROMPT: \", prompt)\n",
        "display(image.resize((image.width // 2, image.height // 2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c5558d0-b21e-4b04-bb81-910d398d8f4d",
      "metadata": {
        "id": "0c5558d0-b21e-4b04-bb81-910d398d8f4d"
      },
      "source": [
        "We pause the Endpoint again to stop billing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2adc841-0e37-413d-902f-f389ddceb867",
      "metadata": {
        "id": "b2adc841-0e37-413d-902f-f389ddceb867"
      },
      "outputs": [],
      "source": [
        "endpoint.pause()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3ab1868-8159-44ad-a4d4-1c2df281f20b",
      "metadata": {
        "id": "c3ab1868-8159-44ad-a4d4-1c2df281f20b"
      },
      "source": [
        "### Vision Language Models: Reasoning over text and images\n",
        "\n",
        "Now let's create an Endpoint for a vision language model (VLM). VLMs are very similar to LLMs, only that they can take both text and images as input simultaneously. Their output is autoregressively generated text, just like for a standard LLM. VLMs can tackle many tasks from visual question answering to document understanding. For this example, we use [Idefics2](https://huggingface.co/blog/idefics2), a powerful 8B parameter VLM.\n",
        "\n",
        "We first need to convert our PIL image generated with Stable Diffusion to a `base64` encoded string so that we can send it to the model over the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84a12676-088b-48f4-b68d-c733bb79abde",
      "metadata": {
        "tags": [],
        "id": "84a12676-088b-48f4-b68d-c733bb79abde"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "def pil_image_to_base64(image):\n",
        "    buffered = BytesIO()\n",
        "    image.save(buffered, format=\"JPEG\")\n",
        "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "    return img_str\n",
        "\n",
        "\n",
        "image_b64 = pil_image_to_base64(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5188327c-b32e-475e-8889-80c144402aba",
      "metadata": {
        "id": "5188327c-b32e-475e-8889-80c144402aba"
      },
      "source": [
        "Because VLMs and LLMs are so similar, we can use almost the same messages format and chat template again, only with some additional code for including the image in the prompt. See the [Idefics2 model card](https://huggingface.co/HuggingFaceM4/idefics2-8b) for specific details on prompt formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee7d13f-d79e-46aa-94f8-966e61329fa9",
      "metadata": {
        "tags": [],
        "id": "bee7d13f-d79e-46aa-94f8-966e61329fa9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor\n",
        "\n",
        "# load the processor\n",
        "model_id_vlm = \"HuggingFaceM4/idefics2-8b-chatty\"\n",
        "processor = AutoProcessor.from_pretrained(model_id_vlm)\n",
        "\n",
        "# define the user messages\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},  # the image is placed here in the prompt. You can add multiple images throughout the conversation.\n",
        "            {\"type\": \"text\", \"text\": \"Write a short limerick about this image.\"},\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "\n",
        "# apply the chat template to the messages\n",
        "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "# the chat template places a special \"<image>\" token at the position where the image should go\n",
        "# here we replace the \"<image>\" token with the base64 encoded image string in the prompt\n",
        "# to be able to send the image via an API request\n",
        "image_input = f\"data:image/jpeg;base64,{image_b64}\"\n",
        "image_input = f\"![]({image_input})\"\n",
        "prompt = prompt.replace(\"<image>\", image_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6b308a3-9746-467b-af37-5845ac3273a5",
      "metadata": {
        "id": "f6b308a3-9746-467b-af37-5845ac3273a5"
      },
      "source": [
        "> [!TIP]\n",
        "> For VLMs, an image represents a certain amount of tokens. For Idefics2, for example, one image represents 64 tokens at low resolution and 5*64=320 tokens in high resolution. High resolution is the default in TGI (see `do_image_splitting` in the [model card](https://huggingface.co/HuggingFaceM4/idefics2-8b-chatty) for details). This means that one image consumed 320 tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "296b4569-bf7c-4f72-85fb-cfcf05c9bca9",
      "metadata": {
        "id": "296b4569-bf7c-4f72-85fb-cfcf05c9bca9"
      },
      "source": [
        "Several VLMs like Idefics2 are also supported by TGI (see [list of supported models](https://huggingface.co/docs/text-generation-inference/supported_models)), so we use the TGI container again when creating the Endpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b88d6b22-6b9e-406f-b858-3cd0ff689f97",
      "metadata": {
        "tags": [],
        "id": "b88d6b22-6b9e-406f-b858-3cd0ff689f97"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import create_inference_endpoint\n",
        "\n",
        "endpoint_name = \"idefics2-8b-chatty-001\"\n",
        "namespace = \"MoritzLaurer\"\n",
        "task = \"text-generation\"\n",
        "\n",
        "# check if endpoint with this name already exists from previous tests\n",
        "available_endpoints_names = [endpoint.name for endpoint in huggingface_hub.list_inference_endpoints()]\n",
        "if endpoint_name in available_endpoints_names:\n",
        "    endpoint_exists = True\n",
        "else:\n",
        "    endpoint_exists = False\n",
        "print(\"Does the endpoint already exist?\", endpoint_exists)\n",
        "\n",
        "\n",
        "if endpoint_exists:\n",
        "    endpoint = huggingface_hub.get_inference_endpoint(name=endpoint_name, namespace=namespace)\n",
        "    if endpoint.status in [\"paused\", \"scaledToZero\"]:\n",
        "        print(\"Resuming endpoint\")\n",
        "        endpoint.resume()\n",
        "    print(\"Waiting for endpoint to start\")\n",
        "    endpoint.wait()\n",
        "    print(\"Endpoint ready\")\n",
        "\n",
        "else:\n",
        "    endpoint = create_inference_endpoint(\n",
        "        endpoint_name,\n",
        "        repository=model_id_vlm,\n",
        "        namespace=namespace,\n",
        "        framework=\"pytorch\",\n",
        "        task=task,\n",
        "        accelerator=\"gpu\",\n",
        "        vendor=\"aws\",\n",
        "        region=\"us-east-1\",\n",
        "        type=\"protected\",\n",
        "        instance_size=\"x1\",\n",
        "        instance_type=\"nvidia-a100\",\n",
        "        min_replica=0,\n",
        "        max_replica=1,\n",
        "        custom_image={\n",
        "            \"health_route\": \"/health\",\n",
        "            \"env\": {\n",
        "                \"MAX_BATCH_PREFILL_TOKENS\": \"2048\",\n",
        "                \"MAX_INPUT_LENGTH\": \"1024\",\n",
        "                \"MAX_TOTAL_TOKENS\": \"1536\",\n",
        "                \"MODEL_ID\": \"/repository\"\n",
        "            },\n",
        "            \"url\": \"ghcr.io/huggingface/text-generation-inference:latest\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "    print(\"Waiting for endpoint to be created\")\n",
        "    endpoint.wait()\n",
        "    print(\"Endpoint ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f49d795c-83c8-4003-babd-cb5fb7eb0cdb",
      "metadata": {
        "tags": [],
        "id": "f49d795c-83c8-4003-babd-cb5fb7eb0cdb"
      },
      "outputs": [],
      "source": [
        "output = client.text_generation(\n",
        "    prompt, model=model_id_vlm, max_new_tokens=200, seed=42\n",
        ")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f860719c-d3b1-4b6e-a841-d679499a1030",
      "metadata": {
        "id": "f860719c-d3b1-4b6e-a841-d679499a1030"
      },
      "outputs": [],
      "source": [
        "endpoint.pause()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad607d1-e214-4f7b-8d07-d406980af2b8",
      "metadata": {
        "id": "9ad607d1-e214-4f7b-8d07-d406980af2b8"
      },
      "source": [
        "## Additional information\n",
        "- When creating several Endpoints, you will probably get an error message that your GPU quota has been reached. Don't hesitate to send a message to the email address in the error message and we will most likely increase your GPU quota.\n",
        "- What is the difference between `paused` and `scaled-to-zero` Endpoints? `scaled-to-zero` Endpoints can be flexibly woken up and scaled up by user requests, while `paused` Endpoints need to be unpaused by the creator of the Endpoint. Moreover, `scaled-to-zero` Endpoints count towards your GPU quota (with the maximum possible replica it could be scaled up to), while `paused` Endpoints do not. A simple way of freeing up your GPU quota is therefore to pause some Endpoints.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cbe16bd-ac37-417b-9833-9c31f8ce9e00",
      "metadata": {
        "id": "4cbe16bd-ac37-417b-9833-9c31f8ce9e00"
      },
      "source": [
        "## Conclusion and next steps\n",
        "\n",
        "That's it, you've created three different Endpoints (your own APIs!) for text-to-text, text-to-image, and image-to-text generation and the same is possible for many other models and tasks.\n",
        "\n",
        "We encourage you to read the Dedicated Inference Endpoint [docs](https://huggingface.co/docs/inference-endpoints/index) to learn more. If you are using generative LLMs and VLMs, we also recommend reading the TGI [docs](https://huggingface.co/docs/text-generation-inference/index), as the most popular LLMs/VLMs are also supported by TGI, which makes your Endpoints significantly more efficient.\n",
        "\n",
        "You can, for example, use **JSON-mode or function calling** with open-source models via [TGI Guidance](https://huggingface.co/docs/text-generation-inference/basic_tutorials/using_guidance) (see also this [recipe](https://huggingface.co/learn/cookbook/structured_generation) for an example for RAG with structured generation).\n",
        "\n",
        "When moving your Endpoints into production, you will want to make several additional improvements to make your setup more efficient. When using TGI, you should send batches of requests to the Endpoint with asynchronous function calls to fully utilize the Endpoint's hardware and you can adapt several container parameters to optimize latency and throughput for your use-case. We will cover these optimizations in another recipe.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbe4dfb7",
      "metadata": {
        "id": "fbe4dfb7"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83fb0d4e"
      },
      "source": [
        "# Task\n",
        "Please clarify your specific task or goal related to 'Epigenetica del GLP-1: Dinamiche e Implicazioni Terapeu'."
      ],
      "id": "83fb0d4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6370159"
      },
      "source": [
        "## Understand User Intent\n",
        "\n",
        "### Subtask:\n",
        "Await further clarification from the user regarding the specific task or goal they have in mind for the topic 'Epigenetica del GLP-1: Dinamiche e Implicazioni Terapeu'.\n"
      ],
      "id": "f6370159"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d28cf9c"
      },
      "source": [
        "#### Instructions\n",
        "1. Please provide more details about what you would like to do with the topic 'Epigenetica del GLP-1: Dinamiche e Implicazioni Terapeu'. For example, are you looking for research papers, a summary, a presentation, or something else entirely?"
      ],
      "id": "8d28cf9c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a499579c"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Conclude the task based on the clarified user intent and any subsequent actions.\n"
      ],
      "id": "a499579c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a24ee863"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The primary objective of the subtask was to obtain clarification from the user regarding their specific goal concerning the topic 'Epigenetica del GLP-1: Dinamiche e Implicazioni Terapeu'.\n",
        "*   The agent successfully generated a clear markdown prompt instructing the user to provide more details about their desired outcome (e.g., research papers, a summary, a presentation).\n",
        "*   The task is currently awaiting user input to proceed, and it was marked as 'failure' to indicate that further user interaction is required for completion, rather than an execution error.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The next crucial step is to meticulously process the user's forthcoming clarification to accurately define and execute the subsequent analytical actions.\n",
        "*   The current state highlights the importance of precise initial user input for efficient task progression and successful output generation.\n"
      ],
      "id": "a24ee863"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7876704"
      },
      "source": [
        "# Task\n",
        "Please clarify your specific task or goal related to 'Epigenetica del GLP-1: Dinamiche e Implicazioni Terapeu'."
      ],
      "id": "f7876704"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f045c281"
      },
      "source": [
        "## Understand User Intent\n",
        "\n",
        "### Subtask:\n",
        "Await further clarification from the user regarding the specific task or goal they have in mind for the topic 'Epigenetica del GLP-1: Dinamiche e Implicazioni Terapeu'.\n"
      ],
      "id": "f045c281"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac8d7465"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Conclude the task based on the clarified user intent and any subsequent actions.\n"
      ],
      "id": "ac8d7465"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb6272cc"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The agent successfully performed its subtask, which was to await further clarification from the user regarding their specific goal related to 'Epigenetica del GLP-1: Dinamiche e Implicazioni Terapeu'.\n",
        "*   The `finish_task` command was executed with a `status: success`, indicating that the required action (requesting clarification) was completed.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The next step is to await the user's specific task or goal related to the provided topic before proceeding with any data analysis.\n"
      ],
      "id": "fb6272cc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79a6818c"
      },
      "source": [
        "# Task\n",
        "Perform a conceptual simulation of epigenetic mechanisms influencing a hypothetical GLP-1 related gene by:\n",
        "1. Defining a simplified model for epigenetic states (DNA methylation, histone acetylation).\n",
        "2. Creating a function to simulate gene expression based on these epigenetic states.\n",
        "3. Developing functions to simulate the effects of DNMT and HDAC inhibitors on epigenetic marks.\n",
        "4. Running simulation scenarios (control, DNMT inhibition, HDAC inhibition).\n",
        "5. Visualizing the changes in epigenetic marks and gene expression over time with legends.\n",
        "6. Summarizing the implications of DNMT and HDAC targeting in this conceptual model."
      ],
      "id": "79a6818c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "534c4dcb"
      },
      "source": [
        "## Define Epigenetic State Model\n",
        "\n",
        "### Subtask:\n",
        "Establish a simplified model to represent epigenetic states (e.g., DNA methylation status, histone acetylation status) for a hypothetical gene related to GLP-1. This will involve defining variables or arrays to hold these states.\n"
      ],
      "id": "534c4dcb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67dd20f2"
      },
      "source": [
        "**Reasoning**:\n",
        "To establish a simplified model for epigenetic states, I will initialize a Python dictionary to store the DNA methylation and histone acetylation status of a hypothetical GLP-1 related gene, setting initial baseline numerical values for both.\n",
        "\n"
      ],
      "id": "67dd20f2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "090186da"
      },
      "source": [
        "epigenetic_state_glp1_gene = {\n",
        "    \"DNA_methylation\": 0.5,  # 0: no methylation, 1: full methylation\n",
        "    \"histone_acetylation\": 0.5 # 0: no acetylation, 1: full acetylation\n",
        "}\n",
        "\n",
        "print(\"Initial epigenetic state of hypothetical GLP-1 gene:\")\n",
        "print(epigenetic_state_glp1_gene)"
      ],
      "id": "090186da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10f1007e"
      },
      "source": [
        "## Simulate Gene Expression\n",
        "\n",
        "### Subtask:\n",
        "Create a function that simulates gene expression based on the current epigenetic state. For example, high methylation might decrease expression, while high acetylation might increase it.\n"
      ],
      "id": "10f1007e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcdad04f"
      },
      "source": [
        "**Reasoning**:\n",
        "I will create a Python function `simulate_gene_expression` that calculates gene expression based on the provided epigenetic states. The function will use a simple linear model where DNA methylation negatively impacts expression and histone acetylation positively impacts it, returning a numerical value for gene expression.\n",
        "\n"
      ],
      "id": "fcdad04f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4f9f0dc"
      },
      "source": [
        "def simulate_gene_expression(epigenetic_state):\n",
        "    # Simplified model: high methylation decreases expression, high acetylation increases it.\n",
        "    # Assuming a baseline expression of 0.5, and each epigenetic mark has an equal weight.\n",
        "    # We normalize the impact of methylation and acetylation to keep the expression between 0 and 1.\n",
        "    # For instance, a simple linear model: Expression = Baseline + (Acetylation_factor * Histone_acetylation) - (Methylation_factor * DNA_methylation)\n",
        "\n",
        "    # Let's assume a baseline expression of 0.5 and a sensitivity factor for epigenetic marks\n",
        "    baseline_expression = 0.5\n",
        "    epigenetic_sensitivity = 0.8 # How much epigenetic marks influence expression\n",
        "\n",
        "    # Calculate the impact of acetylation (positive)\n",
        "    acetylation_impact = epigenetic_sensitivity * epigenetic_state[\"histone_acetylation\"]\n",
        "\n",
        "    # Calculate the impact of methylation (negative)\n",
        "    methylation_impact = epigenetic_sensitivity * epigenetic_state[\"DNA_methylation\"]\n",
        "\n",
        "    # Combine impacts to get the raw expression level\n",
        "    gene_expression = baseline_expression + acetylation_impact - methylation_impact\n",
        "\n",
        "    # Ensure expression stays within a reasonable biological range (e.g., 0 to 1)\n",
        "    gene_expression = max(0.0, min(1.0, gene_expression))\n",
        "\n",
        "    return gene_expression\n",
        "\n",
        "# Test the function with the initial epigenetic state\n",
        "initial_expression = simulate_gene_expression(epigenetic_state_glp1_gene)\n",
        "print(f\"Simulated initial GLP-1 gene expression: {initial_expression:.2f}\")"
      ],
      "id": "e4f9f0dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac8eee37"
      },
      "source": [
        "## Simulate DNMT Inhibition\n",
        "\n",
        "### Subtask:\n",
        "Develop a function to simulate the effect of a DNMT inhibitor, which would typically lead to a decrease in DNA methylation levels over time or with increasing 'drug' concentration.\n"
      ],
      "id": "ac8eee37"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4478b5e"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define a Python function `simulate_dnmt_inhibition` as instructed, which will decrease the DNA methylation level in the `epigenetic_state` based on the `dnmt_inhibitor_strength`, ensuring the value remains within the 0 to 1 range, and then test it with a sample strength.\n",
        "\n"
      ],
      "id": "d4478b5e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89f2f76c"
      },
      "source": [
        "def simulate_dnmt_inhibition(epigenetic_state, dnmt_inhibitor_strength):\n",
        "    # Create a copy to avoid modifying the original dictionary directly\n",
        "    new_epigenetic_state = epigenetic_state.copy()\n",
        "\n",
        "    # Calculate the reduction in DNA methylation\n",
        "    # A strength of 0 means no reduction, 1 means maximum possible reduction (towards 0)\n",
        "    # We'll model this as a proportional reduction from the current methylation level.\n",
        "    # For example, if methylation is 0.5 and strength is 0.2, reduction is 0.5 * 0.2 = 0.1\n",
        "    # New methylation will be 0.5 - 0.1 = 0.4\n",
        "    reduction_amount = new_epigenetic_state[\"DNA_methylation\"] * dnmt_inhibitor_strength\n",
        "    new_methylation = new_epigenetic_state[\"DNA_methylation\"] - reduction_amount\n",
        "\n",
        "    # Ensure the new methylation value stays within the biological range [0, 1]\n",
        "    new_epigenetic_state[\"DNA_methylation\"] = max(0.0, min(1.0, new_methylation))\n",
        "\n",
        "    return new_epigenetic_state\n",
        "\n",
        "# Test the function with a sample dnmt_inhibitor_strength\n",
        "sample_dnmt_inhibitor_strength = 0.3 # 30% inhibition\n",
        "\n",
        "updated_epigenetic_state_after_dnmt_inhibition = simulate_dnmt_inhibition(\n",
        "    epigenetic_state_glp1_gene, sample_dnmt_inhibitor_strength\n",
        ")\n",
        "\n",
        "print(f\"Original GLP-1 gene epigenetic state: {epigenetic_state_glp1_gene}\")\n",
        "print(f\"GLP-1 gene epigenetic state after DNMT inhibition (strength={sample_dnmt_inhibitor_strength}):\")\n",
        "print(updated_epigenetic_state_after_dnmt_inhibition)"
      ],
      "id": "89f2f76c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5a55643"
      },
      "source": [
        "## Simulate HDAC Inhibition\n",
        "\n",
        "### Subtask:\n",
        "Develop a function to simulate the effect of an HDAC inhibitor, which would typically lead to an increase in histone acetylation levels.\n"
      ],
      "id": "c5a55643"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "298dc67b"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define a Python function `simulate_hdac_inhibition` as instructed, which will increase the histone acetylation level in the `epigenetic_state` based on the `hdac_inhibitor_strength`, ensuring the value remains within the 0 to 1 range, and then test it with a sample strength.\n",
        "\n"
      ],
      "id": "298dc67b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beb8cb74"
      },
      "source": [
        "def simulate_hdac_inhibition(epigenetic_state, hdac_inhibitor_strength):\n",
        "    # Create a copy to avoid modifying the original dictionary directly\n",
        "    new_epigenetic_state = epigenetic_state.copy()\n",
        "\n",
        "    # Calculate the increase in histone acetylation\n",
        "    # A strength of 0 means no increase, 1 means maximum possible increase (towards 1)\n",
        "    # We'll model this as a proportional increase towards 1.0.\n",
        "    # For example, if acetylation is 0.5 and strength is 0.3, the increase is (1.0 - 0.5) * 0.3 = 0.15\n",
        "    # New acetylation will be 0.5 + 0.15 = 0.65\n",
        "    increase_amount = (1.0 - new_epigenetic_state[\"histone_acetylation\"]) * hdac_inhibitor_strength\n",
        "    new_acetylation = new_epigenetic_state[\"histone_acetylation\"] + increase_amount\n",
        "\n",
        "    # Ensure the new acetylation value stays within the biological range [0, 1]\n",
        "    new_epigenetic_state[\"histone_acetylation\"] = max(0.0, min(1.0, new_acetylation))\n",
        "\n",
        "    return new_epigenetic_state\n",
        "\n",
        "# Test the function with a sample hdac_inhibitor_strength\n",
        "sample_hdac_inhibitor_strength = 0.4 # 40% inhibition\n",
        "\n",
        "updated_epigenetic_state_after_hdac_inhibition = simulate_hdac_inhibition(\n",
        "    epigenetic_state_glp1_gene, sample_hdac_inhibitor_strength\n",
        ")\n",
        "\n",
        "print(f\"Original GLP-1 gene epigenetic state: {epigenetic_state_glp1_gene}\")\n",
        "print(f\"GLP-1 gene epigenetic state after HDAC inhibition (strength={sample_hdac_inhibitor_strength}):\")\n",
        "print(updated_epigenetic_state_after_hdac_inhibition)"
      ],
      "id": "beb8cb74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "071b21ce"
      },
      "source": [
        "## Run Simulation and Visualize\n",
        "\n",
        "### Subtask:\n",
        "Integrate the simulation functions and run scenarios (e.g., control, DNMT inhibition, HDAC inhibition). Visualize the changes in epigenetic marks and resulting gene expression over time using a plot. Make sure to include legends.\n"
      ],
      "id": "071b21ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bce822d0"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the simulation parameters and initialize the data structures to store the epigenetic states and gene expression for the control, DNMT inhibition, and HDAC inhibition scenarios over time.\n",
        "\n"
      ],
      "id": "bce822d0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5adec998"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Define simulation parameters\n",
        "num_time_steps = 20\n",
        "dnmt_inhibitor_strength = 0.1 # Moderate inhibition strength\n",
        "hdac_inhibitor_strength = 0.1 # Moderate inhibition strength\n",
        "\n",
        "# 2. Initialize lists to store historical values for each scenario\n",
        "\n",
        "# Control Scenario\n",
        "control_dna_methylation_history = []\n",
        "control_histone_acetylation_history = []\n",
        "control_gene_expression_history = []\n",
        "\n",
        "# DNMT Inhibition Scenario\n",
        "dnmt_dna_methylation_history = []\n",
        "dnmt_histone_acetylation_history = []\n",
        "dnmt_gene_expression_history = []\n",
        "\n",
        "# HDAC Inhibition Scenario\n",
        "hdac_dna_methylation_history = []\n",
        "hdac_histone_acetylation_history = []\n",
        "hdac_gene_expression_history = []\n",
        "\n",
        "# Initial epigenetic state for each scenario (copy the original to keep it clean)\n",
        "control_epigenetic_state = epigenetic_state_glp1_gene.copy()\n",
        "dnmt_epigenetic_state = epigenetic_state_glp1_gene.copy()\n",
        "hdac_epigenetic_state = epigenetic_state_glp1_gene.copy()"
      ],
      "id": "5adec998",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe1ba0ad"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now implement the simulation loops for the control, DNMT inhibition, and HDAC inhibition scenarios, recording the epigenetic states and gene expression at each time step. Then, I will generate the required plots to visualize the simulation results.\n",
        "\n"
      ],
      "id": "fe1ba0ad"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3290b4ba"
      },
      "source": [
        "for t in range(num_time_steps):\n",
        "    # Control Scenario\n",
        "    control_dna_methylation_history.append(control_epigenetic_state[\"DNA_methylation\"])\n",
        "    control_histone_acetylation_history.append(control_epigenetic_state[\"histone_acetylation\"])\n",
        "    control_gene_expression_history.append(simulate_gene_expression(control_epigenetic_state))\n",
        "\n",
        "    # DNMT Inhibition Scenario\n",
        "    dnmt_epigenetic_state = simulate_dnmt_inhibition(dnmt_epigenetic_state, dnmt_inhibitor_strength)\n",
        "    dnmt_dna_methylation_history.append(dnmt_epigenetic_state[\"DNA_methylation\"])\n",
        "    dnmt_histone_acetylation_history.append(dnmt_epigenetic_state[\"histone_acetylation\"])\n",
        "    dnmt_gene_expression_history.append(simulate_gene_expression(dnmt_epigenetic_state))\n",
        "\n",
        "    # HDAC Inhibition Scenario\n",
        "    hdac_epigenetic_state = simulate_hdac_inhibition(hdac_epigenetic_state, hdac_inhibitor_strength)\n",
        "    hdac_dna_methylation_history.append(hdac_epigenetic_state[\"DNA_methylation\"])\n",
        "    hdac_histone_acetylation_history.append(hdac_epigenetic_state[\"histone_acetylation\"])\n",
        "    hdac_gene_expression_history.append(simulate_gene_expression(hdac_epigenetic_state))\n",
        "\n",
        "# Plotting results\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Plot 1: DNA Methylation Levels Over Time\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(control_dna_methylation_history, label='Control', color='blue')\n",
        "plt.plot(dnmt_dna_methylation_history, label='DNMT Inhibition', color='red')\n",
        "plt.plot(hdac_dna_methylation_history, label='HDAC Inhibition', color='green')\n",
        "plt.title('DNA Methylation Levels Over Time')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('DNA Methylation Level')\n",
        "plt.ylim(0, 1) # Ensure consistent y-axis for epigenetic marks\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 2: Histone Acetylation Levels Over Time\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(control_histone_acetylation_history, label='Control', color='blue')\n",
        "plt.plot(dnmt_histone_acetylation_history, label='DNMT Inhibition', color='red')\n",
        "plt.plot(hdac_histone_acetylation_history, label='HDAC Inhibition', color='green')\n",
        "plt.title('Histone Acetylation Levels Over Time')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Histone Acetylation Level')\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 3: Gene Expression Levels Over Time\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(control_gene_expression_history, label='Control', color='blue')\n",
        "plt.plot(dnmt_gene_expression_history, label='DNMT Inhibition', color='red')\n",
        "plt.plot(hdac_gene_expression_history, label='HDAC Inhibition', color='green')\n",
        "plt.title('Gene Expression Levels Over Time')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Gene Expression Level')\n",
        "plt.ylim(0, 1) # Ensure consistent y-axis for gene expression\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "3290b4ba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add03d9b"
      },
      "source": [
        "## Summarize Implications of DNMT and HDAC Targeting\n",
        "\n",
        "### Subtask:\n",
        "Summarize the implications of DNMT and HDAC targeting in this conceptual model, relating the observed changes in epigenetic marks and gene expression to potential therapeutic outcomes for GLP-1 related conditions.\n",
        "\n",
        "**Reasoning**:\n",
        "Based on the simulation results, I will describe the observed effects of DNMT and HDAC inhibitors on DNA methylation, histone acetylation, and GLP-1 gene expression, and then discuss the potential therapeutic implications of these findings.\n",
        "\n",
        "**Summary of Implications:**\n",
        "\n",
        "*   **Control Scenario**: In the absence of any inhibitors, the epigenetic marks (DNA methylation and histone acetylation) and GLP-1 gene expression remain stable at their initial baseline levels. This represents the physiological state without intervention.\n",
        "\n",
        "*   **DNMT Inhibition**: The simulation shows that inhibiting DNA methyltransferases (DNMTs) leads to a progressive decrease in DNA methylation levels over time. As DNA methylation generally represses gene expression, this decrease in methylation results in an increase in GLP-1 gene expression. This suggests that DNMT inhibitors could be a therapeutic avenue to enhance GLP-1 production by de-repressing the GLP-1 gene, potentially beneficial in conditions like type 2 diabetes where increased GLP-1 activity is desired.\n",
        "\n",
        "*   **HDAC Inhibition**: The simulation demonstrates that inhibiting histone deacetylases (HDACs) leads to a progressive increase in histone acetylation levels over time. Since histone acetylation typically promotes a more open chromatin structure, facilitating gene transcription, this increase in acetylation leads to an increase in GLP-1 gene expression. Similar to DNMT inhibitors, HDAC inhibitors could serve as a strategy to boost GLP-1 levels, offering another epigenetic approach to modulate metabolic health.\n",
        "\n",
        "**Overall Therapeutic Implications**:\n",
        "This conceptual model illustrates how targeting epigenetic mechanisms, specifically DNA methylation and histone acetylation, can influence the expression of a hypothetical GLP-1 related gene. Both DNMT and HDAC inhibitors, by decreasing DNA methylation and increasing histone acetylation respectively, lead to an upregulation of GLP-1 gene expression. This provides a simplified but clear demonstration of how epigenetic drugs could potentially be used to *enhance GLP-1 production or signaling pathways*, thereby opening \"New Avenues for Intervention\" in metabolic diseases as discussed in the initial context. Such interventions could complement existing GLP-1 based therapies or offer novel treatment strategies by addressing the epigenetic root causes of GLP-1 dysregulation."
      ],
      "id": "add03d9b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b397e2e"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the conceptual simulation of DNMT and HDAC targeting and its potential implications.\n"
      ],
      "id": "5b397e2e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48afa441"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The subtask asked to summarize the conceptual simulation of DNMT and HDAC targeting and its potential implications.\n",
        "\n",
        "*   **Control Scenario**: In the absence of inhibitors, DNA methylation and histone acetylation remained stable at their initial levels (0.5 for both), resulting in a stable GLP-1 gene expression of 0.5.\n",
        "*   **DNMT Inhibition**: Inhibition of DNMTs led to a progressive decrease in DNA methylation levels over time, while histone acetylation remained unchanged. This decrease in methylation resulted in a progressive increase in GLP-1 gene expression.\n",
        "*   **HDAC Inhibition**: Inhibition of HDACs led to a progressive increase in histone acetylation levels over time, while DNA methylation remained unchanged. This increase in acetylation resulted in a progressive increase in GLP-1 gene expression.\n",
        "*   **Potential Implications**: Both DNMT and HDAC inhibitors, by altering their respective epigenetic marks (decreasing DNA methylation and increasing histone acetylation), led to an upregulation of GLP-1 gene expression in this conceptual model. This suggests that such epigenetic drugs could potentially be used to enhance GLP-1 production or signaling pathways, offering novel therapeutic strategies for metabolic diseases like type 2 diabetes by addressing epigenetic dysregulation.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The initial epigenetic state of the hypothetical GLP-1 gene was set with DNA methylation at 0.5 and histone acetylation at 0.5.\n",
        "*   The simulated gene expression function indicated that with these initial balanced epigenetic states, the GLP-1 gene expression was 0.50.\n",
        "*   Simulating DNMT inhibition with a strength of 0.3 reduced the DNA methylation level from 0.5 to 0.35.\n",
        "*   Simulating HDAC inhibition with a strength of 0.4 increased the histone acetylation level from 0.5 to 0.7.\n",
        "*   Across 20 time steps, the control scenario maintained stable DNA methylation, histone acetylation, and gene expression levels.\n",
        "*   DNMT inhibition resulted in a decrease in DNA methylation and a corresponding increase in GLP-1 gene expression over time.\n",
        "*   HDAC inhibition resulted in an increase in histone acetylation and a corresponding increase in GLP-1 gene expression over time.\n",
        "*   Visualizations clearly demonstrated the distinct effects of each inhibitor on epigenetic marks and their convergent effect on increasing gene expression.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Targeting epigenetic mechanisms with DNMT or HDAC inhibitors presents a promising conceptual pathway to modulate GLP-1 gene expression, potentially offering new therapeutic avenues for conditions where increased GLP-1 activity is beneficial.\n",
        "*   Further research could involve exploring synergistic effects of combining DNMT and HDAC inhibitors, investigating dose-dependent responses, or integrating more complex regulatory feedback loops to enhance the biological realism of the simulation.\n"
      ],
      "id": "48afa441"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "657bd36f"
      },
      "source": [
        "# Task\n",
        "The conceptual simulation of epigenetic mechanisms influencing a hypothetical GLP-1 related gene by defining simplified models for epigenetic states (DNA methylation, histone acetylation), creating functions to simulate gene expression and the effects of DNMT and HDAC inhibitors, running simulation scenarios (control, DNMT inhibition, HDAC inhibition), visualizing the changes in epigenetic marks and gene expression over time, and summarizing the implications of DNMT and HDAC targeting in this conceptual model has been successfully completed.\n",
        "\n",
        "The key findings from the simulation are:\n",
        "*   **Control Scenario**: Epigenetic marks (DNA methylation and histone acetylation) and GLP-1 gene expression remained stable at their initial baseline levels.\n",
        "*   **DNMT Inhibition**: Led to a progressive decrease in DNA methylation levels, which in turn resulted in a progressive increase in GLP-1 gene expression.\n",
        "*   **HDAC Inhibition**: Led to a progressive increase in histone acetylation levels, which also resulted in a progressive increase in GLP-1 gene expression.\n",
        "\n",
        "The implications are that both DNMT and HDAC inhibitors, by altering their respective epigenetic marks, can upregulate GLP-1 gene expression. This suggests a promising conceptual pathway for developing novel therapeutic strategies for metabolic diseases by targeting epigenetic mechanisms to enhance GLP-1 production or signaling pathways."
      ],
      "id": "657bd36f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9005929d"
      },
      "source": [
        "## Understand User Intent\n",
        "\n",
        "### Subtask:\n",
        "Acknowledge that the user's intent regarding the specific task related to 'Epigenetica del GLP-1: Dinamiche e Implicazioni Terapeu' has been fulfilled through the successful completion and summary of the conceptual simulation.\n"
      ],
      "id": "9005929d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd10b6d2"
      },
      "source": [
        "The user's intent regarding the specific task related to 'Epigenetica del GLP-1: Dinamiche e Implicazioni Terapeu' has been fulfilled through the successful completion and summary of the conceptual simulation of epigenetic mechanisms influencing a hypothetical GLP-1 related gene."
      ],
      "id": "bd10b6d2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e73d03a7"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Conclude the task based on the clarified user intent and any subsequent actions.\n"
      ],
      "id": "e73d03a7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4824b62b"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The conceptual simulation of epigenetic mechanisms influencing a hypothetical GLP-1 related gene demonstrated that:\n",
        "*   In the **Control Scenario**, epigenetic marks (DNA methylation and histone acetylation) and GLP-1 gene expression remained stable at their initial baseline levels.\n",
        "*   **DNMT Inhibition** led to a progressive decrease in DNA methylation levels, resulting in a progressive increase in GLP-1 gene expression.\n",
        "*   **HDAC Inhibition** led to a progressive increase in histone acetylation levels, also resulting in a progressive increase in GLP-1 gene expression.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The simulation successfully modeled the dynamic changes in epigenetic marks and gene expression under different conditions.\n",
        "*   **DNMT inhibition** was observed to progressively decrease DNA methylation, directly correlating with an increase in GLP-1 gene expression.\n",
        "*   **HDAC inhibition** was observed to progressively increase histone acetylation, similarly correlating with an increase in GLP-1 gene expression.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Both DNMT and HDAC inhibitors show potential as conceptual therapeutic targets to upregulate GLP-1 gene expression, suggesting a novel approach for metabolic disease treatment.\n",
        "*   Further research into the specific mechanisms and potential side effects of targeting these epigenetic pathways in a biological context could validate these conceptual findings.\n"
      ],
      "id": "4824b62b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b26acf82"
      },
      "source": [
        "# Task\n",
        "The conceptual simulation of epigenetic mechanisms influencing a hypothetical GLP-1 related gene, including the definition of simplified models for epigenetic states, simulation of gene expression and the effects of DNMT and HDAC inhibitors, running of simulation scenarios, visualization of changes, and a summary of implications, has been successfully completed and approved by the user."
      ],
      "id": "b26acf82"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06ea6291"
      },
      "source": [
        "## Understand User Intent\n",
        "\n",
        "### Subtask:\n",
        "Acknowledge that the user's intent regarding the specific task related to 'Epigenetica del GLP-1: Dinamiche e Implicazioni Terap' has been fulfilled through the successful completion and summary of the conceptual simulation, and then prompt for any new or follow-up tasks related to this topic.\n"
      ],
      "id": "06ea6291"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da655146"
      },
      "source": [
        "The conceptual simulation of epigenetic mechanisms influencing a hypothetical GLP-1 related gene, including the definition of simplified models, simulation of gene expression and inhibitor effects, running scenarios, visualization, and summarization of implications, has been successfully completed and its findings summarized.\n",
        "\n",
        "Now that this specific task is fulfilled, do you have any new or follow-up tasks in mind regarding the broader topic of 'Epigenetica del GLP-1: Dinamiche e Implicazioni Terapeu'? For example, would you like to:\n",
        "\n",
        "*   Explore a different aspect of GLP-1 epigenetics?\n",
        "*   Delve deeper into the therapeutic implications?\n",
        "*   Suggest a new simulation scenario?\n",
        "*   Request a literature review on specific epigenetic modulators?"
      ],
      "id": "da655146"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b057ba1a"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Conclude the task based on the clarified user intent and any subsequent actions.\n"
      ],
      "id": "b057ba1a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c77a78a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The conceptual simulation of epigenetic mechanisms influencing a hypothetical GLP-1 related gene, including simplified models, gene expression simulation, inhibitor effects, scenarios, visualization, and implications, has been successfully completed and its findings summarized.\n",
        "*   A prompt was successfully generated to solicit new or follow-up tasks from the user regarding the broader topic of 'Epigenetica del GLP-1: Dinamiche e Implicazioni Terap', offering specific examples like exploring different aspects, delving deeper into therapeutic implications, suggesting new simulation scenarios, or requesting a literature review on specific epigenetic modulators.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The system is prepared to engage in further analysis or discussion related to GLP-1 epigenetics, building upon the completed simulation.\n",
        "*   The next step involves awaiting user input for new directions, ensuring continuity and relevance in the ongoing research or exploration of the topic.\n"
      ],
      "id": "4c77a78a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8c748baa19b4daf89578f209336f371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3267df31e79849789ca40adeff9faf1d",
              "IPY_MODEL_cbb1d64ce17a43b7a7d13e5f3669b582",
              "IPY_MODEL_9cfa705013b145458e730fd9df36d796"
            ],
            "layout": "IPY_MODEL_363e7b2d76984a35be4431fbb59058d5"
          }
        },
        "3267df31e79849789ca40adeff9faf1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_733f5c3940d841c397ed204b61f7a7ff",
            "placeholder": "â",
            "style": "IPY_MODEL_b1713b6d8a904fd6ac7cdd2a22a9ea09",
            "value": "config.json:â100%"
          }
        },
        "cbb1d64ce17a43b7a7d13e5f3669b582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b065d38755cc46fe99dac9edadbb43f3",
            "max": 629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_715d626d02074d9794d0a669d042eb80",
            "value": 629
          }
        },
        "9cfa705013b145458e730fd9df36d796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eedfe1122ce74e92bc655c1a9de8e392",
            "placeholder": "â",
            "style": "IPY_MODEL_2e67648a60644a9580b3452c75dfc7ba",
            "value": "â629/629â[00:00&lt;00:00,â15.3kB/s]"
          }
        },
        "363e7b2d76984a35be4431fbb59058d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "733f5c3940d841c397ed204b61f7a7ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1713b6d8a904fd6ac7cdd2a22a9ea09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b065d38755cc46fe99dac9edadbb43f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "715d626d02074d9794d0a669d042eb80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eedfe1122ce74e92bc655c1a9de8e392": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e67648a60644a9580b3452c75dfc7ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0d743bae4574702864da2a7c27f6205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_128ec12e639843a6b8a4895648a43e27",
              "IPY_MODEL_8ae9259982b24b028551a91fdadb3f2f",
              "IPY_MODEL_61130cf1fd1845cba40c3930fa2a8967"
            ],
            "layout": "IPY_MODEL_bde3161a14154d088d3151a34040f7e7"
          }
        },
        "128ec12e639843a6b8a4895648a43e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c0d4c2129ca4c298dd40e327b7152e1",
            "placeholder": "â",
            "style": "IPY_MODEL_a443b8433883451bbfdd0c54f64d202d",
            "value": "model.safetensors:â100%"
          }
        },
        "8ae9259982b24b028551a91fdadb3f2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f03b2aff5fd40adbc754373306d78a2",
            "max": 267832558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be84032a40a54ebf96c8a1605392e1a1",
            "value": 267832558
          }
        },
        "61130cf1fd1845cba40c3930fa2a8967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2a7538441824968a19cc0e504bf3a0e",
            "placeholder": "â",
            "style": "IPY_MODEL_a6d62f4b1b5f4363ac9b4e15d9a3947d",
            "value": "â268M/268Mâ[00:02&lt;00:00,â144MB/s]"
          }
        },
        "bde3161a14154d088d3151a34040f7e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c0d4c2129ca4c298dd40e327b7152e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a443b8433883451bbfdd0c54f64d202d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f03b2aff5fd40adbc754373306d78a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be84032a40a54ebf96c8a1605392e1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2a7538441824968a19cc0e504bf3a0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6d62f4b1b5f4363ac9b4e15d9a3947d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77d083cc1436469c98468bdd2f86b0b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5622ea92cafc411ab5050bf029422c32",
              "IPY_MODEL_94c98f6427494204afa355e7b2f6de97",
              "IPY_MODEL_c9f0bc36a9954a969b4716cdd493d343"
            ],
            "layout": "IPY_MODEL_02d044819e9e43179df83b9ccce46c87"
          }
        },
        "5622ea92cafc411ab5050bf029422c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51c63c95bdec43ae97c1af477293ef82",
            "placeholder": "â",
            "style": "IPY_MODEL_4c1364996e0b4b5d94363272ca8c9de7",
            "value": "tokenizer_config.json:â100%"
          }
        },
        "94c98f6427494204afa355e7b2f6de97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48907d09fdaf477fa7ee73dfcb494528",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8da7f1b7e6344a779e0263dd472c1c0e",
            "value": 48
          }
        },
        "c9f0bc36a9954a969b4716cdd493d343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cb04ec504b94fdabcfdfffcd5b5efa5",
            "placeholder": "â",
            "style": "IPY_MODEL_647b03e18ba64fedb327f31a2dd61e1b",
            "value": "â48.0/48.0â[00:00&lt;00:00,â901B/s]"
          }
        },
        "02d044819e9e43179df83b9ccce46c87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c63c95bdec43ae97c1af477293ef82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c1364996e0b4b5d94363272ca8c9de7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48907d09fdaf477fa7ee73dfcb494528": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8da7f1b7e6344a779e0263dd472c1c0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cb04ec504b94fdabcfdfffcd5b5efa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "647b03e18ba64fedb327f31a2dd61e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "822905e57fc246f9b4fd9c21599ae453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8f7d66e0daa4017894e2c6ec15b1fc1",
              "IPY_MODEL_09636e93fc1443a5b57b4908053f86ae",
              "IPY_MODEL_2c1c983716a6423f91d4555064d55941"
            ],
            "layout": "IPY_MODEL_67ed44d68fbf4598bfbca6fc459f5d9a"
          }
        },
        "d8f7d66e0daa4017894e2c6ec15b1fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a43a66c7c814ca49ac9cfffcffcd1d6",
            "placeholder": "â",
            "style": "IPY_MODEL_796188bba8944b2f99dbd2d48cafca0f",
            "value": "vocab.txt:â"
          }
        },
        "09636e93fc1443a5b57b4908053f86ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65f8b45e7c1a446e8747c73b9efc00bd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f2d50a5219744c7bdcbc8b2595fad5f",
            "value": 1
          }
        },
        "2c1c983716a6423f91d4555064d55941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a94b315522044a83b97cc56d2c020e6e",
            "placeholder": "â",
            "style": "IPY_MODEL_417e3bc5e43042fea85ddb581af3ccf9",
            "value": "â232k/?â[00:00&lt;00:00,â2.36MB/s]"
          }
        },
        "67ed44d68fbf4598bfbca6fc459f5d9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a43a66c7c814ca49ac9cfffcffcd1d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "796188bba8944b2f99dbd2d48cafca0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65f8b45e7c1a446e8747c73b9efc00bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8f2d50a5219744c7bdcbc8b2595fad5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a94b315522044a83b97cc56d2c020e6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "417e3bc5e43042fea85ddb581af3ccf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}