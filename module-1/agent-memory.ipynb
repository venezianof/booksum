{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venezianof/booksum/blob/main/module-1/agent-memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13cd1c3e",
      "metadata": {
        "id": "13cd1c3e"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-1/agent-memory.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239417-lesson-7-agent-with-memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c451ffd-a18b-4412-85fa-85186824dd03",
      "metadata": {
        "id": "8c451ffd-a18b-4412-85fa-85186824dd03"
      },
      "source": [
        "# Agent memory\n",
        "\n",
        "## Review\n",
        "\n",
        "Previously, we built an agent that can:\n",
        "\n",
        "* `act` - let the model call specific tools\n",
        "* `observe` - pass the tool output back to the model\n",
        "* `reason` - let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly)\n",
        "\n",
        "![Screenshot 2024-08-21 at 12.45.32 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab7453080e6802cd1703_agent-memory1.png)\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, we're going extend our agent by introducing memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2b4b45b-cbaa-41b1-b3ed-f6b0645be3f9",
      "metadata": {
        "id": "d2b4b45b-cbaa-41b1-b3ed-f6b0645be3f9"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain_openai langchain_core langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b0cfa99",
      "metadata": {
        "id": "2b0cfa99"
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02eff247-a2aa-4f7a-8be1-73dfebfecc63",
      "metadata": {
        "id": "02eff247-a2aa-4f7a-8be1-73dfebfecc63"
      },
      "source": [
        "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74ef2ff0",
      "metadata": {
        "id": "74ef2ff0"
      },
      "outputs": [],
      "source": [
        "_set_env(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c5f123b-db5d-4816-a6a3-2e4247611512",
      "metadata": {
        "id": "9c5f123b-db5d-4816-a6a3-2e4247611512"
      },
      "source": [
        "This follows what we did previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46647bbe-def5-4ea7-a315-1de8d97c8288",
      "metadata": {
        "id": "46647bbe-def5-4ea7-a315-1de8d97c8288"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "# This will be a tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a / b\n",
        "\n",
        "tools = [add, multiply, divide]\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9092b40-20c4-4872-b0ed-be1b53a15ef3",
      "metadata": {
        "id": "a9092b40-20c4-4872-b0ed-be1b53a15ef3"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# System message\n",
        "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
        "\n",
        "# Node\n",
        "def assistant(state: MessagesState):\n",
        "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "771123a3-91ac-4076-92c0-93bcd69cf048",
      "metadata": {
        "id": "771123a3-91ac-4076-92c0-93bcd69cf048"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.prebuilt import tools_condition, ToolNode\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Graph\n",
        "builder = StateGraph(MessagesState)\n",
        "\n",
        "# Define nodes: these do the work\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define edges: these determine how the control flow moves\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
        "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "react_graph = builder.compile()\n",
        "\n",
        "# Show\n",
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e830b7ae-3673-4cc6-8627-4740b7b8b217",
      "metadata": {
        "id": "e830b7ae-3673-4cc6-8627-4740b7b8b217"
      },
      "source": [
        "## Memory\n",
        "\n",
        "Let's run our agent, as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "596a71a0-1337-44d4-971d-f80c367bd868",
      "metadata": {
        "id": "596a71a0-1337-44d4-971d-f80c367bd868"
      },
      "outputs": [],
      "source": [
        "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
        "messages = react_graph.invoke({\"messages\": messages})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# ... your existing code ...\n",
        "\n",
        "# Get your API key\n",
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "\n",
        "# ... your existing code ...\n",
        "\n",
        "# Explicitly set the API key when initializing the ChatOpenAI instance:\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# ...rest of your existing code..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SPuHOI1m6ZM1"
      },
      "id": "SPuHOI1m6ZM1",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# ... your existing code ...\n",
        "\n",
        "# Get your API key\n",
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "\n",
        "# ... your existing code ...\n",
        "\n",
        "# Explicitly set the API key when initializing the ChatOpenAI instance:\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# ...rest of your existing code..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ieg2XUjr6MSd"
      },
      "id": "ieg2XUjr6MSd",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# ... (your other function definitions) ...\n",
        "\n",
        "# Get your API key from environment variable or user input\n",
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Explicitly set the API key when initializing the ChatOpenAI instance:\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# ... (rest of your code) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "k618_Qe86AaF"
      },
      "id": "k618_Qe86AaF",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# ... your existing code ...\n",
        "\n",
        "# Get your API key\n",
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "\n",
        "# ... your existing code ...\n",
        "\n",
        "# Explicitly set the API key when initializing the ChatOpenAI instance:\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# ...rest of your existing code..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "HQyfbdct52LF"
      },
      "id": "HQyfbdct52LF",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# ... your existing code ...\n",
        "\n",
        "# Get your API key\n",
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "\n",
        "# ... your existing code ...\n",
        "\n",
        "# Explicitly set the API key when initializing the ChatOpenAI instance:\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# ...rest of your existing code..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1NpkYyAE5lYb"
      },
      "id": "1NpkYyAE5lYb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "92f8128c-f4a5-4dee-b20b-3245bd33f6b3",
      "metadata": {
        "id": "92f8128c-f4a5-4dee-b20b-3245bd33f6b3"
      },
      "source": [
        "Now, let's multiply by 2!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41cc1d7-e6de-4d86-8958-8cf7446f4c22",
      "metadata": {
        "id": "b41cc1d7-e6de-4d86-8958-8cf7446f4c22"
      },
      "outputs": [],
      "source": [
        "messages = [HumanMessage(content=\"Multiply that by 2.\")]\n",
        "messages = react_graph.invoke({\"messages\": messages})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "source": [
        "import os\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_ACTUAL_API_KEY\" # Replace with your API key"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NronHrou6io5"
      },
      "id": "NronHrou6io5",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import os\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "    llm.invoke(\"Hello!\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Z472d0jg6kTN"
      },
      "id": "Z472d0jg6kTN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "26e65f3c-e1dc-4a62-b8ab-02b33a6ff268",
      "metadata": {
        "id": "26e65f3c-e1dc-4a62-b8ab-02b33a6ff268"
      },
      "source": [
        "We don't retain memory of 7 from our initial chat!\n",
        "\n",
        "This is because [state is transient](https://github.com/langchain-ai/langgraph/discussions/352#discussioncomment-9291220) to a single graph execution.\n",
        "\n",
        "Of course, this limits our ability to have multi-turn conversations with interruptions.\n",
        "\n",
        "We can use [persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) to address this!\n",
        "\n",
        "LangGraph can use a checkpointer to automatically save the graph state after each step.\n",
        "\n",
        "This built-in persistence layer gives us memory, allowing LangGraph to pick up from the last state update.\n",
        "\n",
        "One of the easiest checkpointers to use is the `MemorySaver`, an in-memory key-value store for Graph state.\n",
        "\n",
        "All we need to do is simply compile the graph with a checkpointer, and our graph has memory!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "637fcd79-3896-42e4-9131-e03b123a0a90",
      "metadata": {
        "id": "637fcd79-3896-42e4-9131-e03b123a0a90"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "memory = MemorySaver()\n",
        "react_graph_memory = builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff8fc3bf-3999-47cb-af34-06b2b94d7192",
      "metadata": {
        "id": "ff8fc3bf-3999-47cb-af34-06b2b94d7192"
      },
      "source": [
        "When we use memory, we need to specify a `thread_id`.\n",
        "\n",
        "This `thread_id` will store our collection of graph states.\n",
        "\n",
        "Here is a cartoon:\n",
        "\n",
        "* The checkpointer write the state at every step of the graph\n",
        "* These checkpoints are saved in a thread\n",
        "* We can access that thread in the future using the `thread_id`\n",
        "\n",
        "![state.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e0e9f526b41a4ed9e2d28b_agent-memory2.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f722a1d6-e73c-4023-86ed-8b07d392278d",
      "metadata": {
        "id": "f722a1d6-e73c-4023-86ed-8b07d392278d"
      },
      "outputs": [],
      "source": [
        "# Specify a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Specify an input\n",
        "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
        "\n",
        "# Run\n",
        "messages = react_graph_memory.invoke({\"messages\": messages},config)\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c91a8a16-6bf1-48e2-a889-ae04a37c7a2b",
      "metadata": {
        "id": "c91a8a16-6bf1-48e2-a889-ae04a37c7a2b"
      },
      "source": [
        "If we pass the same `thread_id`, then we can proceed from from the previously logged state checkpoint!\n",
        "\n",
        "In this case, the above conversation is captured in the thread.\n",
        "\n",
        "The `HumanMessage` we pass (`\"Multiply that by 2.\"`) is appended to the above conversation.\n",
        "\n",
        "So, the model now know that `that` refers to the `The sum of 3 and 4 is 7.`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee38c6ef-8bfb-4c66-9214-6f474c9b8451",
      "metadata": {
        "id": "ee38c6ef-8bfb-4c66-9214-6f474c9b8451"
      },
      "outputs": [],
      "source": [
        "messages = [HumanMessage(content=\"Multiply that by 2.\")]\n",
        "messages = react_graph_memory.invoke({\"messages\": messages}, config)\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4b7774e-566f-4c92-9429-ed953bcacaa5",
      "metadata": {
        "id": "c4b7774e-566f-4c92-9429-ed953bcacaa5"
      },
      "source": [
        "## LangGraph Studio\n",
        "\n",
        "\n",
        "**⚠️ DISCLAIMER**\n",
        "\n",
        "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `module-1/studio/` directory in this module:\n",
        "\n",
        "```\n",
        "langgraph dev\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d72986c-ff6f-4f81-b585-d268e2710e53",
      "metadata": {
        "id": "6d72986c-ff6f-4f81-b585-d268e2710e53"
      },
      "outputs": [],
      "source": []
    },
    {
      "source": [
        "!pip install langchain openai tiktoken\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# Replace with your OpenAI API key\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_ACTUAL_API_KEY\"\n",
        "\n",
        "\n",
        "# Initialize the OpenAI language model\n",
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "# Sample text to summarize\n",
        "text = \"\"\"\n",
        "LangChain is a powerful framework for developing applications powered by language models. It provides a set of tools and abstractions for working with LLMs, including prompting, chaining, and memory. By simplifying the development process, LangChain makes it easier to build sophisticated AI-driven applications.\n",
        "\"\"\"\n",
        "\n",
        "# Split the text into smaller chunks for processing\n",
        "text_splitter = CharacterTextSplitter()\n",
        "texts = text_splitter.split_text(text)\n",
        "\n",
        "# Create a LangChain document object\n",
        "docs = [Document(page_content=t) for t in texts]\n",
        "\n",
        "# Load a summarization chain\n",
        "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "# Generate the summary\n",
        "summary = chain.run(docs)\n",
        "\n",
        "# Print the summary\n",
        "print(summary)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vsDBjivV7Jem"
      },
      "id": "vsDBjivV7Jem",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install langchain openai tiktoken langchain_community"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-injbob47Wk-"
      },
      "id": "-injbob47Wk-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}