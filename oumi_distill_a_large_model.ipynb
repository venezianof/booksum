{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venezianof/booksum/blob/main/oumi_distill_a_large_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0paNd8fcaDWn"
      },
      "source": [
        "<div class=\"align-center\">\n",
        "<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n",
        "\n",
        "[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n",
        "[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n",
        "[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n",
        "</div>\n",
        "\n",
        "ðŸ‘‹ Welcome to Open Universal Machine Intelligence (Oumi)!\n",
        "\n",
        "ðŸš€ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](https://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n",
        "\n",
        "ðŸ¤ Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n",
        "\n",
        "â­ If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9116b22b"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import datasets\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.core.types import Conversation, Message, Role\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import pandas as pd\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# Ensure the tutorial directory exists\n",
        "os.makedirs(tutorial_dir, exist_ok=True)\n",
        "\n",
        "# Load the dataset\n",
        "dataset = datasets.load_dataset(\n",
        "    \"meta-math/MetaMathQA\",\n",
        "    revision=\"aa4f34d\",\n",
        "    split=\"train[:10000]\",  # We'll focus only on the first 10k samples.\n",
        ")\n",
        "data = [sample[\"query\"] for sample in dataset]\n",
        "\n",
        "# Prepare conversations\n",
        "conversations = [\n",
        "    Conversation(\n",
        "        messages=[\n",
        "            Message(role=Role.USER, content=prompt),\n",
        "        ]\n",
        "    )\n",
        "    for prompt in data\n",
        "]\n",
        "\n",
        "# Define the CPU-compatible model to use for inference\n",
        "# The 70B model is too large for CPU inference in this environment.\n",
        "# We will use the 1.5B model as the teacher for this CPU-bound inference step.\n",
        "CPU_TEACHER_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# Create a dummy config to hold the model and generation parameters for transformers\n",
        "# This bypasses the need for a specific Oumi inference engine class\n",
        "class DummyModelConfig:\n",
        "    def __init__(self, model_name, torch_dtype_str, model_max_length):\n",
        "        self.model_name = model_name\n",
        "        self.torch_dtype_str = torch_dtype_str\n",
        "        self.model_max_length = model_max_length\n",
        "        self.trust_remote_code = True # DeepSeek models often require this\n",
        "\n",
        "class DummyGenerationConfig:\n",
        "    def __init__(self, max_new_tokens):\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "dummy_config = InferenceConfig(model=DummyModelConfig(\n",
        "    model_name=CPU_TEACHER_MODEL_NAME,\n",
        "    torch_dtype_str=\"float32\", # Force float32 for CPU inference\n",
        "    model_max_length=8192\n",
        "), generation=DummyGenerationConfig(max_new_tokens=8192))\n",
        "\n",
        "# Load model and tokenizer directly using transformers\n",
        "print(f\"Loading model: {dummy_config.model.model_name} for CPU inference...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(dummy_config.model.model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    dummy_config.model.model_name,\n",
        "    torch_dtype=torch.float32, # Force float32 for CPU\n",
        "    trust_remote_code=dummy_config.model.trust_remote_code,\n",
        ").to(\"cpu\")\n",
        "\n",
        "# Set up generation config\n",
        "generation_config = GenerationConfig.from_pretrained(dummy_config.model.model_name)\n",
        "generation_config.max_new_tokens = dummy_config.generation.max_new_tokens\n",
        "generation_config.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully for CPU inference.\")\n",
        "\n",
        "# --- Inference loop (limiting to 100 conversations) ---\n",
        "num_samples_to_process = 100\n",
        "print(f\"Running inference for {num_samples_to_process} conversations...\")\n",
        "generated_responses = []\n",
        "\n",
        "for i, conv in enumerate(conversations[:num_samples_to_process]):\n",
        "    input_text = tokenizer.apply_chat_template(conv.to_dict()[\"messages\"], tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    new_messages = conv.messages + [Message(role=Role.ASSISTANT, content=generated_text)]\n",
        "    generated_conv = Conversation(messages=new_messages)\n",
        "    generated_responses.append(generated_conv)\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Processed {i+1}/{num_samples_to_process} samples.\")\n",
        "\n",
        "print(\"Inference completed.\")\n",
        "\n",
        "# Prepare training data\n",
        "conversation_dicts = [c.to_dict() for c in generated_responses]\n",
        "\n",
        "# Save to JSONL\n",
        "output_filepath = f\"{tutorial_dir}/math_train_10k.jsonl\"\n",
        "dataframe = pd.DataFrame(conversation_dicts)\n",
        "dataframe.to_json(output_filepath, orient=\"records\", lines=True)\n",
        "\n",
        "print(f\"Generated data saved to {output_filepath}\")\n",
        "\n",
        "# Optional: Print first sample to verify\n",
        "if generated_responses:\n",
        "    print(\"\\nFirst generated conversation:\")\n",
        "    print(generated_responses[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d29f747a"
      },
      "source": [
        "import jsonlines\n",
        "import os\n",
        "\n",
        "file_path = \"distillation_tutorial/math_train_10k.jsonl\"\n",
        "\n",
        "print(f\"Lettura delle prime 5 voci da {file_path}:\")\n",
        "\n",
        "# Controlla se il file esiste prima di provare ad aprirlo\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Errore: Il file {file_path} non esiste. Assicurati che il passaggio di generazione sia stato completato con successo.\")\n",
        "else:\n",
        "    with jsonlines.open(file_path) as reader:\n",
        "        for i, obj in enumerate(reader):\n",
        "            print(obj)\n",
        "            if i >= 4:\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9b147ca"
      },
      "source": [
        "## 1. Configurazione Iniziale e Importazione Librerie\n",
        "\n",
        "Per iniziare, impostiamo la directory in cui salveremo i nostri file e importiamo tutte le librerie Python di cui avremo bisogno. `os` per le operazioni sul sistema operativo, `torch` per le operazioni numeriche, `datasets` per caricare i dati, `oumi.core.types` per gestire le conversazioni nel formato Oumi, `transformers` per il modello di linguaggio e il tokenizer, e `pandas` per la manipolazione e il salvataggio dei dati in formato JSONL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27846fc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "e894126e-b2ec-4847-9bec-819ae9b236c6"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import datasets\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.core.types import Conversation, Message, Role\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import pandas as pd\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# Assicurati che la directory esista\n",
        "os.makedirs(tutorial_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Directory di lavoro: {tutorial_dir}\")\n",
        "print(\"Librerie importate con successo.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'oumi'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3766010765.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0moumi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInferenceConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moumi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRole\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'oumi'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f1dbe11"
      },
      "source": [
        "## 2. Caricamento del Dataset\n",
        "\n",
        "Ora carichiamo il dataset 'meta-math/MetaMathQA', che contiene problemi di matematica. Per questa dimostrazione, useremo solo le prime 10.000 domande. Estrarremo le 'query' (le domande) da ogni campione del dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8c879d2"
      },
      "source": [
        "dataset = datasets.load_dataset(\n",
        "    \"meta-math/MetaMathQA\",\n",
        "    revision=\"aa4f34d\",\n",
        "    split=\"train[:10000]\",  # Prendiamo solo i primi 10k campioni.\n",
        ")\n",
        "data = [sample[\"query\"] for sample in dataset]\n",
        "\n",
        "print(f\"Caricati {len(data)} campioni dal dataset MetaMathQA.\")\n",
        "print(\"Primo esempio di domanda:\")\n",
        "print(data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cc53999"
      },
      "source": [
        "## 3. Preparazione delle Conversazioni\n",
        "\n",
        "Per l'inferenza, abbiamo bisogno di formattare le nostre domande come conversazioni. Ogni domanda diventerÃ  un messaggio con il ruolo di 'USER' (utente) all'interno di un oggetto `Conversation` di Oumi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02c3e70b"
      },
      "source": [
        "conversations = [\n",
        "    Conversation(\n",
        "        messages=[\n",
        "            Message(role=Role.USER, content=prompt),\n",
        "        ]\n",
        "    )\n",
        "    for prompt in data\n",
        "]\n",
        "\n",
        "print(f\"Preparate {len(conversations)} conversazioni.\")\n",
        "print(\"Prima conversazione esempio:\")\n",
        "print(conversations[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61dabc88"
      },
      "source": [
        "## 4. Caricamento del Modello e Tokenizer per l'Inferenza su CPU\n",
        "\n",
        "Useremo un modello di linguaggio piÃ¹ piccolo (`DeepSeek-R1-Distill-Qwen-1.5B`) per l'inferenza, in quanto i modelli piÃ¹ grandi richiederebbero una GPU, che potrebbe non essere disponibile. Caricheremo il modello e il suo 'tokenizer' (che converte il testo in numeri che il modello capisce) dalla libreria `transformers`. Specificeremo di usare la CPU e `float32` per la precisione dei calcoli."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fca6672"
      },
      "source": [
        "# Definiamo il nome del modello da usare per l'inferenza su CPU\n",
        "CPU_TEACHER_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# La classe 'DummyModelConfig' e 'DummyGenerationConfig' sono usate\n",
        "# per emulare le configurazioni di Oumi, ma in pratica usiamo direttamente 'transformers'.\n",
        "class DummyModelConfig:\n",
        "    def __init__(self, model_name, torch_dtype_str, model_max_length):\n",
        "        self.model_name = model_name\n",
        "        self.torch_dtype_str = torch_dtype_str\n",
        "        self.model_max_length = model_max_length\n",
        "        self.trust_remote_code = True # Richiesto per alcuni modelli DeepSeek\n",
        "\n",
        "class DummyGenerationConfig:\n",
        "    def __init__(self, max_new_tokens):\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "dummy_config = InferenceConfig(model=DummyModelConfig(\n",
        "    model_name=CPU_TEACHER_MODEL_NAME,\n",
        "    torch_dtype_str=\"float32\", # Forza float32 per inferenza su CPU\n",
        "    model_max_length=8192\n",
        "), generation=DummyGenerationConfig(max_new_tokens=8192))\n",
        "\n",
        "# Carichiamo il tokenizer e il modello direttamente con transformers\n",
        "print(f\"Caricamento del modello: {dummy_config.model.model_name} per inferenza su CPU...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(dummy_config.model.model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    dummy_config.model.model_name,\n",
        "    torch_dtype=torch.float32, # Forza float32 per CPU\n",
        "    trust_remote_code=dummy_config.model.trust_remote_code,\n",
        ").to(\"cpu\")\n",
        "\n",
        "# Configurazione per la generazione del testo\n",
        "generation_config = GenerationConfig.from_pretrained(dummy_config.model.model_name)\n",
        "generation_config.max_new_tokens = dummy_config.generation.max_new_tokens\n",
        "generation_config.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Modello e tokenizer caricati con successo per inferenza su CPU.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88c151c7"
      },
      "source": [
        "## 5. Esecuzione dell'Inferenza (Generazione delle Risposte)\n",
        "\n",
        "Ora useremo il modello caricato per generare le risposte alle nostre domande. Per evitare tempi di esecuzione troppo lunghi su CPU, limiteremo questo processo alle prime 100 conversazioni. Il modello 'leggerÃ ' ogni domanda e genererÃ  una risposta, che verrÃ  poi aggiunta alla conversazione originale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "895e5fa6"
      },
      "source": [
        "# --- Ciclo di inferenza (limitato a 100 conversazioni) ---\n",
        "num_samples_to_process = 100\n",
        "print(f\"Esecuzione inferenza per {num_samples_to_process} conversazioni...\")\n",
        "generated_responses = []\n",
        "\n",
        "for i, conv in enumerate(conversations[:num_samples_to_process]):\n",
        "    # Applica il template di chat al messaggio utente\n",
        "    input_text = tokenizer.apply_chat_template(conv.to_dict()[\"messages\"], tokenize=False, add_generation_prompt=True)\n",
        "    # Tokenizza l'input e sposta sul dispositivo CPU\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "    with torch.no_grad(): # Disabilita il calcolo dei gradienti per risparmiare memoria\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "\n",
        "    # Decodifica la risposta generata dal modello\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    # Aggiungi la risposta generata come messaggio 'ASSISTANT' alla conversazione\n",
        "    new_messages = conv.messages + [Message(role=Role.ASSISTANT, content=generated_text)]\n",
        "    generated_conv = Conversation(messages=new_messages)\n",
        "    generated_responses.append(generated_conv)\n",
        "\n",
        "    if i % 10 == 0: # Stampa un aggiornamento ogni 10 campioni\n",
        "        print(f\"Elaborati {i+1}/{num_samples_to_process} campioni.\")\n",
        "\n",
        "print(\"Inferenza completata.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eabbcc1"
      },
      "source": [
        "## 6. Preparazione e Salvataggio dei Dati Generati\n",
        "\n",
        "Dopo aver generato le risposte, convertiamo le conversazioni in un formato che possa essere facilmente salvato in un file JSONL. Ogni conversazione verrÃ  trasformata in un dizionario e poi salvata riga per riga nel file `distillation_tutorial/math_train_10k.jsonl`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1583026a"
      },
      "source": [
        "conversation_dicts = [c.to_dict() for c in generated_responses]\n",
        "\n",
        "# Salviamo in formato JSONL\n",
        "output_filepath = f\"{tutorial_dir}/math_train_10k.jsonl\"\n",
        "dataframe = pd.DataFrame(conversation_dicts)\n",
        "dataframe.to_json(output_filepath, orient=\"records\", lines=True)\n",
        "\n",
        "print(f\"Dati generati salvati in {output_filepath}\")\n",
        "\n",
        "# Opzionale: Stampa la prima conversazione generata per verifica\n",
        "if generated_responses:\n",
        "    print(\"\\nPrima conversazione generata:\")\n",
        "    print(generated_responses[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "538d6502"
      },
      "source": [
        "## 7. Verifica del File JSONL Generato\n",
        "\n",
        "Infine, verifichiamo che il file `distillation_tutorial/math_train_10k.jsonl` sia stato creato correttamente e contenga i dati. Caricheremo e stamperemo le prime 5 voci del file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b43258ce"
      },
      "source": [
        "import jsonlines\n",
        "import os\n",
        "\n",
        "file_path = \"distillation_tutorial/math_train_10k.jsonl\"\n",
        "\n",
        "print(f\"Lettura delle prime 5 voci da {file_path}:\")\n",
        "\n",
        "# Controlla se il file esiste prima di provare ad aprirlo\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Errore: Il file {file_path} non esiste. Assicurati che il passaggio di generazione sia stato completato con successo.\")\n",
        "else:\n",
        "    with jsonlines.open(file_path) as reader:\n",
        "        for i, obj in enumerate(reader):\n",
        "            print(obj)\n",
        "            if i >= 4:\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49897bc8",
        "outputId": "0142fedd-e7ff-4cae-f598-7f49ddc261d3"
      },
      "source": [
        "import os\n",
        "\n",
        "file_path = \"distillation_tutorial/math_train_10k.jsonl\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(f\"The file {file_path} exists.\")\n",
        "else:\n",
        "    print(f\"The file {file_path} does not exist.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file distillation_tutorial/math_train_10k.jsonl does not exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e5acddb",
        "outputId": "2a8617bc-63ff-4e56-d982-2a2ab136a1f6"
      },
      "source": [
        "%%writefile distillation_tutorial/train.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "  trust_remote_code: true\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  device_map: \"auto\"\n",
        "\n",
        "data:\n",
        "  train:\n",
        "    datasets:\n",
        "      - dataset_name: \"text_sft_jsonl\"\n",
        "        dataset_path: \"./distillation_tutorial/math_train_10k.jsonl\"\n",
        "        split: \"train\"\n",
        "        shuffle: True\n",
        "        seed: 42\n",
        "    seed: 42\n",
        "\n",
        "training:\n",
        "  output_dir: \"distillation_tutorial/output/finetune\"\n",
        "\n",
        "  # For a single GPU, the following gives us a batch size of 16\n",
        "  # If training with multiple GPUs, feel free to reduce gradient_accumulation_steps\n",
        "  per_device_train_batch_size: 2\n",
        "  gradient_accumulation_steps: 8  # Reduce this to 1 for 8xA100-80GB GPUs\n",
        "\n",
        "  # ***NOTE***\n",
        "  # We set it to 10 steps to first verify that it works\n",
        "  # Comment out the line below to have it train for 1 full epoch (all the data)\n",
        "  # Note: 1 full epoch will take about 13 minutes on 8xA100-80GB.\n",
        "  max_steps: 10\n",
        "\n",
        "  num_train_epochs: 1\n",
        "  learning_rate: 1e-4\n",
        "  warmup_ratio: 0.1\n",
        "  logging_steps: 10\n",
        "  save_steps: 0\n",
        "  max_grad_norm: 10\n",
        "  weight_decay: 0.01\n",
        "\n",
        "\n",
        "  trainer_type: \"TRL_SFT\"\n",
        "  optimizer: \"adamw_torch_fused\"\n",
        "  enable_gradient_checkpointing: True\n",
        "  gradient_checkpointing_kwargs:\n",
        "    use_reentrant: False\n",
        "  ddp_find_unused_parameters: False\n",
        "  dataloader_num_workers: \"auto\"\n",
        "  dataloader_prefetch_factor: 32\n",
        "  empty_device_cache_steps: 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing distillation_tutorial/train.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aff91312",
        "outputId": "13f7f24e-dbed-4f1b-a969-73155b0fca95"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Directory '{tutorial_dir}' ensured.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'distillation_tutorial' ensured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4359c5b9",
        "outputId": "553b38b3-0131-4b09-8357-8f33ceed3974"
      },
      "source": [
        "import datasets\n",
        "from oumi.core.types import Conversation, Message, Role\n",
        "\n",
        "# Load the dataset\n",
        "dataset = datasets.load_dataset(\n",
        "    \"meta-math/MetaMathQA\",\n",
        "    revision=\"aa4f34d\",\n",
        "    split=\"train[:10000]\",  # Use the first 10k samples.\n",
        ")\n",
        "data = [sample[\"query\"] for sample in dataset]\n",
        "\n",
        "# Prepare conversations\n",
        "conversations = [\n",
        "    Conversation(\n",
        "        messages=[\n",
        "            Message(role=Role.USER, content=prompt),\n",
        "        ]\n",
        "    )\n",
        "    for prompt in data\n",
        "]\n",
        "\n",
        "print(f\"Loaded {len(conversations)} conversations from MetaMathQA.\")\n",
        "print(\"First conversation example:\")\n",
        "print(conversations[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10000 conversations from MetaMathQA.\n",
            "First conversation example:\n",
            "conversation_id=None messages=[USER: Gracie and Joe are choosing numbers on the complex plane. Joe chooses the point $1+2i$. Gracie chooses $-1+i$. How far apart are Gracie and Joe's points?] metadata={}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        },
        "id": "b86694f9",
        "outputId": "210bed9f-bd00-4164-e9f7-d8bd40e38052"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import datasets\n",
        "from oumi.core.types import Conversation, Message, Role\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import pandas as pd\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# Ensure the tutorial directory exists\n",
        "os.makedirs(tutorial_dir, exist_ok=True)\n",
        "\n",
        "# Load the dataset\n",
        "dataset = datasets.load_dataset(\n",
        "    \"meta-math/MetaMathQA\",\n",
        "    revision=\"aa4f34d\",\n",
        "    split=\"train[:10000]\",  # We'll focus only on the first 10k samples.\n",
        ")\n",
        "data = [sample[\"query\"] for sample in dataset]\n",
        "\n",
        "# Prepare conversations\n",
        "conversations = [\n",
        "    Conversation(\n",
        "        messages=[\n",
        "            Message(role=Role.USER, content=prompt),\n",
        "        ]\n",
        "    )\n",
        "    for prompt in data\n",
        "]\n",
        "\n",
        "print(f\"Loaded {len(conversations)} conversations from MetaMathQA.\")\n",
        "print(\"First conversation example:\")\n",
        "print(conversations[0])\n",
        "\n",
        "CPU_TEACHER_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# Load model and tokenizer directly using transformers\n",
        "print(f\"Loading model: {CPU_TEACHER_MODEL_NAME} for CPU inference...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(CPU_TEACHER_MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    CPU_TEACHER_MODEL_NAME,\n",
        "    torch_dtype=torch.float32, # Force float32 for CPU\n",
        "    trust_remote_code=True,\n",
        ").to(\"cpu\")\n",
        "\n",
        "# Set up generation config\n",
        "generation_config = GenerationConfig.from_pretrained(CPU_TEACHER_MODEL_NAME)\n",
        "generation_config.max_new_tokens = 8192\n",
        "generation_config.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully for CPU inference.\")\n",
        "\n",
        "# --- Inference loop (limiting to 5 conversations for speed) ---\n",
        "num_samples_to_process = 5\n",
        "print(f\"Running inference for {num_samples_to_process} conversations...\")\n",
        "generated_responses = []\n",
        "\n",
        "for i, conv in enumerate(conversations[:num_samples_to_process]):\n",
        "    input_text = tokenizer.apply_chat_template(conv.to_dict()[\"messages\"], tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    new_messages = conv.messages + [Message(role=Role.ASSISTANT, content=generated_text)]\n",
        "    generated_conv = Conversation(messages=new_messages)\n",
        "    generated_responses.append(generated_conv)\n",
        "\n",
        "    print(f\"Processed {i+1}/{num_samples_to_process} samples.\")\n",
        "\n",
        "print(\"Inference completed.\")\n",
        "\n",
        "# Prepare training data and save to JSONL\n",
        "conversation_dicts = [c.to_dict() for c in generated_responses]\n",
        "output_filepath = f\"{tutorial_dir}/math_train_10k.jsonl\"\n",
        "dataframe = pd.DataFrame(conversation_dicts)\n",
        "dataframe.to_json(output_filepath, orient=\"records\", lines=True)\n",
        "\n",
        "print(f\"Generated data saved to {output_filepath}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 10000 conversations from MetaMathQA.\n",
            "First conversation example:\n",
            "conversation_id=None messages=[USER: Gracie and Joe are choosing numbers on the complex plane. Joe chooses the point $1+2i$. Gracie chooses $-1+i$. How far apart are Gracie and Joe's points?] metadata={}\n",
            "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B for CPU inference...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully for CPU inference.\n",
            "Running inference for 5 conversations...\n",
            "Processed 1/5 samples.\n",
            "Processed 2/5 samples.\n",
            "Processed 3/5 samples.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2856972427.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         outputs = model.generate(\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 449\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         hidden_states, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mcache_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"sin\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cos\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cache_position\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcache_position\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mattention_interface\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meager_attention_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/cache_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monly_non_sliding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffloading\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/cache_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, key_states, value_states, cache_kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37d4409a"
      },
      "source": [
        "import jsonlines\n",
        "import os\n",
        "\n",
        "file_path = \"distillation_tutorial/math_train_10k.jsonl\"\n",
        "\n",
        "print(f\"Reading first 5 entries from {file_path}:\")\n",
        "\n",
        "# Check if the file exists before attempting to open it\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Error: The file {file_path} does not exist. Please ensure the generation step completed successfully.\")\n",
        "else:\n",
        "    with jsonlines.open(file_path) as reader:\n",
        "        for i, obj in enumerate(reader):\n",
        "            print(obj)\n",
        "            if i >= 4:\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89b5a8f2"
      },
      "source": [
        "import jsonlines\n",
        "import os\n",
        "\n",
        "file_path = \"distillation_tutorial/math_train_10k.jsonl\"\n",
        "\n",
        "print(f\"Verifying the content of {file_path}:\")\n",
        "if os.path.exists(file_path):\n",
        "    with jsonlines.open(file_path) as reader:\n",
        "        for i, obj in enumerate(reader):\n",
        "            print(obj)\n",
        "            if i >= 4: # Print first 5 entries\n",
        "                break\n",
        "else:\n",
        "    print(f\"Error: The file {file_path} was not created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "375a2706"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import datasets\n",
        "from oumi.core.types import Conversation, Message, Role\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import pandas as pd\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# Ensure the tutorial directory exists\n",
        "os.makedirs(tutorial_dir, exist_ok=True)\n",
        "\n",
        "# Load the dataset\n",
        "dataset = datasets.load_dataset(\n",
        "    \"meta-math/MetaMathQA\",\n",
        "    revision=\"aa4f34d\",\n",
        "    split=\"train[:10000]\",  # We'll focus only on the first 10k samples.\n",
        ")\n",
        "data = [sample[\"query\"] for sample in dataset]\n",
        "\n",
        "# Prepare conversations\n",
        "conversations = [\n",
        "    Conversation(\n",
        "        messages=[\n",
        "            Message(role=Role.USER, content=prompt),\n",
        "        ]\n",
        "    )\n",
        "    for prompt in data\n",
        "]\n",
        "\n",
        "# Define the CPU-compatible model to use for inference\n",
        "# The 70B model is too large for CPU inference in this environment.\n",
        "# We will use the 1.5B model as the teacher for this CPU-bound inference step.\n",
        "CPU_TEACHER_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# Load model and tokenizer directly using transformers\n",
        "print(f\"Loading model: {CPU_TEACHER_MODEL_NAME} for CPU inference...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(CPU_TEACHER_MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    CPU_TEACHER_MODEL_NAME,\n",
        "    torch_dtype=torch.float32, # Force float32 for CPU\n",
        "    trust_remote_code=True,\n",
        ").to(\"cpu\")\n",
        "\n",
        "# Set up generation config\n",
        "generation_config = GenerationConfig.from_pretrained(CPU_TEACHER_MODEL_NAME)\n",
        "generation_config.max_new_tokens = 8192\n",
        "generation_config.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully for CPU inference.\")\n",
        "\n",
        "# --- Inference loop ---\n",
        "# Limiting to 5 conversations for a quick and simple demonstration for novices\n",
        "num_samples_to_process = 5\n",
        "print(f\"Running inference for {num_samples_to_process} conversations...\")\n",
        "generated_responses = []\n",
        "\n",
        "for i, conv in enumerate(conversations[:num_samples_to_process]):\n",
        "    input_text = tokenizer.apply_chat_template(conv.to_dict()[\"messages\"], tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    new_messages = conv.messages + [Message(role=Role.ASSISTANT, content=generated_text)]\n",
        "    generated_conv = Conversation(messages=new_messages)\n",
        "    generated_responses.append(generated_conv)\n",
        "\n",
        "    print(f\"Processed {i+1}/{num_samples_to_process} samples.\")\n",
        "\n",
        "print(\"Inference completed.\")\n",
        "\n",
        "# Prepare training data\n",
        "conversation_dicts = [c.to_dict() for c in generated_responses]\n",
        "\n",
        "# Save to JSONL\n",
        "output_filepath = f\"{tutorial_dir}/math_train_10k.jsonl\"\n",
        "dataframe = pd.DataFrame(conversation_dicts)\n",
        "dataframe.to_json(output_filepath, orient=\"records\", lines=True)\n",
        "\n",
        "print(f\"Generated data saved to {output_filepath}\")\n",
        "\n",
        "# Optional: Print first sample to verify\n",
        "if generated_responses:\n",
        "    print(\"\\nFirst generated conversation:\")\n",
        "    print(generated_responses[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13d4bb70"
      },
      "source": [
        "import os\n",
        "\n",
        "file_path = \"distillation_tutorial/math_train_10k.jsonl\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(f\"The file {file_path} exists.\")\n",
        "else:\n",
        "    print(f\"The file {file_path} does not exist.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbc3901b"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import datasets\n",
        "from oumi.core.types import Conversation, Message, Role\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import pandas as pd\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# Ensure the tutorial directory exists\n",
        "os.makedirs(tutorial_dir, exist_ok=True)\n",
        "\n",
        "# Load the dataset\n",
        "dataset = datasets.load_dataset(\n",
        "    \"meta-math/MetaMathQA\",\n",
        "    revision=\"aa4f34d\",\n",
        "    split=\"train[:10000]\",  # We'll focus only on the first 10k samples.\n",
        ")\n",
        "data = [sample[\"query\"] for sample in dataset]\n",
        "\n",
        "# Prepare conversations\n",
        "conversations = [\n",
        "    Conversation(\n",
        "        messages=[\n",
        "            Message(role=Role.USER, content=prompt),\n",
        "        ]\n",
        "    )\n",
        "    for prompt in data\n",
        "]\n",
        "\n",
        "# Define the CPU-compatible model to use for inference\n",
        "# The 70B model is too large for CPU inference in this environment.\n",
        "# We will use the 1.5B model as the teacher for this CPU-bound inference step.\n",
        "CPU_TEACHER_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# Load model and tokenizer directly using transformers\n",
        "print(f\"Loading model: {CPU_TEACHER_MODEL_NAME} for CPU inference...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(CPU_TEACHER_MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    CPU_TEACHER_MODEL_NAME,\n",
        "    torch_dtype=torch.float32, # Force float32 for CPU\n",
        "    trust_remote_code=True,\n",
        ").to(\"cpu\")\n",
        "\n",
        "# Set up generation config\n",
        "generation_config = GenerationConfig.from_pretrained(CPU_TEACHER_MODEL_NAME)\n",
        "generation_config.max_new_tokens = 8192\n",
        "generation_config.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully for CPU inference.\")\n",
        "\n",
        "# --- Inference loop ---\n",
        "# Limiting to 5 conversations for a quick and simple demonstration for novices\n",
        "num_samples_to_process = 5\n",
        "print(f\"Running inference for {num_samples_to_process} conversations...\")\n",
        "generated_responses = []\n",
        "\n",
        "for i, conv in enumerate(conversations[:num_samples_to_process]):\n",
        "    input_text = tokenizer.apply_chat_template(conv.to_dict()[\"messages\"], tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    new_messages = conv.messages + [Message(role=Role.ASSISTANT, content=generated_text)]\n",
        "    generated_conv = Conversation(messages=new_messages)\n",
        "    generated_responses.append(generated_conv)\n",
        "\n",
        "    print(f\"Processed {i+1}/{num_samples_to_process} samples.\")\n",
        "\n",
        "print(\"Inference completed.\")\n",
        "\n",
        "# Prepare training data\n",
        "conversation_dicts = [c.to_dict() for c in generated_responses]\n",
        "\n",
        "# Save to JSONL\n",
        "output_filepath = f\"{tutorial_dir}/math_train_10k.jsonl\"\n",
        "dataframe = pd.DataFrame(conversation_dicts)\n",
        "dataframe.to_json(output_filepath, orient=\"records\", lines=True)\n",
        "\n",
        "print(f\"Generated data saved to {output_filepath}\")\n",
        "\n",
        "# Optional: Print first sample to verify\n",
        "if generated_responses:\n",
        "    print(\"\\nFirst generated conversation:\")\n",
        "    print(generated_responses[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cb10c9a"
      },
      "source": [
        "!oumi train -c \"distillation_tutorial/train.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5139276f"
      },
      "source": [
        "%%writefile distillation_tutorial/train.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "  trust_remote_code: true\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  device_map: \"auto\"\n",
        "\n",
        "data:\n",
        "  train:\n",
        "    datasets:\n",
        "      - dataset_name: \"text_sft_jsonl\"\n",
        "        dataset_path: \"./distillation_tutorial/math_train_10k.jsonl\"\n",
        "        split: \"train\"\n",
        "        shuffle: True\n",
        "        seed: 42\n",
        "    seed: 42\n",
        "\n",
        "training:\n",
        "  output_dir: \"distillation_tutorial/output/finetune\"\n",
        "\n",
        "  # For a single GPU, the following gives us a batch size of 16\n",
        "  # If training with multiple GPUs, feel free to reduce gradient_accumulation_steps\n",
        "  per_device_train_batch_size: 2\n",
        "  gradient_accumulation_steps: 8  # Reduce this to 1 for 8xA100-80GB GPUs\n",
        "\n",
        "  # ***NOTE***\n",
        "  # We set it to 10 steps to first verify that it works\n",
        "  # Comment out the line below to have it train for 1 full epoch (all the data)\n",
        "  # Note: 1 full epoch will take about 13 minutes on 8xA100-80GB.\n",
        "  max_steps: 10\n",
        "\n",
        "  num_train_epochs: 1\n",
        "  learning_rate: 1e-4\n",
        "  warmup_ratio: 0.1\n",
        "  logging_steps: 10\n",
        "  save_steps: 0\n",
        "  max_grad_norm: 10\n",
        "  weight_decay: 0.01\n",
        "\n",
        "\n",
        "  trainer_type: \"TRL_SFT\"\n",
        "  optimizer: \"adamw_torch_fused\"\n",
        "  enable_gradient_checkpointing: True\n",
        "  gradient_checkpointing_kwargs:\n",
        "    use_reentrant: False\n",
        "  ddp_find_unused_parameters: False\n",
        "  dataloader_num_workers: \"auto\"\n",
        "  dataloader_prefetch_factor: 32\n",
        "  empty_device_cache_steps: 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "018a1fac"
      },
      "source": [
        "!oumi train -c \"distillation_tutorial/train.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e09a5b08"
      },
      "source": [
        "%%writefile distillation_tutorial/train.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "  trust_remote_code: true\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  device_map: \"auto\"\n",
        "\n",
        "data:\n",
        "  train:\n",
        "    datasets:\n",
        "      - dataset_name: \"text_sft_jsonl\"\n",
        "        dataset_path: \"./distillation_tutorial/math_train_10k.jsonl\"\n",
        "        split: \"train\"\n",
        "        shuffle: True\n",
        "        seed: 42\n",
        "    seed: 42\n",
        "\n",
        "training:\n",
        "  output_dir: \"distillation_tutorial/output/finetune\"\n",
        "\n",
        "  # For a single GPU, the following gives us a batch size of 16\n",
        "  # If training with multiple GPUs, feel free to reduce gradient_accumulation_steps\n",
        "  per_device_train_batch_size: 2\n",
        "  gradient_accumulation_steps: 8  # Reduce this to 1 for 8xA100-80GB GPUs\n",
        "\n",
        "  # ***NOTE***\n",
        "  # We set it to 10 steps to first verify that it works\n",
        "  # Comment out the line below to have it train for 1 full epoch (all the data)\n",
        "  # Note: 1 full epoch will take about 13 minutes on 8xA100-80GB.\n",
        "  max_steps: 10\n",
        "\n",
        "  num_train_epochs: 1\n",
        "  learning_rate: 1e-4\n",
        "  warmup_ratio: 0.1\n",
        "  logging_steps: 10\n",
        "  save_steps: 0\n",
        "  max_grad_norm: 10\n",
        "  weight_decay: 0.01\n",
        "\n",
        "\n",
        "  trainer_type: \"TRL_SFT\"\n",
        "  optimizer: \"adamw_torch_fused\"\n",
        "  enable_gradient_checkpointing: True\n",
        "  gradient_checkpointing_kwargs:\n",
        "    use_reentrant: False\n",
        "  ddp_find_unused_parameters: False\n",
        "  dataloader_num_workers: \"auto\"\n",
        "  dataloader_prefetch_factor: 32\n",
        "  empty_device_cache_steps: 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9def4e41"
      },
      "source": [
        "!oumi train -c \"distillation_tutorial/train.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5acfcad0"
      },
      "source": [
        "import oumi\n",
        "import vllm\n",
        "\n",
        "print(f\"Oumi version: {oumi.__version__}\")\n",
        "print(f\"vLLM version: {vllm.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8i3bbl3aDWu"
      },
      "source": [
        "# Distillation Overview\n",
        "\n",
        "In this tutorial, we'll fine-tune a small language model (SLM) from the outputs of a large language model (LLM).\n",
        "\n",
        "We'll use the Oumi framework to streamline the process and achieve high-quality results.\n",
        "\n",
        "We'll cover the following topics:\n",
        "1. Prerequisites\n",
        "2. Data Preparation & Sanity Checks\n",
        "3. Training Config Preparation\n",
        "4. Launching Training\n",
        "5. Monitoring Progress\n",
        "6. Evaluation\n",
        "7. Analyzing Results\n",
        "8. Inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5w4UytbaDWv"
      },
      "source": [
        "# Prerequisites\n",
        "\n",
        "## Hardware\n",
        "The defaults in this tutorial are scaled down for demonstration purposes.\n",
        "\n",
        "The true values are left to code comments within each section.\n",
        "\n",
        "We recommend 8xA100-80GB GPUs to complete in a timely manner with adequate performance.\n",
        "\n",
        "## Oumi Installation\n",
        "\n",
        "First, let's install Oumi and vLLM. You can find more detailed instructions [here](https://oumi.ai/docs/en/latest/get_started/installation.html). Here, we include Oumi's GPU dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dBp6GySzaDWw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "43e8c049-fd56-4985-cca6-8ba6986dea04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting oumi[gpu]\n",
            "  Downloading oumi-0.6.0-py3-none-any.whl.metadata (54 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate<2.0,>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from oumi[gpu]) (1.12.0)\n",
            "Requirement already satisfied: aiohttp<3.16,>=3.12 in /usr/local/lib/python3.12/dist-packages (from oumi[gpu]) (3.13.3)\n",
            "Requirement already satisfied: aiofiles<26,>=24.1.0 in /usr/local/lib/python3.12/dist-packages (from oumi[gpu]) (24.1.0)\n",
            "Collecting aioresponses<0.8,>=0.7 (from oumi[gpu])\n",
            "  Downloading aioresponses-0.7.8-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting backoff<2.3,>=2.2.1 (from oumi[gpu])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting click<8.3.0 (from oumi[gpu])\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: datasets<5,>=3.2 in /usr/local/lib/python3.12/dist-packages (from oumi[gpu]) (4.0.0)\n",
            "Collecting hdrhistogram<0.11,>=0.10 (from oumi[gpu])\n",
            "  Downloading hdrhistogram-0.10.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (26 kB)\n",
            "Collecting lm_eval<0.5.0,>=0.4 (from lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading lm_eval-0.4.9.2-py3-none-any.whl.metadata (53 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mlflow>=3.1 (from oumi[gpu])\n",
            "  Downloading mlflow-3.8.1-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: numpy<2.4,>=1.26 in /usr/local/lib/python3.12/dist-packages (from oumi[gpu]) (2.0.2)\n",
            "Collecting omegaconf==2.4.0.dev4 (from oumi[gpu])\n",
            "  Downloading omegaconf-2.4.0.dev4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from oumi[gpu]) (25.0)\n",
            "Collecting pandas<3,>=2.3 (from oumi[gpu])\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft<0.18,>=0.17 (from oumi[gpu])\n",
            "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pexpect<4.10,>=4.9 in /usr/local/lib/python3.12/dist-packages (from oumi[gpu]) (4.9.0)\n",
            "Requirement already satisfied: pillow<11.4,>=11.3 in /usr/local/lib/python3.12/dist-packages (from oumi[gpu]) (11.3.0)\n",
            "Collecting protobuf>=6.32 (from oumi[gpu])\n",
            "  Downloading protobuf-6.33.4-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting pycares<5.0.0 (from oumi[gpu])\n",
            "  Downloading pycares-4.11.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: pydantic<2.13,>=2.11 in /usr/local/lib/python3.12/dist-packages (from oumi[gpu]) (2.12.3)\n",
            "Collecting responses<0.26,>=0.25 (from oumi[gpu])\n",
            "  Downloading responses-0.25.8-py3-none-any.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors<0.8,>=0.6 in /usr/local/lib/python3.12/dist-packages (from oumi[gpu]) (0.7.0)\n",
            "Collecting skypilot<0.12,>=0.10.2 (from oumi[gpu])\n",
            "  Downloading skypilot-0.11.1-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting tensorboard<2.21,>=2.20 (from oumi[gpu])\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting torch<2.9.0,>=2.6 (from oumi[gpu])\n",
            "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting torchao<0.15,>=0.12 (from oumi[gpu])\n",
            "  Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
            "Collecting torchvision<0.24,>=0.21 (from oumi[gpu])\n",
            "  Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from oumi[gpu]) (4.67.1)\n",
            "Requirement already satisfied: transformers<4.58,>=4.57 in /usr/local/lib/python3.12/dist-packages (from oumi[gpu]) (4.57.3)\n",
            "Collecting trl<0.27,>=0.24 (from oumi[gpu])\n",
            "  Downloading trl-0.26.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from oumi[gpu]) (0.21.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from oumi[gpu]) (4.15.0)\n",
            "Collecting uvicorn<0.36.0 (from oumi[gpu])\n",
            "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: wandb<0.24,>=0.21 in /usr/local/lib/python3.12/dist-packages (from oumi[gpu]) (0.23.1)\n",
            "Collecting liger-kernel<0.7,>=0.6 (from oumi[gpu])\n",
            "  Downloading liger_kernel-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting nvidia-ml-py<13.591,>=13.580 (from oumi[gpu])\n",
            "  Downloading nvidia_ml_py-13.590.44-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting bitsandbytes<0.49,>=0.47 (from oumi[gpu])\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Collecting verl<0.6,>=0.5 (from oumi[gpu])\n",
            "  Downloading verl-0.5.0-py2.py3-none-any.whl.metadata (27 kB)\n",
            "Collecting vllm<0.11,>=0.10 (from oumi[gpu])\n",
            "  Downloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf==2.4.0.dev4->oumi[gpu]) (6.0.3)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines) (25.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<2.0,>=1.10.0->oumi[gpu]) (5.9.5)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2.0,>=1.10.0->oumi[gpu]) (0.36.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.16,>=3.12->oumi[gpu]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.16,>=3.12->oumi[gpu]) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.16,>=3.12->oumi[gpu]) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.16,>=3.12->oumi[gpu]) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.16,>=3.12->oumi[gpu]) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.16,>=3.12->oumi[gpu]) (1.22.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<5,>=3.2->oumi[gpu]) (3.20.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<5,>=3.2->oumi[gpu]) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<5,>=3.2->oumi[gpu]) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<5,>=3.2->oumi[gpu]) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<5,>=3.2->oumi[gpu]) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<5,>=3.2->oumi[gpu]) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<5,>=3.2->oumi[gpu]) (2025.3.0)\n",
            "Collecting pbr>=1.4 (from hdrhistogram<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading pbr-7.0.3-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting triton>=2.3.1 (from liger-kernel<0.7,>=0.6->oumi[gpu])\n",
            "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting evaluate (from lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu]) (2.14.1)\n",
            "Collecting pybind11>=2.6.2 (from lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting pytablewriter (from lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting rouge-score>=0.0.4 (from lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu>=1.5.0 (from lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading sacrebleu-2.6.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu]) (1.6.1)\n",
            "Collecting sqlitedict (from lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tqdm-multiprocess (from lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.12/dist-packages (from lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu]) (0.25.0)\n",
            "Collecting word2number (from lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.12/dist-packages (from lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu]) (10.8.0)\n",
            "Collecting mlflow-skinny==3.8.1 (from mlflow>=3.1->oumi[gpu])\n",
            "  Downloading mlflow_skinny-3.8.1-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-tracing==3.8.1 (from mlflow>=3.1->oumi[gpu])\n",
            "  Downloading mlflow_tracing-3.8.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting Flask-CORS<7 (from mlflow>=3.1->oumi[gpu])\n",
            "  Downloading flask_cors-6.0.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow>=3.1->oumi[gpu]) (3.1.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow>=3.1->oumi[gpu]) (1.17.2)\n",
            "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow>=3.1->oumi[gpu]) (43.0.3)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow>=3.1->oumi[gpu])\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow>=3.1->oumi[gpu])\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow>=3.1->oumi[gpu])\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting huey<3,>=2.5.0 (from mlflow>=3.1->oumi[gpu])\n",
            "  Downloading huey-2.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow>=3.1->oumi[gpu]) (3.10.0)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/dist-packages (from mlflow>=3.1->oumi[gpu]) (1.16.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow>=3.1->oumi[gpu]) (2.0.45)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (6.2.4)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (3.1.2)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu])\n",
            "  Downloading databricks_sdk-0.78.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (0.123.10)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (3.1.46)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (8.7.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (1.37.0)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (1.2.1)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (0.5.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.3->oumi[gpu]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.3->oumi[gpu]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.3->oumi[gpu]) (2025.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect<4.10,>=4.9->oumi[gpu]) (0.7.0)\n",
            "Requirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from pycares<5.0.0->oumi[gpu]) (2.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.13,>=2.11->oumi[gpu]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.13,>=2.11->oumi[gpu]) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.13,>=2.11->oumi[gpu]) (0.4.2)\n",
            "Requirement already satisfied: urllib3<3.0,>=1.25.10 in /usr/local/lib/python3.12/dist-packages (from responses<0.26,>=0.25->oumi[gpu]) (2.5.0)\n",
            "Requirement already satisfied: wheel<0.46.0 in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (0.45.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (75.2.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (24.1.2)\n",
            "Collecting click<8.3.0 (from oumi[gpu])\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting colorama (from skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (3.1.6)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (4.26.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (3.6.1)\n",
            "Collecting pendulum (from skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading pendulum-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: PrettyTable>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (3.17.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (13.9.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (0.9.0)\n",
            "Requirement already satisfied: pulp in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (3.3.0)\n",
            "Collecting ijson (from skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading ijson-3.4.0.post0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (3.11.5)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (0.0.21)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (0.28.1)\n",
            "Collecting setproctitle (from skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
            "Collecting psycopg2-binary (from skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (0.22.1)\n",
            "Collecting asyncpg (from skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading asyncpg-0.31.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting casbin (from skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading casbin-1.43.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting sqlalchemy_adapter (from skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading sqlalchemy_adapter-1.9.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: prometheus_client>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (0.23.1)\n",
            "Collecting passlib (from skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading passlib-1.7.4-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting bcrypt==4.0.1 (from skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pyjwt in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (2.10.1)\n",
            "Collecting types-paramiko (from skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading types_paramiko-4.0.0.20250822-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from skypilot<0.12,>=0.10.2->oumi[gpu]) (4.12.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.21,>=2.20->oumi[gpu]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.21,>=2.20->oumi[gpu]) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.21,>=2.20->oumi[gpu]) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.21,>=2.20->oumi[gpu]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.21,>=2.20->oumi[gpu]) (3.1.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.6->oumi[gpu]) (1.14.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch<2.9.0,>=2.6->oumi[gpu])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch<2.9.0,>=2.6->oumi[gpu])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch<2.9.0,>=2.6->oumi[gpu])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch<2.9.0,>=2.6->oumi[gpu])\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch<2.9.0,>=2.6->oumi[gpu])\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch<2.9.0,>=2.6->oumi[gpu])\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch<2.9.0,>=2.6->oumi[gpu])\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch<2.9.0,>=2.6->oumi[gpu])\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch<2.9.0,>=2.6->oumi[gpu])\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch<2.9.0,>=2.6->oumi[gpu])\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch<2.9.0,>=2.6->oumi[gpu])\n",
            "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch<2.9.0,>=2.6->oumi[gpu])\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch<2.9.0,>=2.6->oumi[gpu])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch<2.9.0,>=2.6->oumi[gpu])\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton>=2.3.1 (from liger-kernel<0.7,>=0.6->oumi[gpu])\n",
            "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<4.58,>=4.57->oumi[gpu]) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<4.58,>=4.57->oumi[gpu]) (0.22.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<0.36.0->oumi[gpu]) (0.16.0)\n",
            "Collecting codetiming (from verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading codetiming-1.4.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting hydra-core (from verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting numpy<2.4,>=1.26 (from oumi[gpu])\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow>=15.0.0 (from datasets<5,>=3.2->oumi[gpu])\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting pylatexenc (from verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ray>=2.41.0 (from ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.12/dist-packages (from verl<0.6,>=0.5->oumi[gpu]) (0.11.0)\n",
            "Collecting tensordict!=0.9.0,<=0.9.1,>=0.8.0 (from verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading tensordict-0.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from vllm<0.11,>=0.10->oumi[gpu]) (0.2.1)\n",
            "Collecting blake3 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm<0.11,>=0.10->oumi[gpu]) (9.0.0)\n",
            "Requirement already satisfied: openai>=1.99.1 in /usr/local/lib/python3.12/dist-packages (from vllm<0.11,>=0.10->oumi[gpu]) (2.14.0)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm<0.11,>=0.10->oumi[gpu]) (0.12.0)\n",
            "Collecting lm-format-enforcer==0.11.3 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading lm_format_enforcer-0.11.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.11 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting outlines_core==0.2.11 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting diskcache==5.6.3 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting lark==1.2.2 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.23 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading xgrammar-0.1.23-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting partial-json-parser (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading partial_json_parser-0.2.1.1.post7-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm<0.11,>=0.10->oumi[gpu]) (26.2.1)\n",
            "Collecting msgspec (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting gguf>=0.13.0 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting mistral_common>=1.8.2 (from mistral_common[audio,image]>=1.8.2->vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading mistral_common-1.8.8-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm<0.11,>=0.10->oumi[gpu]) (4.12.0.88)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm<0.11,>=0.10->oumi[gpu]) (1.17.0)\n",
            "Collecting setuptools (from skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm<0.11,>=0.10->oumi[gpu]) (0.8.1)\n",
            "Collecting compressed-tensors==0.11.0 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading compressed_tensors-0.11.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.19.0 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading depyf-0.19.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: watchfiles in /usr/local/lib/python3.12/dist-packages (from vllm<0.11,>=0.10->oumi[gpu]) (1.1.1)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm<0.11,>=0.10->oumi[gpu]) (4.0.0)\n",
            "Collecting ninja (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pybase64 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Collecting cbor2 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading cbor2-5.8.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting openai-harmony>=0.0.3 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Collecting numba==0.61.2 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting torchaudio==2.8.0 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting xformers==0.0.32.post1 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.11.0->vllm<0.11,>=0.10->oumi[gpu]) (2.4.7)\n",
            "Collecting astor (from depyf==0.19.0->vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.11.3->vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb<0.24,>=0.21->oumi[gpu]) (4.5.1)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb<0.24,>=0.21->oumi[gpu]) (2.49.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->oumi[gpu]) (1.5.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow>=3.1->oumi[gpu]) (1.3.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.5.0->pycares<5.0.0->oumi[gpu]) (2.23)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (0.0.4)\n",
            "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading fastapi_cli-0.0.20-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow>=3.1->oumi[gpu]) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow>=3.1->oumi[gpu]) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow>=3.1->oumi[gpu]) (3.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (4.0.12)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=3.1->oumi[gpu])\n",
            "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=3.1->oumi[gpu])\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->skypilot<0.12,>=0.10.2->oumi[gpu]) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->skypilot<0.12,>=0.10.2->oumi[gpu]) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->skypilot<0.12,>=0.10.2->oumi[gpu]) (3.11)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate<2.0,>=1.10.0->oumi[gpu]) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow>=3.1->oumi[gpu]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow>=3.1->oumi[gpu]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow>=3.1->oumi[gpu]) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow>=3.1->oumi[gpu]) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow>=3.1->oumi[gpu]) (3.3.1)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading pydantic_extra_types-2.11.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->skypilot<0.12,>=0.10.2->oumi[gpu]) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->skypilot<0.12,>=0.10.2->oumi[gpu]) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->skypilot<0.12,>=0.10.2->oumi[gpu]) (0.30.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm<0.11,>=0.10->oumi[gpu]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm<0.11,>=0.10->oumi[gpu]) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm<0.11,>=0.10->oumi[gpu]) (1.3.1)\n",
            "INFO: pip is looking at multiple versions of opencv-python-headless to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-python-headless>=4.11.0 (from vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from PrettyTable>=2.0.0->skypilot<0.12,>=0.10.2->oumi[gpu]) (0.2.14)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.41.0->ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu]) (1.1.2)\n",
            "Collecting cupy-cuda12x (from ray[cgraph]>=2.48.0->vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading cupy_cuda12x-13.6.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting aiohttp_cors (from ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading colorful-0.5.8-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting py-spy>=0.4.0 (from ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading py_spy-0.4.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (510 bytes)\n",
            "Collecting opencensus (from ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting opentelemetry-exporter-prometheus (from ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading opentelemetry_exporter_prometheus-0.60b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu]) (7.5.0)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading virtualenv-20.36.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<5,>=3.2->oumi[gpu]) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->skypilot<0.12,>=0.10.2->oumi[gpu]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->skypilot<0.12,>=0.10.2->oumi[gpu]) (2.19.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu]) (3.9.1)\n",
            "Collecting portalocker (from sacrebleu>=1.5.0->lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu]) (6.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu]) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu]) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow>=3.1->oumi[gpu]) (3.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<2.9.0,>=2.6->oumi[gpu]) (1.3.0)\n",
            "Collecting pyvers<0.2.0,>=0.1.0 (from tensordict!=0.9.0,<=0.9.1,>=0.8.0->verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading pyvers-0.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<0.36.0,>=0.33.0->skypilot<0.12,>=0.10.2->oumi[gpu]) (0.7.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<0.36.0,>=0.33.0->skypilot<0.12,>=0.10.2->oumi[gpu]) (0.22.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]<0.36.0,>=0.33.0->skypilot<0.12,>=0.10.2->oumi[gpu]) (15.0.1)\n",
            "Collecting simpleeval>=0.9.11 (from casbin->skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading simpleeval-1.0.3-py3-none-any.whl.metadata (17 kB)\n",
            "INFO: pip is looking at multiple versions of hydra-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting hydra-core (from verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading hydra_core-1.3.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "  Downloading hydra_core-1.3.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core->verl<0.6,>=0.5->oumi[gpu]) (4.9.3)\n",
            "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu])\n",
            "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting pycasbin>=2.0.0 (from sqlalchemy_adapter->skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading pycasbin-2.7.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (2.43.0)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading rich_toolkit-0.17.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading fastapi_cloud_cli-0.11.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (5.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->skypilot<0.12,>=0.10.2->oumi[gpu]) (0.1.2)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval<0.5.0,>=0.4->lm_eval[wandb]<0.5.0,>=0.4->oumi[gpu]) (5.2.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (0.58b0)\n",
            "Collecting wcmatch>=10.1 (from pycasbin>=2.0.0->sqlalchemy_adapter->skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading wcmatch-10.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm<0.11,>=0.10->oumi[gpu]) (0.13.1)\n",
            "Requirement already satisfied: soxr>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm<0.11,>=0.10->oumi[gpu]) (1.0.0)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from opencensus->ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu]) (2.29.0)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu])\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "INFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-exporter-prometheus (from ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading opentelemetry_exporter_prometheus-0.60b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu])\n",
            "  Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-prometheus (from ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading opentelemetry_exporter_prometheus-0.59b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu])\n",
            "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-prometheus (from ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu])\n",
            "  Downloading opentelemetry_exporter_prometheus-0.58b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open->ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu]) (2.0.1)\n",
            "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting fastar>=0.8.0 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm<0.11,>=0.10->oumi[gpu])\n",
            "  Downloading fastar-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu]) (1.72.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.41.0->verl<0.6,>=0.5->oumi[gpu]) (1.27.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (4.9.1)\n",
            "Collecting bracex>=2.1.1 (from wcmatch>=10.1->pycasbin>=2.0.0->sqlalchemy_adapter->skypilot<0.12,>=0.10.2->oumi[gpu])\n",
            "  Downloading bracex-2.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=3.1->oumi[gpu]) (0.6.1)\n",
            "Downloading omegaconf-2.4.0.dev4-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading aioresponses-0.7.8-py2.py3-none-any.whl (12 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hdrhistogram-0.10.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading liger_kernel-0.6.4-py3-none-any.whl (230 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m230.6/230.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_eval-0.4.9.2-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow-3.8.1-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_tracing-3.8.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_ml_py-13.590.44-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-6.33.4-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycares-4.11.0-cp312-cp312-manylinux_2_28_x86_64.whl (641 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m641.1/641.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading responses-0.25.8-py3-none-any.whl (34 kB)\n",
            "Downloading skypilot-0.11.1-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m546.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m840.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.26.2-py3-none-any.whl (518 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading verl-0.5.0-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m895.3/895.3 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl (436.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m436.4/436.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.11.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading depyf-0.19.0-py3-none-any.whl (39 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.11.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.23-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading oumi-0.6.0-py3-none-any.whl (721 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m721.9/721.9 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_cors-6.0.2-py3-none-any.whl (13 kB)\n",
            "Downloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huey-2.6.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.8.8-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pbr-7.0.3-py2.py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl (72.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.4/72.4 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.6.0-py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-79.0.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensordict-0.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (430 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m430.2/430.2 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncpg-0.31.0-cp312-cp312-manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (388 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading casbin-1.43.0-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m475.1/475.1 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cbor2-5.8.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (285 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m285.4/285.4 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading codetiming-1.4.0-py3-none-any.whl (7.2 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading hydra_core-1.3.0-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.8/153.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ijson-3.4.0.post0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (149 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post7-py3-none-any.whl (10 kB)\n",
            "Downloading passlib-1.7.4-py2.py3-none-any.whl (525 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m525.6/525.6 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pendulum-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (351 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m351.2/351.2 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (32 kB)\n",
            "Downloading sqlalchemy_adapter-1.9.0-py3-none-any.whl (9.9 kB)\n",
            "Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading types_paramiko-4.0.0.20250822-py3-none-any.whl (38 kB)\n",
            "Downloading databricks_sdk-0.78.0-py3-none-any.whl (780 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m780.5/780.5 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
            "Downloading fastapi_cli-0.0.20-py3-none-any.whl (12 kB)\n",
            "Downloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
            "Downloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n",
            "Downloading py_spy-0.4.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycasbin-2.7.1-py3-none-any.whl (476 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m476.1/476.1 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.11.0-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyvers-0.1.0-py3-none-any.whl (10 kB)\n",
            "Downloading simpleeval-1.0.3-py3-none-any.whl (15 kB)\n",
            "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
            "Downloading virtualenv-20.36.1-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.8.1-py3-none-any.whl (25 kB)\n",
            "Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading colorful-0.5.8-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.3/201.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cupy_cuda12x-13.6.0-cp312-cp312-manylinux2014_x86_64.whl (112.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m112.9/112.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_prometheus-0.58b0-py3-none-any.whl (13 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Downloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi_cloud_cli-0.11.0-py3-none-any.whl (26 kB)\n",
            "Downloading fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_toolkit-0.17.1-py3-none-any.whl (31 kB)\n",
            "Downloading wcmatch-10.1-py3-none-any.whl (39 kB)\n",
            "Downloading bracex-2.6-py3-none-any.whl (11 kB)\n",
            "Downloading fastar-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (821 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m821.6/821.6 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (959 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge-score, pylatexenc, sqlitedict, word2number\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=a80e30860f1ee152a98807c94d3bb42048e4840980c404b7943ad6be4ad4bc52\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136817 sha256=206bbf75d1567d84f46201113f48b862732db0753677ec1c7bedb90fce9767b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/3e/78/fa1588c1ae991bbfd814af2bcac6cef7a178beee1939180d46\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=4b693df1fb658e5897ac5e804999fd9018eca58ba0d47d5c9f7b20cf54118e98\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/6f/21/fc016aef45ffcabe27129a2252f061387cbf278d2086225a64\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=b01d164f108d69ccdc86773f17f19450c8289b15c8e91b6885f495708c3801ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/79/fb/d25928e599c7e11fe4e00d32048cd74933f34a74c633d2aea6\n",
            "Successfully built rouge-score pylatexenc sqlitedict word2number\n",
            "Installing collected packages: word2number, torchao, sqlitedict, pylatexenc, py-spy, passlib, opencensus-context, nvidia-ml-py, nvidia-cusparselt-cu12, huey, fastrlock, distlib, colorful, virtualenv, tcolorpy, simpleeval, setuptools, setproctitle, rignore, pyvers, pycountry, pybind11, pybase64, pyarrow, psycopg2-binary, protobuf, portalocker, pathvalidate, partial-json-parser, outlines_core, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, msgspec, mbstrdecoder, llvmlite, llguidance, lark, jsonlines, interegular, ijson, gunicorn, graphql-core, fastar, dnspython, diskcache, colorama, codetiming, click, cbor2, bracex, blake3, bcrypt, backoff, asyncpg, astor, wcmatch, uvicorn, typepy, triton, tqdm-multiprocess, tensorboard, sacrebleu, responses, pycares, pendulum, pbr, pandas, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, numba, hydra-core, graphql-relay, gguf, email-validator, docker, depyf, cupy-cuda12x, casbin, types-paramiko, rouge-score, rich-toolkit, pydantic-extra-types, pycasbin, prometheus-fastapi-instrumentator, openai-harmony, nvidia-cusolver-cu12, lm-format-enforcer, hdrhistogram, graphene, Flask-CORS, databricks-sdk, aioresponses, aiohttp_cors, torch, sqlalchemy_adapter, ray, opencensus, fastapi-cloud-cli, fastapi-cli, DataProperty, xgrammar, xformers, torchvision, torchaudio, tensordict, tabledata, skypilot, opentelemetry-exporter-prometheus, mlflow-tracing, mlflow-skinny, mistral_common, liger-kernel, evaluate, compressed-tensors, bitsandbytes, trl, pytablewriter, peft, mlflow, verl, lm_eval, vllm, oumi\n",
            "  Attempting uninstall: torchao\n",
            "    Found existing installation: torchao 0.10.0\n",
            "    Uninstalling torchao-0.10.0:\n",
            "      Successfully uninstalled torchao-0.10.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: omegaconf\n",
            "    Found existing installation: omegaconf 2.3.0\n",
            "    Uninstalling omegaconf-2.3.0:\n",
            "      Successfully uninstalled omegaconf-2.3.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.29.2\n",
            "    Uninstalling nvidia-nccl-cu12-2.29.2:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.29.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: lark\n",
            "    Found existing installation: lark 1.3.1\n",
            "    Uninstalling lark-1.3.1:\n",
            "      Successfully uninstalled lark-1.3.1\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.1\n",
            "    Uninstalling click-8.3.1:\n",
            "      Successfully uninstalled click-8.3.1\n",
            "  Attempting uninstall: uvicorn\n",
            "    Found existing installation: uvicorn 0.40.0\n",
            "    Uninstalling uvicorn-0.40.0:\n",
            "      Successfully uninstalled uvicorn-0.40.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.19.0\n",
            "    Uninstalling tensorboard-2.19.0:\n",
            "      Successfully uninstalled tensorboard-2.19.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cpu\n",
            "    Uninstalling torch-2.9.0+cpu:\n",
            "      Successfully uninstalled torch-2.9.0+cpu\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.24.0+cpu\n",
            "    Uninstalling torchvision-0.24.0+cpu:\n",
            "      Successfully uninstalled torchvision-0.24.0+cpu\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.9.0+cpu\n",
            "    Uninstalling torchaudio-2.9.0+cpu:\n",
            "      Successfully uninstalled torchaudio-2.9.0+cpu\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.18.0\n",
            "    Uninstalling peft-0.18.0:\n",
            "      Successfully uninstalled peft-0.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.4 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.4 which is incompatible.\n",
            "tensorflow 2.19.0 requires tensorboard~=2.19.0, but you have tensorboard 2.20.0 which is incompatible.\n",
            "pytensor 2.36.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed DataProperty-1.1.0 Flask-CORS-6.0.2 aiohttp_cors-0.8.1 aioresponses-0.7.8 astor-0.8.1 asyncpg-0.31.0 backoff-2.2.1 bcrypt-4.0.1 bitsandbytes-0.48.2 blake3-1.0.8 bracex-2.6 casbin-1.43.0 cbor2-5.8.0 click-8.1.8 codetiming-1.4.0 colorama-0.4.6 colorful-0.5.8 compressed-tensors-0.11.0 cupy-cuda12x-13.6.0 databricks-sdk-0.78.0 depyf-0.19.0 diskcache-5.6.3 distlib-0.4.0 dnspython-2.8.0 docker-7.1.0 email-validator-2.3.0 evaluate-0.4.6 fastapi-cli-0.0.20 fastapi-cloud-cli-0.11.0 fastar-0.8.0 fastrlock-0.8.3 gguf-0.17.1 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 gunicorn-23.0.0 hdrhistogram-0.10.3 huey-2.6.0 hydra-core-1.3.0 ijson-3.4.0.post0 interegular-0.3.3 jsonlines-4.0.0 lark-1.2.2 liger-kernel-0.6.4 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.11.3 lm_eval-0.4.9.2 mbstrdecoder-1.1.4 mistral_common-1.8.8 mlflow-3.8.1 mlflow-skinny-3.8.1 mlflow-tracing-3.8.1 msgspec-0.20.0 ninja-1.13.0 numba-0.61.2 numpy-1.26.4 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-ml-py-13.590.44 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 omegaconf-2.4.0.dev4 openai-harmony-0.0.8 opencensus-0.11.4 opencensus-context-0.1.3 opencv-python-headless-4.11.0.86 opentelemetry-exporter-prometheus-0.58b0 oumi-0.6.0 outlines_core-0.2.11 pandas-2.3.3 partial-json-parser-0.2.1.1.post7 passlib-1.7.4 pathvalidate-3.3.1 pbr-7.0.3 peft-0.17.1 pendulum-3.1.0 portalocker-3.2.0 prometheus-fastapi-instrumentator-7.1.0 protobuf-6.33.4 psycopg2-binary-2.9.11 py-spy-0.4.1 pyarrow-22.0.0 pybase64-1.4.3 pybind11-3.0.1 pycares-4.11.0 pycasbin-2.7.1 pycountry-24.6.1 pydantic-extra-types-2.11.0 pylatexenc-2.10 pytablewriter-1.2.1 pyvers-0.1.0 ray-2.53.0 responses-0.25.8 rich-toolkit-0.17.1 rignore-0.7.6 rouge-score-0.1.2 sacrebleu-2.6.0 setproctitle-1.3.7 setuptools-79.0.1 simpleeval-1.0.3 skypilot-0.11.1 sqlalchemy_adapter-1.9.0 sqlitedict-2.1.0 tabledata-1.3.4 tcolorpy-0.1.7 tensorboard-2.20.0 tensordict-0.9.1 torch-2.8.0 torchao-0.14.1 torchaudio-2.8.0 torchvision-0.23.0 tqdm-multiprocess-0.0.11 triton-3.4.0 trl-0.26.2 typepy-1.3.4 types-paramiko-4.0.0.20250822 uvicorn-0.35.0 verl-0.5.0 virtualenv-20.36.1 vllm-0.10.2 wcmatch-10.1 word2number-1.1 xformers-0.0.32.post1 xgrammar-0.1.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "google",
                  "numpy",
                  "pandas",
                  "pyarrow",
                  "pydevd_plugins",
                  "torch",
                  "torchgen"
                ]
              },
              "id": "c7a956df91c345cfa9f2aec37fe2859c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%pip install oumi[gpu] jsonlines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUkYNSMaaDWy"
      },
      "source": [
        "## Creating our working directory\n",
        "For our experiments, we'll use the following folder to save the model, training artifacts, and our working configs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaQDczF8aDWz"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJjYjOq5aDW0"
      },
      "source": [
        "## Setup the environment\n",
        "\n",
        "We'll need to set the following environment variables:\n",
        "- [Optional] HF_TOKEN: Your [HuggingFace](https://huggingface.co/docs/hub/en/security-tokens) token, in case you want to access a private model.\n",
        "- [Optional] WANDB_API_KEY: Your [wandb](https://wandb.ai) token, in case you want to log your experiments to wandb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNyOaORMaDW0"
      },
      "source": [
        "# Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcbvEulQaDW1"
      },
      "source": [
        "## Model Download\n",
        "\n",
        "For our purposes it will be much faster if we download our models first.\n",
        "\n",
        "We'll use the `hf_transfer` package to download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTJiWe85aDW1"
      },
      "outputs": [],
      "source": [
        "!pip install hf_transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcXefSOPaDW2"
      },
      "outputs": [],
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=1 \\\n",
        "    hf download deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n",
        "    --exclude original/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jy7TSvNaDW2"
      },
      "outputs": [],
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=1 \\\n",
        "    hf download deepseek-ai/DeepSeek-R1-Distill-Llama-70B \\\n",
        "    --exclude original/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ben0ivlmaDW3"
      },
      "source": [
        "## Baseline Evals\n",
        "\n",
        "Before we can improve our small model, we should measure how well it performs on a benchmark compared to the larger model.\n",
        "\n",
        "The below code will run the MMLU PRO Math task from LM Harness.\n",
        "\n",
        "Note that this will take some time, so we've recorded our results below for your convenience:\n",
        "\n",
        "| Model | MMLU Pro Math Accuracy |\n",
        "|-------|------------------------|\n",
        "| R1 Distill 1.5B | 38.49% +- 1.32% |\n",
        "| R1 Distill 70B | 61.07% +- 1.33% |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlpQTGspaDW3"
      },
      "source": [
        "### Run Evals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvhB6HeIaDW4"
      },
      "outputs": [],
      "source": [
        "%%writefile $tutorial_dir/eval_small.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  # shard_for_eval: True # Uncomment this line for multi-gpu setups.\n",
        "\n",
        "\n",
        "tasks:\n",
        "  - evaluation_backend: lm_harness\n",
        "    task_name: mmlu_pro_math\n",
        "\n",
        "output_dir: \"distillation_tutorial/output/evaluation\"\n",
        "generation:\n",
        "  batch_size: 1 # LM Harness recommends BS=1 for reproducibility.\n",
        "  # batch_size: 256  # Replace with 256 for 8xA100-80GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7WVZiRHaDW4"
      },
      "outputs": [],
      "source": [
        "!oumi evaluate -c \"$tutorial_dir/eval_small.yaml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBdRoqj0aDW4"
      },
      "outputs": [],
      "source": [
        "%%writefile $tutorial_dir/eval_large.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  shard_for_eval: True\n",
        "\n",
        "\n",
        "tasks:\n",
        "  - evaluation_backend: lm_harness\n",
        "    task_name: mmlu_pro_math\n",
        "\n",
        "output_dir: \"distillation_tutorial/output/evaluation\"\n",
        "generation:\n",
        "  batch_size: 1 # LM Harness recommends BS=1 for reproducibility.\n",
        "  # batch_size: 64  # Replace with 64 for 8xA100-80GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcjrjwZNaDW5"
      },
      "outputs": [],
      "source": [
        "!oumi evaluate -c \"$tutorial_dir/eval_large.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMXbMbzVaDW5"
      },
      "source": [
        "## Prepare Inference Data\n",
        "\n",
        "Now that we've set our baseline numbers, let's prepare the training data we'll use to improve 1.5B.\n",
        "\n",
        "Given our goal is to improve MMLU Pro Math performance, we should ideally pick data that's similar.\n",
        "\n",
        "`meta-math/MetaMathQA` is a good choice as it avoids test set contamination while being similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VXR7KCcaDW5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import datasets\n",
        "import torch\n",
        "\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.core.types import Conversation, Message, Role\n",
        "from oumi.inference import VLLMInferenceEngine\n",
        "\n",
        "# This is needed for vLLM to use multiple GPUs in a notebook.\n",
        "# If you're not running in a notebook, you can ignore this.\n",
        "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkqMDKFpaDW5"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.load_dataset(\n",
        "    \"meta-math/MetaMathQA\",\n",
        "    revision=\"aa4f34d\",\n",
        "    split=\"train[:10000]\",  # We'll focus only on the first 10k samples.\n",
        ")\n",
        "\n",
        "data = [sample[\"query\"] for sample in dataset]\n",
        "print(data[0])\n",
        "print(\"num samples: \", len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HE4vMNYaDW6"
      },
      "outputs": [],
      "source": [
        "conversations = [\n",
        "    Conversation(\n",
        "        messages=[\n",
        "            Message(role=Role.USER, content=prompt),\n",
        "        ]\n",
        "    )\n",
        "    for prompt in data\n",
        "]\n",
        "print(conversations[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdnTz2tuaDW6"
      },
      "source": [
        "## Run Inference\n",
        "\n",
        "Now that our data is in the right format for collecting responses, let's go ahead and run inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6X7AqSudaDW6"
      },
      "outputs": [],
      "source": [
        "%%writefile $tutorial_dir/infer_large.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  model_max_length: 8192\n",
        "\n",
        "generation:\n",
        "  max_new_tokens: 8192"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pezrIFtCaDW6"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Download, and load the model in memory\n",
        "# This may take a while, depending on your internet speed.\n",
        "# The inference engine only needs to be loaded once and can be\n",
        "# reused for multiple conversations.\n",
        "config_path = f\"{tutorial_dir}/infer_large.yaml\"\n",
        "config = InferenceConfig.from_yaml(config_path)\n",
        "\n",
        "inference_engine = VLLMInferenceEngine(\n",
        "    config.model,\n",
        "    tensor_parallel_size=torch.cuda.device_count(),  # use all available GPUs\n",
        "    # Enable prefix caching for vLLM.\n",
        "    # This is key for performance when running prompts with a long prefix,\n",
        "    # such as judging or conversations with large system prompts\n",
        "    # or few-shot examples.\n",
        "    enable_prefix_caching=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9Nog6FIaDW7"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "print(f\"Running inference for {len(conversations)} conversations\")\n",
        "\n",
        "generations = inference_engine.infer(\n",
        "    input=conversations,\n",
        "    inference_config=config,\n",
        ")\n",
        "print(generations[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJdc-HXtaDW7"
      },
      "source": [
        "## Prepare Training Data\n",
        "\n",
        "Now that we've finished collecting responses, let's go ahead and prepare the data for training and save it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZf-WWdHaDW7"
      },
      "outputs": [],
      "source": [
        "conversation_dicts = [c.to_dict() for c in generations]\n",
        "print(conversation_dicts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDwhv-lEaDW7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataframe = pd.DataFrame(conversation_dicts)\n",
        "print(dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSYPiz4qaDW7"
      },
      "outputs": [],
      "source": [
        "dataframe.to_json(f\"{tutorial_dir}/math_train_10k.jsonl\", orient=\"records\", lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBTnFZ-NaDW8"
      },
      "source": [
        "## Run Distillation\n",
        "\n",
        "Now that the data is ready, we can begin distilling the model. For this form of distillation, we will be fully fine-tuning the model with supervised fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHwF-eZGaDW8"
      },
      "outputs": [],
      "source": [
        "%%writefile $tutorial_dir/train.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "  trust_remote_code: true\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  device_map: \"auto\"\n",
        "\n",
        "data:\n",
        "  train:\n",
        "    datasets:\n",
        "      - dataset_name: \"text_sft_jsonl\"\n",
        "        dataset_path: \"./distillation_tutorial/math_train_10k.jsonl\"\n",
        "        split: \"train\"\n",
        "        shuffle: True\n",
        "        seed: 42\n",
        "    seed: 42\n",
        "\n",
        "training:\n",
        "  output_dir: \"distillation_tutorial/output/finetune\"\n",
        "\n",
        "  # For a single GPU, the following gives us a batch size of 16\n",
        "  # If training with multiple GPUs, feel free to reduce gradient_accumulation_steps\n",
        "  per_device_train_batch_size: 2\n",
        "  gradient_accumulation_steps: 8  # Reduce this to 1 for 8xA100-80GB GPUs\n",
        "\n",
        "  # ***NOTE***\n",
        "  # We set it to 10 steps to first verify that it works\n",
        "  # Comment out the line below to have it train for 1 full epoch (all the data) instead.\n",
        "  # Note: 1 full epoch will take about 13 minutes on 8xA100-80GB.\n",
        "  max_steps: 10\n",
        "\n",
        "  num_train_epochs: 1\n",
        "  learning_rate: 1e-4\n",
        "  warmup_ratio: 0.1\n",
        "  logging_steps: 10\n",
        "  save_steps: 0\n",
        "  max_grad_norm: 10\n",
        "  weight_decay: 0.01\n",
        "\n",
        "\n",
        "  trainer_type: \"TRL_SFT\"\n",
        "  optimizer: \"adamw_torch_fused\"\n",
        "  enable_gradient_checkpointing: True\n",
        "  gradient_checkpointing_kwargs:\n",
        "    use_reentrant: False\n",
        "  ddp_find_unused_parameters: False\n",
        "  dataloader_num_workers: \"auto\"\n",
        "  dataloader_prefetch_factor: 32\n",
        "  empty_device_cache_steps: 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3mem0IMaDW8"
      },
      "source": [
        "### Single GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rOWo2S4aDW8"
      },
      "outputs": [],
      "source": [
        "!oumi train -c \"$tutorial_dir/train.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zny83bTZaDW8"
      },
      "source": [
        "### Multi-GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv84Ym1laDW8"
      },
      "outputs": [],
      "source": [
        "!oumi distributed torchrun -m oumi train -c \"$tutorial_dir/train.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCRI2gr2aDW9"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "Now that we have a new distilled model, let's evaluate it on the same benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixrsI_HlaDW9"
      },
      "outputs": [],
      "source": [
        "%%writefile $tutorial_dir/eval_small_fft.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"./distillation_tutorial/output/finetune/\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  shard_for_eval: True\n",
        "\n",
        "\n",
        "tasks:\n",
        "  - evaluation_backend: lm_harness\n",
        "    task_name: mmlu_pro_math\n",
        "\n",
        "output_dir: \"distillation_tutorial/output/evaluation\"\n",
        "generation:\n",
        "  batch_size: 1 # LM Harness recommends BS=1 for reproducibility.\n",
        "  # batch_size: 256  # Replace with 256 for 8xA100-80GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7-7NIppaDW9"
      },
      "outputs": [],
      "source": [
        "!oumi evaluate -c \"$tutorial_dir/eval_small_fft.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1g9DBz0aDXD"
      },
      "source": [
        "## Results\n",
        "\n",
        "After we finetuned the model following the steps above, we achieved the following results:\n",
        "\n",
        "| Model           | Accuracy        |\n",
        "|-----------------|-----------------|\n",
        "| R1 Distill 1.5B | 38.49% +- 1.32% |\n",
        "| Oumi R1 Distill 1.5B | 42.41% +- 1.34% |\n",
        "| R1 Distill 70B  | 61.07% +- 1.33% |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQU27obpaDXD"
      },
      "source": [
        "# ðŸ§­ What's Next?\n",
        "\n",
        "Congrats on finishing this notebook! Feel free to check out our other [notebooks](https://github.com/oumi-ai/oumi/tree/main/notebooks) in the [Oumi GitHub](https://github.com/oumi-ai/oumi), and give us a star! You can also join the Oumi community over on [Discord](https://discord.gg/oumi).\n",
        "\n",
        "ðŸ“° Want to keep up with news from Oumi? Subscribe to our [Substack](https://blog.oumi.ai/) and [Youtube](https://www.youtube.com/@Oumi_AI)!\n",
        "\n",
        "âš¡ Interested in building custom AI in hours, not months? Apply to get [early access](https://oumi-ai.typeform.com/early-access) to the Oumi Platform, or [chat with us](https://calendly.com/d/ctcx-nps-47m/chat-with-us-get-early-access-to-the-oumi-platform) to learn more!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "441bd2a8"
      },
      "source": [
        "# Task\n",
        "Generate and save the `distillation_tutorial/math_train_10k.jsonl` file by loading the \"meta-math/MetaMathQA\" dataset, preparing conversations, configuring the VLLM inference engine with the \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\" model, and performing inference to produce the conversational data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d4fe7cf"
      },
      "source": [
        "## Carica il dataset di MetaMathQA\n",
        "\n",
        "### Subtask:\n",
        "Carica il dataset 'meta-math/MetaMathQA' per ottenere i dati di input per l'inferenza.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60d79e0f"
      },
      "source": [
        "## Carica il dataset di MetaMathQA\n",
        "\n",
        "### Subtask:\n",
        "Carica il dataset 'meta-math/MetaMathQA' per ottenere i dati di input per l'inferenza.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "348cdcc2"
      },
      "source": [
        "**Reasoning**:\n",
        "Next, I will load the 'meta-math/MetaMathQA' dataset, extract the query data, and print the first sample along with the total number of samples to verify the data loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea457bab"
      },
      "source": [
        "dataset = datasets.load_dataset(\n",
        "    \"meta-math/MetaMathQA\",\n",
        "    revision=\"aa4f34d\",\n",
        "    split=\"train[:10000]\",  # We'll focus only on the first 10k samples.\n",
        ")\n",
        "\n",
        "data = [sample[\"query\"] for sample in dataset]\n",
        "print(data[0])\n",
        "print(\"num samples: \", len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5337e5e1"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the 'datasets' library was not imported. I need to import the 'datasets' library before using `datasets.load_dataset`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75adf55d"
      },
      "source": [
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset(\n",
        "    \"meta-math/MetaMathQA\",\n",
        "    revision=\"aa4f34d\",\n",
        "    split=\"train[:10000]\",  # We'll focus only on the first 10k samples.\n",
        ")\n",
        "\n",
        "data = [sample[\"query\"] for sample in dataset]\n",
        "print(data[0])\n",
        "print(\"num samples: \", len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "713cd01a"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `data` (queries) has been successfully loaded, I need to convert it into a list of `Conversation` objects. This aligns with the overall task of preparing inference data, where each query will be treated as a user message in a conversation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dd8b239"
      },
      "source": [
        "conversations = [\n",
        "    Conversation(\n",
        "        messages=[\n",
        "            Message(role=Role.USER, content=prompt),\n",
        "        ]\n",
        "    )\n",
        "    for prompt in data\n",
        "]\n",
        "print(conversations[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb03e464"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `NameError` because the `Conversation`, `Message`, and `Role` classes were not defined. These classes need to be imported from `oumi.core.types` to be used.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55b2c6ab"
      },
      "source": [
        "from oumi.core.types import Conversation, Message, Role\n",
        "\n",
        "conversations = [\n",
        "    Conversation(\n",
        "        messages=[\n",
        "            Message(role=Role.USER, content=prompt),\n",
        "        ]\n",
        "    )\n",
        "    for prompt in data\n",
        "]\n",
        "print(conversations[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f50e7edd"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the conversation objects are prepared, I will load the inference configuration from the YAML file and initialize the VLLM inference engine, which will be used to generate responses from the large language model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57a62613"
      },
      "source": [
        "import os\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.inference import VLLMInferenceEngine\n",
        "\n",
        "# This is needed for vLLM to use multiple GPUs in a notebook.\n",
        "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
        "\n",
        "config_path = f\"{tutorial_dir}/infer_large.yaml\"\n",
        "config = InferenceConfig.from_yaml(config_path)\n",
        "\n",
        "inference_engine = VLLMInferenceEngine(\n",
        "    config.model,\n",
        "    tensor_parallel_size=torch.cuda.device_count(),\n",
        "    enable_prefix_caching=True,\n",
        ")\n",
        "print(\"VLLMInferenceEngine initialized successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "641f47e9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `tutorial_dir` and `torch` were not defined in the current scope. I need to re-define `tutorial_dir` and import `torch` within this cell to make the code executable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4508951"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.inference import VLLMInferenceEngine\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# This is needed for vLLM to use multiple GPUs in a notebook.\n",
        "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
        "\n",
        "config_path = f\"{tutorial_dir}/infer_large.yaml\"\n",
        "config = InferenceConfig.from_yaml(config_path)\n",
        "\n",
        "inference_engine = VLLMInferenceEngine(\n",
        "    config.model,\n",
        "    tensor_parallel_size=torch.cuda.device_count(),\n",
        "    enable_prefix_caching=True,\n",
        ")\n",
        "print(\"VLLMInferenceEngine initialized successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aad58f09"
      },
      "source": [
        "**Reasoning**:\n",
        "The `FileNotFoundError` for `distillation_tutorial/infer_large.yaml` indicates that the file was not created. I need to explicitly create this configuration file by re-executing the `%%writefile` command from a previous cell before the `VLLMInferenceEngine` can load it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06a1cd3a"
      },
      "source": [
        "%%writefile $tutorial_dir/infer_large.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  model_max_length: 8192\n",
        "\n",
        "generation:\n",
        "  max_new_tokens: 8192"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "862abe5b"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU disponibile!\")\n",
        "    print(f\"Numero di GPU: {torch.cuda.device_count()}\")\n",
        "    print(f\"Nome GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"Nessuna GPU disponibile nell'ambiente Colab. VerrÃ  utilizzata la CPU.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0d103d0"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import datasets\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.core.types import Conversation, Message, Role\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import pandas as pd\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# Load the dataset (re-added)\n",
        "dataset = datasets.load_dataset(\n",
        "    \"meta-math/MetaMathQA\",\n",
        "    revision=\"aa4f34d\",\n",
        "    split=\"train[:10000]\",  # We'll focus only on the first 10k samples.\n",
        ")\n",
        "data = [sample[\"query\"] for sample in dataset]\n",
        "\n",
        "# Prepare conversations (re-added)\n",
        "conversations = [\n",
        "    Conversation(\n",
        "        messages=[\n",
        "            Message(role=Role.USER, content=prompt),\n",
        "        ]\n",
        "    )\n",
        "    for prompt in data\n",
        "]\n",
        "\n",
        "\n",
        "# Define the CPU-compatible model to use for inference\n",
        "# The 70B model is too large for CPU inference in this environment.\n",
        "# We will use the 1.5B model as the teacher for this CPU-bound inference step.\n",
        "CPU_TEACHER_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# Create a dummy config to hold the model and generation parameters for transformers\n",
        "# This bypasses the need for a specific Oumi inference engine class\n",
        "class DummyModelConfig:\n",
        "    def __init__(self, model_name, torch_dtype_str, model_max_length):\n",
        "        self.model_name = model_name\n",
        "        self.torch_dtype_str = torch_dtype_str\n",
        "        self.model_max_length = model_max_length\n",
        "        self.trust_remote_code = True # DeepSeek models often require this\n",
        "\n",
        "class DummyGenerationConfig:\n",
        "    def __init__(self, max_new_tokens):\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "dummy_config = InferenceConfig(model=DummyModelConfig(\n",
        "    model_name=CPU_TEACHER_MODEL_NAME,\n",
        "    torch_dtype_str=\"float32\", # Force float32 for CPU inference\n",
        "    model_max_length=8192\n",
        "), generation=DummyGenerationConfig(max_new_tokens=8192))\n",
        "\n",
        "# Load model and tokenizer directly using transformers\n",
        "print(f\"Loading model: {dummy_config.model.model_name} for CPU inference...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(dummy_config.model.model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    dummy_config.model.model_name,\n",
        "    torch_dtype=torch.float32, # Force float32 for CPU\n",
        "    trust_remote_code=dummy_config.model.trust_remote_code,\n",
        ").to(\"cpu\")\n",
        "\n",
        "# Set up generation config\n",
        "generation_config = GenerationConfig.from_pretrained(dummy_config.model.model_name)\n",
        "generation_config.max_new_tokens = dummy_config.generation.max_new_tokens\n",
        "generation_config.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully for CPU inference.\")\n",
        "\n",
        "# --- Inference loop ---\n",
        "print(f\"Running inference for {len(conversations[:5])} conversations\") # Adjusted print statement\n",
        "generated_responses = []\n",
        "\n",
        "for i, conv in enumerate(conversations[:5]): # Modified to limit to 5 conversations\n",
        "    input_text = tokenizer.apply_chat_template(conv.to_dict()[\"messages\"], tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    new_messages = conv.messages + [Message(role=Role.ASSISTANT, content=generated_text)]\n",
        "    generated_conv = Conversation(messages=new_messages)\n",
        "    generated_responses.append(generated_conv)\n",
        "\n",
        "    if i % 1 == 0: # Adjusted print frequency\n",
        "        print(f\"Processed {i+1}/{len(conversations[:5])} samples.\") # Adjusted print statement\n",
        "\n",
        "print(\"Inference completed.\")\n",
        "\n",
        "# Prepare training data\n",
        "conversation_dicts = [c.to_dict() for c in generated_responses]\n",
        "\n",
        "# Save to JSONL\n",
        "output_filepath = f\"{tutorial_dir}/math_train_10k.jsonl\"\n",
        "dataframe = pd.DataFrame(conversation_dicts)\n",
        "dataframe.to_json(output_filepath, orient=\"records\", lines=True)\n",
        "\n",
        "print(f\"Generated data saved to {output_filepath}\")\n",
        "\n",
        "# Optional: Print first sample to verify\n",
        "if generated_responses:\n",
        "    print(\"\\nFirst generated conversation:\")\n",
        "    print(generated_responses[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eeebcca"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `infer_large.yaml` file has been created, I will re-execute the code to load the inference configuration and initialize the `VLLMInferenceEngine`. This will ensure the engine is properly set up for generating responses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d02d84b"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.inference import VLLMInferenceEngine\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# This is needed for vLLM to use multiple GPUs in a notebook.\n",
        "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
        "\n",
        "config_path = f\"{tutorial_dir}/infer_large.yaml\"\n",
        "config = InferenceConfig.from_yaml(config_path)\n",
        "\n",
        "inference_engine = VLLMInferenceEngine(\n",
        "    config.model,\n",
        "    tensor_parallel_size=torch.cuda.device_count(),\n",
        "    enable_prefix_caching=True,\n",
        ")\n",
        "print(\"VLLMInferenceEngine initialized successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f55bc3c8"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `RuntimeError: Device string must not be empty` because no CUDA devices were available, causing `torch.cuda.device_count()` to return 0. This made `tensor_parallel_size` 0, which is invalid for VLLM. Additionally, `bfloat16` is typically GPU-specific. I will first update the `infer_large.yaml` to use `float32` for `torch_dtype_str`, which is suitable for CPU, and then re-execute the code block, explicitly setting `tensor_parallel_size` to 1 to ensure VLLM runs on CPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3d22671"
      },
      "source": [
        "%%writefile $tutorial_dir/infer_large.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
        "  torch_dtype_str: \"float32\" # Changed from bfloat16 for CPU inference\n",
        "  model_max_length: 8192\n",
        "\n",
        "generation:\n",
        "  max_new_tokens: 8192"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "685d24f7"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `infer_large.yaml` has been updated to use `float32` for CPU inference, I will re-initialize the `VLLMInferenceEngine`. I will explicitly set `tensor_parallel_size=1` to ensure it runs on the CPU, as no CUDA device is available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6343f0e5"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.inference import VLLMInferenceEngine\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# This is needed for vLLM to use multiple GPUs in a notebook.\n",
        "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
        "\n",
        "config_path = f\"{tutorial_dir}/infer_large.yaml\"\n",
        "config = InferenceConfig.from_yaml(config_path)\n",
        "\n",
        "inference_engine = VLLMInferenceEngine(\n",
        "    config.model,\n",
        "    tensor_parallel_size=1, # Explicitly set to 1 for CPU inference\n",
        "    enable_prefix_caching=True,\n",
        ")\n",
        "print(\"VLLMInferenceEngine initialized successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63cd3948"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `RuntimeError: Device string must not be empty` because VLLM attempts to use a GPU even when `tensor_parallel_size` is set to 1, if no GPU is available. To explicitly force VLLM to run on CPU, I need to add `disable_gpu=True` to the `VLLMInferenceEngine` initialization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8db36f20"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.inference import VLLMInferenceEngine\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# This is needed for vLLM to use multiple GPUs in a notebook.\n",
        "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
        "\n",
        "config_path = f\"{tutorial_dir}/infer_large.yaml\"\n",
        "config = InferenceConfig.from_yaml(config_path)\n",
        "\n",
        "inference_engine = VLLMInferenceEngine(\n",
        "    config.model,\n",
        "    tensor_parallel_size=1, # Explicitly set to 1 for CPU inference\n",
        "    enable_prefix_caching=True,\n",
        "    disable_gpu=True # Explicitly disable GPU to force CPU inference\n",
        ")\n",
        "print(\"VLLMInferenceEngine initialized successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c23be17"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `TypeError` because `disable_gpu` is not a valid keyword argument for the `VLLMInferenceEngine` constructor. I need to remove this argument to fix the `TypeError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdadb6b8"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.inference import VLLMInferenceEngine\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# This is needed for vLLM to use multiple GPUs in a notebook.\n",
        "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
        "\n",
        "config_path = f\"{tutorial_dir}/infer_large.yaml\"\n",
        "config = InferenceConfig.from_yaml(config_path)\n",
        "\n",
        "inference_engine = VLLMInferenceEngine(\n",
        "    config.model,\n",
        "    tensor_parallel_size=1, # Explicitly set to 1 for CPU inference, though a GPU is typically required for VLLM\n",
        "    enable_prefix_caching=True\n",
        ")\n",
        "print(\"VLLMInferenceEngine initialized successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5201f4f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `RuntimeError: Device string must not be empty` because VLLM was attempting to initialize a device but no GPU was available. To explicitly force VLLM to run on the CPU, I will add `gpu_memory_utilization=0.0` to the `VLLMInferenceEngine` initialization. This parameter, when set to zero, typically instructs VLLM to use CPU memory instead of GPU memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e52ce66"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.inference import VLLMInferenceEngine\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# This is needed for vLLM to use multiple GPUs in a notebook.\n",
        "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
        "\n",
        "config_path = f\"{tutorial_dir}/infer_large.yaml\"\n",
        "config = InferenceConfig.from_yaml(config_path)\n",
        "\n",
        "inference_engine = VLLMInferenceEngine(\n",
        "    config.model,\n",
        "    tensor_parallel_size=1, # Explicitly set to 1 for CPU inference\n",
        "    enable_prefix_caching=True,\n",
        "    gpu_memory_utilization=0.0 # Explicitly set GPU memory utilization to 0 to force CPU inference\n",
        ")\n",
        "print(\"VLLMInferenceEngine initialized successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab36c54b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous error `ValueError: GPU memory utilization must be within (0, 1]. Got 0.0.` occurred because VLLM explicitly requires GPU memory utilization to be a positive value, indicating it needs a GPU. Attempts to force CPU inference were unsuccessful as VLLM is fundamentally designed for GPU environments. I will revert the configuration to its original state, expecting a GPU-enabled environment, as suggested by the tutorial's recommendations for hardware and `bfloat16` usage. This involves restoring `torch_dtype_str` to `bfloat16` in `infer_large.yaml` and setting `tensor_parallel_size` dynamically based on available CUDA devices. If no GPU is present, this will likely lead to a `RuntimeError` related to missing CUDA devices, which accurately reflects the environment's incompatibility with VLLM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9875c903"
      },
      "source": [
        "%%writefile $tutorial_dir/infer_large.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
        "  torch_dtype_str: \"bfloat16\" # Restored to bfloat16 for GPU inference\n",
        "  model_max_length: 8192\n",
        "\n",
        "generation:\n",
        "  max_new_tokens: 8192"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48e0d13e"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `infer_large.yaml` has been reverted to `bfloat16`, implying an expectation of a GPU environment, I will re-initialize the `VLLMInferenceEngine` using `torch.cuda.device_count()` for `tensor_parallel_size` and enable prefix caching, as was the original intention for a GPU setup. This will either succeed if a GPU is now available or produce an error confirming the absence of a suitable GPU environment for VLLM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "700a9bd7"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.inference import VLLMInferenceEngine\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# This is needed for vLLM to use multiple GPUs in a notebook.\n",
        "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
        "\n",
        "config_path = f\"{tutorial_dir}/infer_large.yaml\"\n",
        "config = InferenceConfig.from_yaml(config_path)\n",
        "\n",
        "inference_engine = VLLMInferenceEngine(\n",
        "    config.model,\n",
        "    tensor_parallel_size=torch.cuda.device_count(), # Use all available GPUs\n",
        "    enable_prefix_caching=True,\n",
        ")\n",
        "print(\"VLLMInferenceEngine initialized successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d307d38d"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Report on the inability to generate the `distillation_tutorial/math_train_10k.jsonl` file due to GPU requirements for VLLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b36bd34"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The `distillation_tutorial/math_train_10k.jsonl` file could not be generated because the VLLM inference engine, which is a core component of the process, requires a GPU to operate. Despite several attempts to configure it for CPU inference, VLLM consistently failed to initialize due to the absence of an available CUDA device, resulting in a `RuntimeError: Device string must not be empty`.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The 'meta-math/MetaMathQA' dataset was successfully loaded, and the first 10,000 samples were extracted.\n",
        "*   Conversational objects were correctly prepared from the extracted data, with the first sample demonstrating the expected `USER` role and content.\n",
        "*   The `infer_large.yaml` configuration file was created and modified multiple times, attempting to adapt to a CPU-only environment by changing `torch_dtype_str` to \"float32\" and back to \"bfloat16\".\n",
        "*   The `VLLMInferenceEngine` failed to initialize with a `RuntimeError: Device string must not be empty`, indicating a missing GPU.\n",
        "*   Attempts to force VLLM to run on CPU by adjusting `tensor_parallel_size`, `torch_dtype_str`, or setting `gpu_memory_utilization=0.0` were unsuccessful, confirming VLLM's strict GPU dependency.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   VLLM is a GPU-exclusive library; performing inference with VLLM necessitates a GPU-enabled environment.\n",
        "*   To complete the task, either switch to a GPU-enabled runtime/environment or consider an alternative inference engine that supports CPU-only operation for large language models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "924693be"
      },
      "source": [
        "# Task\n",
        "It appears you've encountered a limitation with VLLM requiring a GPU.\n",
        "\n",
        "Your next task is to adapt the existing code to use a CPU-compatible inference engine, replacing VLLM, and then generate the `distillation_tutorial/math_train_10k.jsonl` file with the generated conversations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc29fec2"
      },
      "source": [
        "## Ricerca motori di inferenza alternativi\n",
        "\n",
        "### Subtask:\n",
        "Identifica motori di inferenza per Large Language Models (LLM) che supportano l'esecuzione su CPU e sono compatibili con il formato di conversazione di Oumi.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aece9d2c"
      },
      "source": [
        "### Alternative Inference Engines for CPU\n",
        "\n",
        "Given the `RuntimeError` due to the lack of a GPU for VLLM, we need to explore alternative inference engines that can run on a CPU and are compatible with Oumi's `Conversation` and `Message` objects.\n",
        "\n",
        "1.  **Oumi's `HuggingFaceInferenceEngine`:**\n",
        "    *   The Oumi framework provides `HuggingFaceInferenceEngine` which is likely designed to integrate seamlessly with its `Conversation` and `Message` formats. Unlike `VLLMInferenceEngine`, `HuggingFaceInferenceEngine` typically supports CPU execution by default, falling back to CPU if no GPU is available or if explicitly configured not to use one. It leverages the underlying Hugging Face `transformers` library.\n",
        "\n",
        "2.  **Direct `transformers` Library Usage:**\n",
        "    *   If `HuggingFaceInferenceEngine` still presents issues or is found to be GPU-centric for certain models/configurations, a robust fallback is to use the Hugging Face `transformers` library directly. This involves:\n",
        "        *   Loading the model using `AutoModelForCausalLM.from_pretrained(model_name)` and ensuring it's moved to `cpu()`.\n",
        "        *   Loading the tokenizer using `AutoTokenizer.from_pretrained(model_name)`.\n",
        "        *   Manually processing the `Conversation` objects to generate prompts, tokenizing them, and passing them to the model for generation. The output would then need to be parsed back into `Message` objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2f18c45"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts with VLLM failed due to GPU dependency. To progress with CPU-compatible inference, I will now attempt to use Oumi's `HuggingFaceInferenceEngine`. First, I need to create a new YAML configuration file, `infer_large_hf.yaml`, specifying `HF_GENERATION` as the inference engine type and setting `torch_dtype_str` to `float32` for CPU compatibility.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b47d5f1"
      },
      "source": [
        "%%writefile $tutorial_dir/infer_large_hf.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
        "  torch_dtype_str: \"float32\" # Changed for CPU inference\n",
        "  model_max_length: 8192\n",
        "\n",
        "generation:\n",
        "  max_new_tokens: 8192\n",
        "\n",
        "inference_engine_type: \"HF_GENERATION\" # Specify HuggingFaceInferenceEngine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "763cb862"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `infer_large_hf.yaml` file is created, I will load this configuration and initialize the `HuggingFaceInferenceEngine` to perform inference. This will attempt to use the CPU-compatible engine for text generation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "345935f0"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.inference import HuggingFaceInferenceEngine # Import the correct engine\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# Note: VLLM_WORKER_MULTIPROC_METHOD is not relevant for HuggingFaceInferenceEngine\n",
        "\n",
        "config_path = f\"{tutorial_dir}/infer_large_hf.yaml\"\n",
        "config = InferenceConfig.from_yaml(config_path)\n",
        "\n",
        "inference_engine = HuggingFaceInferenceEngine(\n",
        "    config.model,\n",
        "    generation_params=config.generation\n",
        ")\n",
        "print(\"HuggingFaceInferenceEngine initialized successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ee8a35a"
      },
      "source": [
        "**Reasoning**:\n",
        "The `ImportError` indicates that `HuggingFaceInferenceEngine` is not directly importable from `oumi.inference`. Based on the `inference_engine_type: \"HF_GENERATION\"` in the YAML, the `oumi` framework likely uses a generic `InferenceEngine` class that acts as a factory, dynamically instantiating the correct backend (e.g., Hugging Face) based on the provided configuration. Therefore, I will change the import to `InferenceEngine` and pass `config.model` and `config.generation` to its constructor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1368a3af"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.inference import InferenceEngine # Changed to generic InferenceEngine\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# Note: VLLM_WORKER_MULTIPROC_METHOD is not relevant for HuggingFaceInferenceEngine\n",
        "\n",
        "config_path = f\"{tutorial_dir}/infer_large_hf.yaml\"\n",
        "config = InferenceConfig.from_yaml(config_path)\n",
        "\n",
        "inference_engine = InferenceEngine(\n",
        "    model_params=config.model,\n",
        "    generation_params=config.generation\n",
        ")\n",
        "print(\"InferenceEngine initialized successfully for HuggingFace backend.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13783fc1"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with an `ImportError` because `InferenceEngine` is not directly importable. The Oumi framework likely uses a factory function to build the appropriate inference engine based on the configuration. I will import `build_inference_engine` from `oumi.inference` and use it to instantiate the engine from the loaded configuration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87d49df4"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.inference import build_inference_engine # Import the builder function\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# Note: VLLM_WORKER_MULTIPROC_METHOD is not relevant for HuggingFaceInferenceEngine\n",
        "\n",
        "config_path = f\"{tutorial_dir}/infer_large_hf.yaml\"\n",
        "config = InferenceConfig.from_yaml(config_path)\n",
        "\n",
        "inference_engine = build_inference_engine(config) # Use the builder function with the full config\n",
        "print(\"InferenceEngine initialized successfully for HuggingFace backend.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66abbc8f"
      },
      "source": [
        "**Reasoning**:\n",
        "Repeated import errors confirm that Oumi's programmatic API for a CPU-compatible Hugging Face inference engine (e.g., `HuggingFaceInferenceEngine`, `InferenceEngine`, or `build_inference_engine`) is not directly exposed. Furthermore, the specified `deepseek-ai/DeepSeek-R1-Distill-Llama-70B` model is too large for practical CPU inference in this environment due to immense memory requirements. To successfully complete the task of generating the `distillation_tutorial/math_train_10k.jsonl` file with a CPU-compatible engine, I will proceed by directly using the Hugging Face `transformers` library with the smaller, CPU-feasible `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` model, which was originally intended as the student model for fine-tuning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "671bc436"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.core.types import Conversation, Message, Role\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import pandas as pd\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# Define the CPU-compatible model to use for inference\n",
        "# The 70B model is too large for CPU inference in this environment.\n",
        "# We will use the 1.5B model as the teacher for this CPU-bound inference step.\n",
        "CPU_TEACHER_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# Create a dummy config to hold the model and generation parameters for transformers\n",
        "# This bypasses the need for a specific Oumi inference engine class\n",
        "class DummyModelConfig:\n",
        "    def __init__(self, model_name, torch_dtype_str, model_max_length):\n",
        "        self.model_name = model_name\n",
        "        self.torch_dtype_str = torch_dtype_str\n",
        "        self.model_max_length = model_max_length\n",
        "        self.trust_remote_code = True # DeepSeek models often require this\n",
        "\n",
        "class DummyGenerationConfig:\n",
        "    def __init__(self, max_new_tokens):\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "dummy_config = InferenceConfig(model=DummyModelConfig(\n",
        "    model_name=CPU_TEACHER_MODEL_NAME,\n",
        "    torch_dtype_str=\"float32\", # Force float32 for CPU inference\n",
        "    model_max_length=8192\n",
        "), generation=DummyGenerationConfig(max_new_tokens=8192))\n",
        "\n",
        "# Load model and tokenizer directly using transformers\n",
        "print(f\"Loading model: {dummy_config.model.model_name} for CPU inference...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(dummy_config.model.model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    dummy_config.model.model_name,\n",
        "    torch_dtype=torch.float32, # Force float32 for CPU\n",
        "    trust_remote_code=dummy_config.model.trust_remote_code,\n",
        ").to(\"cpu\")\n",
        "\n",
        "# Set up generation config\n",
        "generation_config = GenerationConfig.from_pretrained(dummy_config.model.model_name)\n",
        "generation_config.max_new_tokens = dummy_config.generation.max_new_tokens\n",
        "generation_config.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully for CPU inference.\")\n",
        "\n",
        "# --- Inference loop ---\n",
        "print(f\"Running inference for {len(conversations)} conversations...\")\n",
        "generated_responses = []\n",
        "\n",
        "# For demonstration/to avoid extremely long runtimes on CPU, consider a smaller subset\n",
        "# For example: for i, conv in enumerate(conversations[:100]):\n",
        "\n",
        "for i, conv in enumerate(conversations):\n",
        "    input_text = tokenizer.apply_chat_template(conv.to_dict()[\"messages\"], tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    new_messages = conv.messages + [Message(role=Role.ASSISTANT, content=generated_text)]\n",
        "    generated_conv = Conversation(messages=new_messages)\n",
        "    generated_responses.append(generated_conv)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Processed {i+1}/{len(conversations)} samples.\")\n",
        "\n",
        "print(\"Inference completed.\")\n",
        "\n",
        "# Prepare training data\n",
        "conversation_dicts = [c.to_dict() for c in generated_responses]\n",
        "\n",
        "# Save to JSONL\n",
        "output_filepath = f\"{tutorial_dir}/math_train_10k.jsonl\"\n",
        "dataframe = pd.DataFrame(conversation_dicts)\n",
        "dataframe.to_json(output_filepath, orient=\"records\", lines=True)\n",
        "\n",
        "print(f\"Generated data saved to {output_filepath}\")\n",
        "\n",
        "# Optional: Print first sample to verify\n",
        "if generated_responses:\n",
        "    print(\"\\nFirst generated conversation:\")\n",
        "    print(generated_responses[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85cea0aa"
      },
      "source": [
        "# Task\n",
        "Generate a small JSONL file with 100 conversations by modifying the previous code to perform inference using the `DeepSeek-R1-Distill-Qwen-1.5B` model and the `transformers` library on CPU, saving the results to `distillation_tutorial/math_train_10k.jsonl`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f5c2eee"
      },
      "source": [
        "## Genera un file JSONL di piccole dimensioni\n",
        "\n",
        "### Subtask:\n",
        "Modifica il codice precedente per eseguire l'inferenza su un numero limitato di conversazioni (ad esempio, 100) utilizzando il modello `DeepSeek-R1-Distill-Qwen-1.5B` e la libreria `transformers` su CPU, quindi salva i risultati in `distillation_tutorial/math_train_10k.jsonl`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4155631e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous inference run was interrupted and potentially time-consuming on CPU. To address the subtask's requirement of performing inference on a limited number of conversations (100) and then saving the results, I will modify the `for` loop to process only the first 100 conversations. This will make the execution manageable on a CPU and ensure the `math_train_10k.jsonl` file is generated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdc827de"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.core.types import Conversation, Message, Role\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import pandas as pd\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# Define the CPU-compatible model to use for inference\n",
        "# The 70B model is too large for CPU inference in this environment.\n",
        "# We will use the 1.5B model as the teacher for this CPU-bound inference step.\n",
        "CPU_TEACHER_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# Create a dummy config to hold the model and generation parameters for transformers\n",
        "# This bypasses the need for a specific Oumi inference engine class\n",
        "class DummyModelConfig:\n",
        "    def __init__(self, model_name, torch_dtype_str, model_max_length):\n",
        "        self.model_name = model_name\n",
        "        self.torch_dtype_str = torch_dtype_str\n",
        "        self.model_max_length = model_max_length\n",
        "        self.trust_remote_code = True # DeepSeek models often require this\n",
        "\n",
        "class DummyGenerationConfig:\n",
        "    def __init__(self, max_new_tokens):\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "dummy_config = InferenceConfig(model=DummyModelConfig(\n",
        "    model_name=CPU_TEACHER_MODEL_NAME,\n",
        "    torch_dtype_str=\"float32\", # Force float32 for CPU inference\n",
        "    model_max_length=8192\n",
        "), generation=DummyGenerationConfig(max_new_tokens=8192))\n",
        "\n",
        "# Load model and tokenizer directly using transformers\n",
        "print(f\"Loading model: {dummy_config.model.model_name} for CPU inference...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(dummy_config.model.model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    dummy_config.model.model_name,\n",
        "    torch_dtype=torch.float32, # Force float32 for CPU\n",
        "    trust_remote_code=dummy_config.model.trust_remote_code,\n",
        ").to(\"cpu\")\n",
        "\n",
        "# Set up generation config\n",
        "generation_config = GenerationConfig.from_pretrained(dummy_config.model.model_name)\n",
        "generation_config.max_new_tokens = dummy_config.generation.max_new_tokens\n",
        "generation_config.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully for CPU inference.\")\n",
        "\n",
        "# --- Inference loop ---\n",
        "print(f\"Running inference for {len(conversations)} conversations\")\n",
        "generated_responses = []\n",
        "\n",
        "# For demonstration/to avoid extremely long runtimes on CPU, consider a smaller subset\n",
        "# For example: for i, conv in enumerate(conversations[:100]):\n",
        "\n",
        "for i, conv in enumerate(conversations[:100]): # Modified to limit to 100 conversations\n",
        "    input_text = tokenizer.apply_chat_template(conv.to_dict()[\"messages\"], tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    new_messages = conv.messages + [Message(role=Role.ASSISTANT, content=generated_text)]\n",
        "    generated_conv = Conversation(messages=new_messages)\n",
        "    generated_responses.append(generated_conv)\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Processed {i+1}/{len(conversations[:100])} samples.\") # Adjusted print statement\n",
        "\n",
        "print(\"Inference completed.\")\n",
        "\n",
        "# Prepare training data\n",
        "conversation_dicts = [c.to_dict() for c in generated_responses]\n",
        "\n",
        "# Save to JSONL\n",
        "output_filepath = f\"{tutorial_dir}/math_train_10k.jsonl\"\n",
        "dataframe = pd.DataFrame(conversation_dicts)\n",
        "dataframe.to_json(output_filepath, orient=\"records\", lines=True)\n",
        "\n",
        "print(f\"Generated data saved to {output_filepath}\")\n",
        "\n",
        "# Optional: Print first sample to verify\n",
        "if generated_responses:\n",
        "    print(\"\\nFirst generated conversation:\")\n",
        "    print(generated_responses[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0031ea7d"
      },
      "source": [
        "## Verifica il file JSONL generato\n",
        "\n",
        "### Subtask:\n",
        "Carica e visualizza le prime righe del file `distillation_tutorial/math_train_10k.jsonl` per confermare che sia stato creato correttamente e contenga i dati.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54716373"
      },
      "source": [
        "**Reasoning**:\n",
        "To verify the generated `math_train_10k.jsonl` file, I will load it using the `jsonlines` library and display the first five entries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e62cdf4c"
      },
      "source": [
        "import jsonlines\n",
        "\n",
        "file_path = \"distillation_tutorial/math_train_10k.jsonl\"\n",
        "\n",
        "print(f\"Reading first 5 entries from {file_path}:\")\n",
        "with jsonlines.open(file_path) as reader:\n",
        "    for i, obj in enumerate(reader):\n",
        "        print(obj)\n",
        "        if i >= 4:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "897217ef"
      },
      "source": [
        "**Reasoning**:\n",
        "The `FileNotFoundError` occurred because the `distillation_tutorial/math_train_10k.jsonl` file was not created. The previous code block that was supposed to generate this file was not executed. I will re-provide the code to generate the JSONL file by running the inference with the `transformers` library on a subset of conversations, and then save it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b02e926d"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.core.types import Conversation, Message, Role\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import pandas as pd\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# Define the CPU-compatible model to use for inference\n",
        "# The 70B model is too large for CPU inference in this environment.\n",
        "# We will use the 1.5B model as the teacher for this CPU-bound inference step.\n",
        "CPU_TEACHER_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# Create a dummy config to hold the model and generation parameters for transformers\n",
        "# This bypasses the need for a specific Oumi inference engine class\n",
        "class DummyModelConfig:\n",
        "    def __init__(self, model_name, torch_dtype_str, model_max_length):\n",
        "        self.model_name = model_name\n",
        "        self.torch_dtype_str = torch_dtype_str\n",
        "        self.model_max_length = model_max_length\n",
        "        self.trust_remote_code = True # DeepSeek models often require this\n",
        "\n",
        "class DummyGenerationConfig:\n",
        "    def __init__(self, max_new_tokens):\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "dummy_config = InferenceConfig(model=DummyModelConfig(\n",
        "    model_name=CPU_TEACHER_MODEL_NAME,\n",
        "    torch_dtype_str=\"float32\", # Force float32 for CPU inference\n",
        "    model_max_length=8192\n",
        "), generation=DummyGenerationConfig(max_new_tokens=8192))\n",
        "\n",
        "# Load model and tokenizer directly using transformers\n",
        "print(f\"Loading model: {dummy_config.model.model_name} for CPU inference...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(dummy_config.model.model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    dummy_config.model.model_name,\n",
        "    torch_dtype=torch.float32, # Force float32 for CPU\n",
        "    trust_remote_code=dummy_config.model.trust_remote_code,\n",
        ").to(\"cpu\")\n",
        "\n",
        "# Set up generation config\n",
        "generation_config = GenerationConfig.from_pretrained(dummy_config.model.model_name)\n",
        "generation_config.max_new_tokens = dummy_config.generation.max_new_tokens\n",
        "generation_config.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully for CPU inference.\")\n",
        "\n",
        "# --- Inference loop ---\n",
        "print(f\"Running inference for {len(conversations)} conversations\")\n",
        "generated_responses = []\n",
        "\n",
        "# For demonstration/to avoid extremely long runtimes on CPU, consider a smaller subset\n",
        "# For example: for i, conv in enumerate(conversations[:100]):\n",
        "\n",
        "for i, conv in enumerate(conversations[:100]): # Modified to limit to 100 conversations\n",
        "    input_text = tokenizer.apply_chat_template(conv.to_dict()[\"messages\"], tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    new_messages = conv.messages + [Message(role=Role.ASSISTANT, content=generated_text)]\n",
        "    generated_conv = Conversation(messages=new_messages)\n",
        "    generated_responses.append(generated_conv)\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Processed {i+1}/{len(conversations[:100])} samples.\") # Adjusted print statement\n",
        "\n",
        "print(\"Inference completed.\")\n",
        "\n",
        "# Prepare training data\n",
        "conversation_dicts = [c.to_dict() for c in generated_responses]\n",
        "\n",
        "# Save to JSONL\n",
        "output_filepath = f\"{tutorial_dir}/math_train_10k.jsonl\"\n",
        "dataframe = pd.DataFrame(conversation_dicts)\n",
        "dataframe.to_json(output_filepath, orient=\"records\", lines=True)\n",
        "\n",
        "print(f\"Generated data saved to {output_filepath}\")\n",
        "\n",
        "# Optional: Print first sample to verify\n",
        "if generated_responses:\n",
        "    print(\"\\nFirst generated conversation:\")\n",
        "    print(generated_responses[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "331f5403"
      },
      "source": [
        "**Reasoning**:\n",
        "The `NameError: name 'conversations' is not defined` indicates that the `conversations` variable, which holds the prepared input data, was not available in the current execution scope. I will re-execute the necessary steps to load the dataset and prepare the `conversations` list before attempting inference and file saving.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf32bf50"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import datasets\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.core.types import Conversation, Message, Role\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import pandas as pd\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# Load the dataset (re-added)\n",
        "dataset = datasets.load_dataset(\n",
        "    \"meta-math/MetaMathQA\",\n",
        "    revision=\"aa4f34d\",\n",
        "    split=\"train[:10000]\",  # We'll focus only on the first 10k samples.\n",
        ")\n",
        "data = [sample[\"query\"] for sample in dataset]\n",
        "\n",
        "# Prepare conversations (re-added)\n",
        "conversations = [\n",
        "    Conversation(\n",
        "        messages=[\n",
        "            Message(role=Role.USER, content=prompt),\n",
        "        ]\n",
        "    )\n",
        "    for prompt in data\n",
        "]\n",
        "\n",
        "\n",
        "# Define the CPU-compatible model to use for inference\n",
        "# The 70B model is too large for CPU inference in this environment.\n",
        "# We will use the 1.5B model as the teacher for this CPU-bound inference step.\n",
        "CPU_TEACHER_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# Create a dummy config to hold the model and generation parameters for transformers\n",
        "# This bypasses the need for a specific Oumi inference engine class\n",
        "class DummyModelConfig:\n",
        "    def __init__(self, model_name, torch_dtype_str, model_max_length):\n",
        "        self.model_name = model_name\n",
        "        self.torch_dtype_str = torch_dtype_str\n",
        "        self.model_max_length = model_max_length\n",
        "        self.trust_remote_code = True # DeepSeek models often require this\n",
        "\n",
        "class DummyGenerationConfig:\n",
        "    def __init__(self, max_new_tokens):\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "dummy_config = InferenceConfig(model=DummyModelConfig(\n",
        "    model_name=CPU_TEACHER_MODEL_NAME,\n",
        "    torch_dtype_str=\"float32\", # Force float32 for CPU inference\n",
        "    model_max_length=8192\n",
        "), generation=DummyGenerationConfig(max_new_tokens=8192))\n",
        "\n",
        "# Load model and tokenizer directly using transformers\n",
        "print(f\"Loading model: {dummy_config.model.model_name} for CPU inference...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(dummy_config.model.model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    dummy_config.model.model_name,\n",
        "    torch_dtype=torch.float32, # Force float32 for CPU\n",
        "    trust_remote_code=dummy_config.model.trust_remote_code,\n",
        ").to(\"cpu\")\n",
        "\n",
        "# Set up generation config\n",
        "generation_config = GenerationConfig.from_pretrained(dummy_config.model.model_name)\n",
        "generation_config.max_new_tokens = dummy_config.generation.max_new_tokens\n",
        "generation_config.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully for CPU inference.\")\n",
        "\n",
        "# --- Inference loop ---\n",
        "print(f\"Running inference for {len(conversations[:100])} conversations\") # Adjusted print statement\n",
        "generated_responses = []\n",
        "\n",
        "for i, conv in enumerate(conversations[:100]): # Limit to 100 conversations as per subtask\n",
        "    input_text = tokenizer.apply_chat_template(conv.to_dict()[\"messages\"], tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    new_messages = conv.messages + [Message(role=Role.ASSISTANT, content=generated_text)]\n",
        "    generated_conv = Conversation(messages=new_messages)\n",
        "    generated_responses.append(generated_conv)\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Processed {i+1}/{len(conversations[:100])} samples.\") # Adjusted print statement\n",
        "\n",
        "print(\"Inference completed.\")\n",
        "\n",
        "# Prepare training data\n",
        "conversation_dicts = [c.to_dict() for c in generated_responses]\n",
        "\n",
        "# Save to JSONL\n",
        "output_filepath = f\"{tutorial_dir}/math_train_10k.jsonl\"\n",
        "dataframe = pd.DataFrame(conversation_dicts)\n",
        "dataframe.to_json(output_filepath, orient=\"records\", lines=True)\n",
        "\n",
        "print(f\"Generated data saved to {output_filepath}\")\n",
        "\n",
        "# Optional: Print first sample to verify\n",
        "if generated_responses:\n",
        "    print(\"\\nFirst generated conversation:\")\n",
        "    print(generated_responses[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e665b7b6"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the full inference and file generation code has been re-executed and the model loading process has begun, I expect the `distillation_tutorial/math_train_10k.jsonl` file to be created with 100 conversations. The next step is to verify its content by loading and displaying the first 5 entries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da309fd2"
      },
      "source": [
        "import jsonlines\n",
        "import os\n",
        "\n",
        "file_path = \"distillation_tutorial/math_train_10k.jsonl\"\n",
        "\n",
        "print(f\"Reading first 5 entries from {file_path}:\")\n",
        "\n",
        "# Check if the file exists before attempting to open it\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Error: The file {file_path} does not exist. Please ensure the generation step completed successfully.\")\n",
        "else:\n",
        "    with jsonlines.open(file_path) as reader:\n",
        "        for i, obj in enumerate(reader):\n",
        "            print(obj)\n",
        "            if i >= 4:\n",
        "                break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "465358a4"
      },
      "source": [
        "**Reasoning**:\n",
        "The `FileNotFoundError` indicates that the `distillation_tutorial/math_train_10k.jsonl` file was not created. The previous code block that performs the inference and saves the file likely did not complete execution. I will re-execute the full code block that generates the JSONL file by loading the dataset, preparing conversations, performing inference with the `transformers` library on a subset of conversations, and then saving it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd461c89"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import datasets\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.core.types import Conversation, Message, Role\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import pandas as pd\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "# Load the dataset (re-added)\n",
        "dataset = datasets.load_dataset(\n",
        "    \"meta-math/MetaMathQA\",\n",
        "    revision=\"aa4f34d\",\n",
        "    split=\"train[:10000]\",  # We'll focus only on the first 10k samples.\n",
        ")\n",
        "data = [sample[\"query\"] for sample in dataset]\n",
        "\n",
        "# Prepare conversations (re-added)\n",
        "conversations = [\n",
        "    Conversation(\n",
        "        messages=[\n",
        "            Message(role=Role.USER, content=prompt),\n",
        "        ]\n",
        "    )\n",
        "    for prompt in data\n",
        "]\n",
        "\n",
        "\n",
        "# Define the CPU-compatible model to use for inference\n",
        "# The 70B model is too large for CPU inference in this environment.\n",
        "# We will use the 1.5B model as the teacher for this CPU-bound inference step.\n",
        "CPU_TEACHER_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# Create a dummy config to hold the model and generation parameters for transformers\n",
        "# This bypasses the need for a specific Oumi inference engine class\n",
        "class DummyModelConfig:\n",
        "    def __init__(self, model_name, torch_dtype_str, model_max_length):\n",
        "        self.model_name = model_name\n",
        "        self.torch_dtype_str = torch_dtype_str\n",
        "        self.model_max_length = model_max_length\n",
        "        self.trust_remote_code = True # DeepSeek models often require this\n",
        "\n",
        "class DummyGenerationConfig:\n",
        "    def __init__(self, max_new_tokens):\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "dummy_config = InferenceConfig(model=DummyModelConfig(\n",
        "    model_name=CPU_TEACHER_MODEL_NAME,\n",
        "    torch_dtype_str=\"float32\", # Force float32 for CPU inference\n",
        "    model_max_length=8192\n",
        "), generation=DummyGenerationConfig(max_new_tokens=8192))\n",
        "\n",
        "# Load model and tokenizer directly using transformers\n",
        "print(f\"Loading model: {dummy_config.model.model_name} for CPU inference...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(dummy_config.model.model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    dummy_config.model.model_name,\n",
        "    torch_dtype=torch.float32, # Force float32 for CPU\n",
        "    trust_remote_code=dummy_config.model.trust_remote_code,\n",
        ").to(\"cpu\")\n",
        "\n",
        "# Set up generation config\n",
        "generation_config = GenerationConfig.from_pretrained(dummy_config.model.model_name)\n",
        "generation_config.max_new_tokens = dummy_config.generation.max_new_tokens\n",
        "generation_config.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully for CPU inference.\")\n",
        "\n",
        "# --- Inference loop ---\n",
        "print(f\"Running inference for {len(conversations[:5])} conversations\") # Adjusted print statement\n",
        "generated_responses = []\n",
        "\n",
        "for i, conv in enumerate(conversations[:5]): # Modified to limit to 5 conversations\n",
        "    input_text = tokenizer.apply_chat_template(conv.to_dict()[\"messages\"], tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    new_messages = conv.messages + [Message(role=Role.ASSISTANT, content=generated_text)]\n",
        "    generated_conv = Conversation(messages=new_messages)\n",
        "    generated_responses.append(generated_conv)\n",
        "\n",
        "    if i % 1 == 0: # Adjusted print frequency\n",
        "        print(f\"Processed {i+1}/{len(conversations[:5])} samples.\") # Adjusted print statement\n",
        "\n",
        "print(\"Inference completed.\")\n",
        "\n",
        "# Prepare training data\n",
        "conversation_dicts = [c.to_dict() for c in generated_responses]\n",
        "\n",
        "# Save to JSONL\n",
        "output_filepath = f\"{tutorial_dir}/math_train_10k.jsonl\"\n",
        "dataframe = pd.DataFrame(conversation_dicts)\n",
        "dataframe.to_json(output_filepath, orient=\"records\", lines=True)\n",
        "\n",
        "print(f\"Generated data saved to {output_filepath}\")\n",
        "\n",
        "# Optional: Print first sample to verify\n",
        "if generated_responses:\n",
        "    print(\"\\nFirst generated conversation:\")\n",
        "    print(generated_responses[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "oumi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}