{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Bonus Day - Extra API features to try",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venezianof/booksum/blob/main/Bonus_Day_Extra_API_features_to_try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2025 Google LLC."
      ],
      "metadata": {
        "id": "abPZ9IKFbKjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "execution": {
          "iopub.status.busy": "2025-04-04T04:53:17.924118Z",
          "iopub.execute_input": "2025-04-04T04:53:17.924587Z",
          "iopub.status.idle": "2025-04-04T04:53:17.950628Z",
          "shell.execute_reply.started": "2025-04-04T04:53:17.924547Z",
          "shell.execute_reply": "2025-04-04T04:53:17.94966Z"
        },
        "id": "86Ty4OpnbKjv"
      },
      "outputs": [],
      "execution_count": 178
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus Content!\n",
        "\n",
        "Congrats on finishing the 5-day Generative AI Intensive course from Kaggle and Google!\n",
        "\n",
        "This notebook is a \"bonus episode\" that highlights a few more things you can do with the Gemini API that weren't covered during the course. This material doesn't pair with the whitepapers or podcast, but covers some extra features that you might find useful when building Gemini API powered apps."
      ],
      "metadata": {
        "id": "vrYuSr7XbKjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get set up\n",
        "\n",
        "Install the SDK and other tools for this notebook, then import the package and set up a retry policy so you don't have to manually retry when you hit a quota limit."
      ],
      "metadata": {
        "id": "23i37Os_bKjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -qyy jupyterlab\n",
        "!pip install -qU \"google-genai==1.9.0\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:53:17.952889Z",
          "iopub.execute_input": "2025-04-04T04:53:17.953283Z",
          "iopub.status.idle": "2025-04-04T04:53:36.295763Z",
          "shell.execute_reply.started": "2025-04-04T04:53:17.953251Z",
          "shell.execute_reply": "2025-04-04T04:53:36.294272Z"
        },
        "id": "HyPjBXVxbKjy",
        "outputId": "e2b20f1d-6c4b-4c65-bec0-f47e35f0ec9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "execution_count": 179
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from IPython.display import display, Image, Markdown, Audio\n",
        "\n",
        "genai.__version__"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:53:36.29739Z",
          "iopub.execute_input": "2025-04-04T04:53:36.297754Z",
          "iopub.status.idle": "2025-04-04T04:53:37.661407Z",
          "shell.execute_reply.started": "2025-04-04T04:53:36.297718Z",
          "shell.execute_reply": "2025-04-04T04:53:37.660347Z"
        },
        "id": "UjSuPofYbKjy",
        "outputId": "21456293-c2f2-41c8-de37-ff3a91edc6a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.9.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 180
        }
      ],
      "execution_count": 180
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n",
        "\n",
        "If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n",
        "\n",
        "To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook."
      ],
      "metadata": {
        "id": "LJTGUXxVbKjz"
      }
    },
    {
      "source": [
        "response = client.models.generate_content(\n",
        "    model='gemini-pro',\n",
        "    contents='Write a poem about a cat.'\n",
        ")\n",
        "print(response.text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Z1e4vZAicidt",
        "outputId": "a1b2d177-8040-4b36-e697-2ff5320b97b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'client' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-181-e2c279fab2fb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = client.models.generate_content(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gemini-pro'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Write a poem about a cat.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(\n",
        "    predicate=is_retriable)(genai.models.Models.generate_content)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Yh2ATGY9jlM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(\n",
        "    predicate=is_retriable)(genai.models.Models.generate_content)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "6LeTgiTRjcsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "async with live_client.aio.live.connect(model=MODEL, config=CONFIG) as session:\n",
        "\n",
        "    message = \"Hi there, can you tell me something fun about spiders?\"\n",
        "    print('>', message)\n",
        "\n",
        "    # Send the message to the model.\n",
        "    await session.send(input=message, end_of_turn=True)\n",
        "\n",
        "    # Set up a temporary audio file to store the audio response.\n",
        "    with wave_file(file_name := \"audio_chat.wav\") as wav:\n",
        "\n",
        "      # Start receiving and handling the response\n",
        "      async for chunk in session.receive():\n",
        "        # Text responses.\n",
        "        if chunk.text is not None:\n",
        "          print(chunk.text, end='')\n",
        "\n",
        "        # Audio responses.\n",
        "        elif chunk.data is not None:\n",
        "          wav.writeframes(chunk.data)\n",
        "          print('.', end='')\n",
        "\n",
        "    display(Audio(file_name, autoplay=True))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "K20LjqySjQ0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "async with live_client.aio.live.connect(model=MODEL, config=CONFIG) as session:\n",
        "\n",
        "    message = \"Hi there, can you tell me something fun about spiders?\"\n",
        "    print('>', message)\n",
        "\n",
        "    # Send the message to the model.\n",
        "    await session.send(input=message, end_of_turn=True)\n",
        "\n",
        "    # Set up a temporary audio file to store the audio response.\n",
        "    with wave_file(file_name := \"audio_chat.wav\") as wav:\n",
        "\n",
        "      # Start receiving and handling the response\n",
        "      async for chunk in session.receive():\n",
        "        # Text responses.\n",
        "        if chunk.text is not None:\n",
        "          print(chunk.text, end='')\n",
        "\n",
        "        # Audio responses."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vsdnJCbDi9mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "async with live_client.aio.live.connect(model=MODEL, config=CONFIG) as session:\n",
        "\n",
        "    message = \"Hi there, can you tell me something fun about spiders?\"\n",
        "    print('>', message)\n",
        "\n",
        "    # Send the message to the model.\n",
        "    await session.send(input=message, end_of_turn=True)\n",
        "\n",
        "    # Set up a temporary audio file to store the audio response.\n",
        "    with wave_file(file_name := \"audio_chat.wav\") as wav:\n",
        "\n",
        "      # Start receiving and handling the response\n",
        "      async for chunk in session.receive():\n",
        "        # Text responses.\n",
        "        if chunk.text is not None:\n",
        "          print(chunk.text, end='')\n",
        "\n",
        "        # Audio responses.\n",
        "        elif chunk.data is not None:\n",
        "          wav.writeframes(chunk.data)\n",
        "          print('.', end='')\n",
        "\n",
        "    display(Audio(file_name, autoplay=True))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3sJQo49AjAJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "async def simplified_live_chat():\n",
        "    async with live_client.aio.live.connect(model=MODEL, config=CONFIG) as session:\n",
        "        print(\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FTCWL6M4iXKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "async def simplified_live_chat():\n",
        "    async with live_client.aio.live.connect(model=MODEL, config=CONFIG) as session:\n",
        "        print(\"Starting text-based chat with Gemini!\")\n",
        "        while True:\n",
        "            message = input(\"> \")\n",
        "            if message.lower() == \"quit\":\n",
        "                break\n",
        "            await session.send(input=message, end_of_turn=True)\n",
        "\n",
        "            async for chunk in session.receive():\n",
        "                if chunk.text is not None:\n",
        "                    print(chunk.text, end='')\n",
        "                # Handle other modalities (like images) if needed"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0w7krh-Sif3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "print(apollo_cache.name)  # Print the cache name for confirmation\n",
        "client.caches."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "srKmje7diM7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "live_client = genai.Client(\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    http_options=types.HttpOptions(api"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "G21WEsLripe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "live_client = genai.Client(\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    http_options=types.HttpOptions(api_version='v1alpha'),\n",
        ")\n",
        "\n",
        "# For the Live API, you need to use the 2.0 experimental model and v1alpha API.\n",
        "MODEL = 'gemini-2.0-flash-exp'\n",
        "\n",
        "# Set up an audio-out session.\n",
        "CONFIG = types.LiveConnectConfig(\n",
        "    response_modalities=[\"AUDIO\"],\n",
        "    speech_config=types.SpeechConfig(\n",
        "        voice_config=types.VoiceConfig(\n",
        "\n",
        "            # Try changing the voice! Pick one of Puck, Charon, Kore, Fenrir, Aoede\n",
        "            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name=\"Kore\")\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OiFqDU22iugI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "live_client = genai.Client(\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    http_options=types.HttpOptions(api_version='v1alpha'),\n",
        ")\n",
        "\n",
        "# For the Live API, you need to use the 2.0 experimental model and v1alpha API.\n",
        "MODEL = 'gemini-2.0-flash-exp'\n",
        "\n",
        "# Set up an audio-out session.\n",
        "CONFIG = types.LiveConnectConfig(\n",
        "    response_modalities=[\"AUDIO\"],\n",
        "    speech_config=types.SpeechConfig(\n",
        "        voice_config=types.VoiceConfig(\n",
        "\n",
        "            # Try changing the voice! Pick one of Puck, Charon, Kore, Fenrir, Aoede\n",
        "            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name=\"Kore\")\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ERRln9mBixIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "print(apollo_cache.name)\n",
        "client.caches.delete(name=apollo_cache.name)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wJLitBeUiAXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "print(apollo_cache.name)\n",
        "client.caches.delete(name=apollo_cache.name)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mPGD4znuiCqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "response = client.models.generate_content(\n",
        "    model=apollo_cache.model,\n",
        "    config=types.GenerateContentConfig(\n",
        "        cached_content=apollo_cache.name,\n",
        "    ),\n",
        "    contents=\"Find a nice moment from this transcript\"\n",
        ")\n",
        "\n",
        "from IPython.display import Markdown\n",
        "Markdown(response.text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mv8bs8svhqCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "response.usage_metadata.to_json_dict()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mSgG7Kj_hzLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "response.usage_metadata.total_token_count - response.usage_metadata.cached_content_token_count"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ww9EJy4Phzl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "response.usage_metadata.to_json_dict()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LtMDKCSvh1Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "response.usage_metadata.total_token_count - response.usage_metadata.cached_content_token_count"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "S84J4zjlh1q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Download the transcript\n",
        "!wget -nv -O apollo11.txt https://storage.googleapis.com/generativeai-downloads/data/a11.txt\n",
        "\n",
        "# Upload to the File API\n",
        "transcript_file = client.files.upload(file='apollo11.txt')\n",
        "\n",
        "# Create a cache\n",
        "apollo_cache = client.caches.create(\n",
        "    model='gemini-1.5-flash-001',  # Specify the model version for caching\n",
        "    config=types.CreateCachedContentConfig(\n",
        "        system_instruction=\"You are a space history buff that enjoys discussing and explaining historical space events.\",\n",
        "        contents=[transcript_file],\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(apollo_cache)  #"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "71J10ytrhgDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Download the transcript\n",
        "!wget -nv -O apollo11.txt https://storage.googleapis.com/generativeai-downloads/data/a11.txt\n",
        "\n",
        "# Upload to the File API\n",
        "transcript_file = client.files.upload(file='apollo11.txt')\n",
        "\n",
        "# Create a cache\n",
        "apollo_cache = client.caches.create(\n",
        "    model='gemini-1.5-flash-001',  # Specify the model version for caching\n",
        "    config=types.CreateCachedContentConfig(\n",
        "        system_instruction=\"You are a space history buff that enjoys discussing and explaining historical space events.\",\n",
        "        contents=[transcript_file],\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(apollo_cache)  # Print the cache object to see its details"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ySk21IG5hS2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Download the transcript\n",
        "!wget -nv -O apollo11.txt https://storage.googleapis.com/generativeai-downloads/data/a11.txt\n",
        "\n",
        "# Upload to the File API\n",
        "transcript_file = client.files.upload(file='apollo11.txt')\n",
        "\n",
        "# Create a cache\n",
        "apollo_cache = client.caches.create(\n",
        "    model='gemini-1.5-flash-001',  # Specify the model version for caching\n",
        "    config=types.CreateCachedContentConfig(\n",
        "        system_instruction=\"You are a space history buff that enjoys discussing and explaining historical space events.\",\n",
        "        contents=[transcript_file],\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(apollo_cache)  # Print the cache object to see its details"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gupTiBQWhXPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "prompt = \"\"\"Can you create a 3d rendered image of a pig with wings\n",
        "and a top hat flying over a happy futuristic scifi city with lots\n",
        "of greenery?\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    # Use the dedicated image generation model.\n",
        "    model=\"gemini-2.0-flash-exp-image-generation\",\n",
        "    contents=prompt,\n",
        "    # This model requires both text and image outputs.\n",
        "    config=types.GenerateContentConfig(\n",
        "      response_modalities=['text', 'image']\n",
        "    )\n",
        ")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "\n",
        "  if part.text:\n",
        "    print(part.text)\n",
        "\n",
        "  elif part.inline_data:\n",
        "    display(Image(part.inline_data.data))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wS6yo4tZhHhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "prompt = \"\"\"Can you create a 3d rendered image of a pig with wings\n",
        "and a top hat flying over a happy futuristic scifi city with lots\n",
        "of greenery?\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    # Use the dedicated image generation model.\n",
        "    model=\"gemini-2.0-flash-exp-image-generation\",\n",
        "    contents=prompt,\n",
        "    # This model requires both text and image outputs.\n",
        "    config=types.GenerateContentConfig(\n",
        "      response_modalities=['text', 'image']\n",
        "    )\n",
        ")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "\n",
        "  if part.text:\n",
        "    print(part.text)\n",
        "\n",
        "  elif part.inline_data:\n",
        "    display(Image(part.inline_data.data))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ilNacJY4g8IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "async def simplified_live_chat():\n",
        "    async with live_client.aio.live.connect(model=MODEL, config=CONFIG) as session:\n",
        "        print(\"Starting text-based chat with Gemini!\")\n",
        "        while True:\n",
        "            message = input(\"> \")\n",
        "            if message.lower() == \"quit\":\n",
        "                break\n",
        "            await session.send(input=message, end_of_turn=True)\n",
        "\n",
        "            async for chunk in session.receive():\n",
        "                if chunk.text is not None:\n",
        "                    print(chunk.text, end='')\n",
        "                # Handle other modalities (like images) if needed"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BpLsxJFcgxUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import time\n",
        "\n",
        "prompt = \"\"\"Write an essay defending why dogs are the best animals.\n",
        "Treat the essay as serious and include proper essay structure.\"\"\"\n",
        "\n",
        "response = client.models.generate_content_stream(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    print(chunk.text, end='')\n",
        "    time.sleep(0.3)  # Slow down to show streaming\n",
        "\n",
        "    # Uncomment this to see the individual tokens in separate sections.\n",
        "    # print(\"\\n----\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qKL0Ve04gHXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import time\n",
        "\n",
        "prompt = \"\"\"Write an essay defending why dogs are the best animals.\n",
        "Treat the essay as serious and include proper essay structure.\"\"\"\n",
        "\n",
        "response = client.models.generate_content_stream(\n",
        "    model='gemini-2.0-flash',  # Or another suitable model\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    print(chunk.text, end='')\n",
        "    time.sleep(0.3)  # Adjust delay as needed\n",
        "\n",
        "    # Uncomment to see individual tokens:\n",
        "    # print(\"\\n----\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nyQE7nAQgTzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import time\n",
        "\n",
        "prompt = \"\"\"Write an essay defending why dogs are the best animals.\n",
        "Treat the essay as serious and include proper essay structure.\"\"\"\n",
        "\n",
        "response = client.models.generate_content_stream(\n",
        "    model='gemini-2.0-flash',  # Or another suitable model\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    print(chunk.text, end='')\n",
        "    time.sleep(0.3)  # Adjust delay as needed\n",
        "\n",
        "    # Uncomment to see individual tokens:\n",
        "    # print(\"\\n----\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "PJb4qvwEgfn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!wget -nv https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_ZmOXQW-fwNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "print(\"Uploading to the File API...\")\n",
        "video_file = client.files.upload(file=\"BigBuckBunny_320x180.mp4\")\n",
        "print(\"Upload complete\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IUGC-yXXfx2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import time\n",
        "\n",
        "while video_file.state.name == \"PROCESSING\":\n",
        "    print('Waiting for video to be processed.')\n",
        "    time.sleep(10)\n",
        "    video_file = client.files.get(name=video_file.name)\n",
        "\n",
        "if video_file.state.name == \"FAILED\":\n",
        "  raise ValueError(video_file.state.name)\n",
        "\n",
        "print(f'Video processing complete: ' + video_file.uri)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "PxysFekWf0yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!wget -nv https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ENOLRv8Zf9Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "print(\"Uploading to the File API...\")\n",
        "video_file = client.files.upload(file=\"BigBuckBunny_320x180.mp4\")\n",
        "print(\"Upload complete\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sttCpzGIf9mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(\n",
        "    predicate=is_retriable)(genai.models.Models.generate_content)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lk93cDRgeihm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "prompt = [\n",
        "  \"What is this? Please describe it in detail.\",\n",
        "  PIL.Image.open(\"cake.jpg\"),\n",
        "]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8SHZWW0cfMnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(\n",
        "    predicate=is_retriable)(genai.models.Models.generate_content)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "VAP_gW4aezaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ooo2vhIicJSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt\n",
        ")\n",
        "from IPython.display import Markdown\n",
        "Markdown(response.text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ZOH4N0OGfPpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import PIL"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "33UtbgNGfb6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "response = client.models.generate_content(\n",
        "    model='gemini-pro',\n",
        "    contents='Write a poem about a cat.'\n",
        ")\n",
        "print(response.text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Yp4J-kUkc4jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dqYD3sUKc7aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!wget -nv https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3 -O speech.mp3"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CEwZabuLflGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install pydub"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "oij3XojXflhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!wget -nv https://storage.googleapis.com/generativeai-downloads/images/cake.jpg"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NyJzYgfdfDB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import PIL"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "6oPLEKcGfE8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "response = client.models.generate_content(\n",
        "    model='gemini-pro',\n",
        "    contents='Write a poem about a cat.'\n",
        ")\n",
        "print(response.text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "X3NAXXU5dUx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install -qU \"google-genai==1.9.0\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "YI6H35jYdx-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "WiTrqMt4eGBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "response = client.models.generate_content(\n",
        "    model='gemini-pro',\n",
        "    contents='Write a poem about a cat.'\n",
        ")\n",
        "print(response.text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "K_Zx8TzmeXli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install -qU \"google-genai==1.9.0\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5JgWy1i_dhh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from IPython.display import display, Image, Markdown, Audio\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "# Set up your API key from Kaggle secrets.\n",
        "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sXlboEwkdiMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google import genai\n",
        "    from google.genai import types\n",
        "    from IPython.display import display, Image, Markdown, Audio"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "d8ZLgQhtd41U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pKmxR7bHdQw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "response = client.models.generate_content(\n",
        "    model='gemini-pro',\n",
        "    contents='Write a poem about a cat.'\n",
        ")\n",
        "print(response.text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ALmcW26XdHNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:53:37.663561Z",
          "iopub.execute_input": "2025-04-04T04:53:37.66402Z",
          "iopub.status.idle": "2025-04-04T04:53:38.003282Z",
          "shell.execute_reply.started": "2025-04-04T04:53:37.663987Z",
          "shell.execute_reply": "2025-04-04T04:53:38.002318Z"
        },
        "id": "b6bWe6VkbKjz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you received an error response along the lines of `No user secrets exist for kernel id ...`, then you need to add your API key via `Add-ons`, `Secrets` **and** enable it.\n",
        "\n",
        "![Screenshot of the checkbox to enable GOOGLE_API_KEY secret](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)"
      ],
      "metadata": {
        "id": "SLTplbtJbKj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automated retry\n",
        "\n",
        "Set up a retry helper. This allows you to \"Run all\" without worrying about per-minute quota."
      ],
      "metadata": {
        "id": "uj89NtW-bKj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(\n",
        "    predicate=is_retriable)(genai.models.Models.generate_content)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:53:38.004457Z",
          "iopub.execute_input": "2025-04-04T04:53:38.004738Z",
          "iopub.status.idle": "2025-04-04T04:53:38.176651Z",
          "shell.execute_reply.started": "2025-04-04T04:53:38.004711Z",
          "shell.execute_reply": "2025-04-04T04:53:38.175774Z"
        },
        "id": "YxD4fFzKbKj0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-modal prompting\n",
        "\n",
        "As you may have noticed in AI Studio, the Gemini models support more than just text as input. You can provide pictures, videos, audio and more.\n",
        "\n",
        "\n",
        "### Images\n",
        "\n",
        "Start by downloading an image."
      ],
      "metadata": {
        "id": "iJJ09QsmbKj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "\n",
        "!wget -nv https://storage.googleapis.com/generativeai-downloads/images/cake.jpg\n",
        "Image('cake.jpg', width=500)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:53:38.177815Z",
          "iopub.execute_input": "2025-04-04T04:53:38.178291Z",
          "iopub.status.idle": "2025-04-04T04:53:39.595763Z",
          "shell.execute_reply.started": "2025-04-04T04:53:38.178257Z",
          "shell.execute_reply": "2025-04-04T04:53:39.594419Z"
        },
        "id": "eDORYywvbKj1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Python SDK can take a list as the prompt input. This list represents a sequence of prompt parts, and while each part needs to be a single mode (such as text or image), you can combine them together to form a multi-modal prompt."
      ],
      "metadata": {
        "id": "elcmzqf_bKj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\n",
        "  \"What is this? Please describe it in detail.\",\n",
        "  PIL.Image.open(\"cake.jpg\"),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt\n",
        ")\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:53:39.597884Z",
          "iopub.execute_input": "2025-04-04T04:53:39.598364Z",
          "iopub.status.idle": "2025-04-04T04:53:42.41863Z",
          "shell.execute_reply.started": "2025-04-04T04:53:39.598311Z",
          "shell.execute_reply": "2025-04-04T04:53:42.417463Z"
        },
        "id": "KV8B1YUAbKj1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image understanding in the Gemini models can be quite powerful. Check out [this guide on object detection](https://github.com/google-gemini/cookbook/blob/main/examples/Object_detection.ipynb), where the Gemini API identifies and highlights objects in an image based on a prompt.\n",
        "\n",
        "More input modes are supported, but first take a look at how to handle large files."
      ],
      "metadata": {
        "id": "f-xXQAA2bKj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use and upload files\n",
        "\n",
        "The Gemini models have very large context windows, most have at least 1 million tokens, and some have up to 2M input tokens! This translates to up to 2 hours of video or up to 19 hours of audio.\n",
        "\n",
        "As files of this length are typically too large to send in HTTP requests, the Gemini API provides a File API to that you can use to send large files in requests. It also means you can reuse the same files across different requests without having to re-upload the same content each time, improving your request latency.\n",
        "\n",
        "Note that some file limits exist, including how long they are kept. See [the vision docs](https://ai.google.dev/gemini-api/docs/vision?hl=en&lang=python) for more info."
      ],
      "metadata": {
        "id": "4DYunPN6bKj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Audio\n",
        "\n",
        "The Gemini API supports audio as an input medium. If you are the kind of person that takes audio notes with the Recorder or Voice Memo apps, this can be an efficient way to interact with your recordings ([check out this example](https://github.com/google-gemini/cookbook/blob/main/examples/Voice_memos.ipynb)), but you are not limited to personal notes.\n",
        "\n",
        "This MP3 audio recording is a State of the Union addess from US president Kennedy. Running the following code should give you a playable audio controller so you can listen to it."
      ],
      "metadata": {
        "id": "ujSyUJ1mbKj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "from IPython.display import Audio\n",
        "\n",
        "\n",
        "!wget -nv https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3 -O speech.mp3\n",
        "\n",
        "# This audio file is over 40mb, so trim the file before sending it to your browser.\n",
        "full_speech = AudioSegment.from_mp3(\"speech.mp3\")\n",
        "\n",
        "# Preview the first 30 seconds.\n",
        "first_30s_speech = full_speech[:30000]\n",
        "first_30s_speech\n",
        "\n",
        "# If you want to download and listen to the whole file, uncomment this.\n",
        "# Audio(\"speech.mp3\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:53:42.419799Z",
          "iopub.execute_input": "2025-04-04T04:53:42.420105Z",
          "iopub.status.idle": "2025-04-04T04:53:53.969816Z",
          "shell.execute_reply.started": "2025-04-04T04:53:42.420075Z",
          "shell.execute_reply": "2025-04-04T04:53:53.968692Z"
        },
        "id": "HL65f6_9bKj2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now upload the full file so it can be used in a prompt."
      ],
      "metadata": {
        "id": "8NhcCWKGbKj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_speech = client.files.upload(file='speech.mp3')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:53:53.971702Z",
          "iopub.execute_input": "2025-04-04T04:53:53.972649Z",
          "iopub.status.idle": "2025-04-04T04:53:55.203689Z",
          "shell.execute_reply.started": "2025-04-04T04:53:53.972597Z",
          "shell.execute_reply": "2025-04-04T04:53:55.202613Z"
        },
        "id": "AwdxmbjwbKj3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Who made the following speech? What were they positive about?\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=[prompt, uploaded_speech]\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:53:55.208318Z",
          "iopub.execute_input": "2025-04-04T04:53:55.208699Z",
          "iopub.status.idle": "2025-04-04T04:54:08.864848Z",
          "shell.execute_reply.started": "2025-04-04T04:53:55.208666Z",
          "shell.execute_reply": "2025-04-04T04:54:08.863813Z"
        },
        "id": "32czr293bKj3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video\n",
        "\n"
      ],
      "metadata": {
        "id": "PjEnb7VBbKj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now try out video understanding. In this example you will upload the [\"Big Buck Bunny\"](https://peach.blender.org/) short film and use the Gemini API to ask questions.\n",
        "\n",
        "> \"Big Buck Bunny\" is (c) copyright 2008, Blender Foundation / www.bigbuckbunny.org and [licensed](https://peach.blender.org/about/) under the [Creative Commons Attribution 3.0](http://creativecommons.org/licenses/by/3.0/) License.\n",
        "\n",
        "Start by downloading the video to this notebook and then uploading to the File API."
      ],
      "metadata": {
        "id": "XcTNSvmabKj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nv https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4\n",
        "\n",
        "print(\"Uploading to the File API...\")\n",
        "video_file = client.files.upload(file=\"BigBuckBunny_320x180.mp4\")\n",
        "print(\"Upload complete\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:54:08.866098Z",
          "iopub.execute_input": "2025-04-04T04:54:08.866407Z",
          "iopub.status.idle": "2025-04-04T04:54:11.950781Z",
          "shell.execute_reply.started": "2025-04-04T04:54:08.866371Z",
          "shell.execute_reply": "2025-04-04T04:54:11.949542Z"
        },
        "id": "q7h2oawzbKj4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Larger files can take some time to process when they upload. Ensure that the file is ready to use."
      ],
      "metadata": {
        "id": "h_ZotmxVbKj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "while video_file.state.name == \"PROCESSING\":\n",
        "    print('Waiting for video to be processed.')\n",
        "    time.sleep(10)\n",
        "    video_file = client.files.get(name=video_file.name)\n",
        "\n",
        "if video_file.state.name == \"FAILED\":\n",
        "  raise ValueError(video_file.state.name)\n",
        "\n",
        "print(f'Video processing complete: ' + video_file.uri)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:54:11.952682Z",
          "iopub.execute_input": "2025-04-04T04:54:11.953026Z",
          "iopub.status.idle": "2025-04-04T04:54:22.278635Z",
          "shell.execute_reply.started": "2025-04-04T04:54:11.952992Z",
          "shell.execute_reply": "2025-04-04T04:54:22.277524Z"
        },
        "id": "JjAhhnBRbKj4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that it is ready, use it in a prompt. Note that using large files in requests typically takes more time than a small text request, so increase the timeout and be aware that you may have to wait for this response."
      ],
      "metadata": {
        "id": "FcgBqTITbKj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What characters are in this movie?\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=[prompt, video_file]\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:54:22.280083Z",
          "iopub.execute_input": "2025-04-04T04:54:22.281007Z",
          "iopub.status.idle": "2025-04-04T04:54:32.710902Z",
          "shell.execute_reply.started": "2025-04-04T04:54:22.280959Z",
          "shell.execute_reply": "2025-04-04T04:54:32.709836Z"
        },
        "id": "NQeYUADGbKj5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming\n",
        "\n",
        "So far, you have been making transactional requests with the Gemini API - send the request, receive a full response. The API also supports response streaming.\n",
        "\n",
        "Use `generate_content_stream` instead of `generate_content` to trigger streaming mode. Note that it may render quickly - uncomment the final `print` to see each streamed chunk on its own."
      ],
      "metadata": {
        "id": "P2BfPEZfbKj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Write an essay defending why dogs are the best animals.\n",
        "Treat the essay as serious and include proper essay structure.\"\"\"\n",
        "\n",
        "response = client.models.generate_content_stream(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    print(chunk.text, end='')\n",
        "    time.sleep(0.3)  # Slow down to show streaming\n",
        "\n",
        "    # Uncomment this to see the individual tokens in separate sections.\n",
        "    # print(\"\\n----\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:54:32.712423Z",
          "iopub.execute_input": "2025-04-04T04:54:32.712755Z",
          "iopub.status.idle": "2025-04-04T04:54:37.336143Z",
          "shell.execute_reply.started": "2025-04-04T04:54:32.712722Z",
          "shell.execute_reply": "2025-04-04T04:54:37.335197Z"
        },
        "id": "i7_Zof82bKj5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Live API\n",
        "\n",
        "Starting with Gemini 2.0, you can interact through a Live API that supports bi-directional streaming of text or audio, and you can also provide a video input stream.\n",
        "\n",
        "This mode provides a powerful way to talk live to the Gemini API in a natural way. Notebooks don't easily support streaming audio as input, but read ahead to see how you can stream text in and audio out.\n",
        "\n",
        "When you're done you should head over to [AI Studio](https://aistudio.google.com/live) to try a live session. Try sharing your screen with the model in an audio chat to get the model to help you with tasks on your computer. It can help you write code or answer questions about whatever is on the screen. Gemini, if you're reading this, say hi!\n",
        "\n",
        "![Gemini Live API with screen sharing](https://storage.googleapis.com/generativeai-downloads/kaggle/live-screen-share.png)"
      ],
      "metadata": {
        "id": "20R2qc6BbKj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First define some configuration."
      ],
      "metadata": {
        "id": "ASfoB9fqbKj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "live_client = genai.Client(\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    http_options=types.HttpOptions(api_version='v1alpha'),\n",
        ")\n",
        "\n",
        "# For the Live API, you need to use the 2.0 experimental model and v1alpha API.\n",
        "MODEL = 'gemini-2.0-flash-exp'\n",
        "\n",
        "# Set up an audio-out session.\n",
        "CONFIG = types.LiveConnectConfig(\n",
        "    response_modalities=[\"AUDIO\"],\n",
        "    speech_config=types.SpeechConfig(\n",
        "        voice_config=types.VoiceConfig(\n",
        "\n",
        "            # Try changing the voice! Pick one of Puck, Charon, Kore, Fenrir, Aoede\n",
        "            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name=\"Kore\")\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:54:37.337333Z",
          "iopub.execute_input": "2025-04-04T04:54:37.337633Z",
          "iopub.status.idle": "2025-04-04T04:54:37.369873Z",
          "shell.execute_reply.started": "2025-04-04T04:54:37.337604Z",
          "shell.execute_reply": "2025-04-04T04:54:37.368903Z"
        },
        "id": "fEdY9dkgbKj6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a helper for buffering audio responses."
      ],
      "metadata": {
        "id": "w2bS4yBDbKj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import contextlib\n",
        "import wave\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def wave_file(filename, channels=1, rate=24000, sample_width=2):\n",
        "  \"\"\"Context managed to buffer audio into a wave file with suitable headers.\"\"\"\n",
        "  with wave.open(filename, \"wb\") as wf:\n",
        "    wf.setnchannels(channels)\n",
        "    wf.setsampwidth(sample_width)\n",
        "    wf.setframerate(rate)\n",
        "    yield wf"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:54:37.37105Z",
          "iopub.execute_input": "2025-04-04T04:54:37.371376Z",
          "iopub.status.idle": "2025-04-04T04:54:37.377339Z",
          "shell.execute_reply.started": "2025-04-04T04:54:37.371345Z",
          "shell.execute_reply": "2025-04-04T04:54:37.376222Z"
        },
        "id": "0Ab2X7X7bKj6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now start the session. You may recall from the course that as the Live API requires real-time interaction, we need to set up the conversation ahead of time. In this example you have a single line of dialog pre-scripted."
      ],
      "metadata": {
        "id": "bXhbHplIbKj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async with live_client.aio.live.connect(model=MODEL, config=CONFIG) as session:\n",
        "\n",
        "    message = \"Hi there, can you tell me something fun about spiders?\"\n",
        "    print('>', message)\n",
        "\n",
        "    # Send the message to the model.\n",
        "    await session.send(input=message, end_of_turn=True)\n",
        "\n",
        "    # Set up a temporary audio file to store the audio response.\n",
        "    with wave_file(file_name := \"audio_chat.wav\") as wav:\n",
        "\n",
        "      # Start receiving and handling the response\n",
        "      async for chunk in session.receive():\n",
        "        # Text responses.\n",
        "        if chunk.text is not None:\n",
        "          print(chunk.text, end='')\n",
        "\n",
        "        # Audio responses.\n",
        "        elif chunk.data is not None:\n",
        "          wav.writeframes(chunk.data)\n",
        "          print('.', end='')\n",
        "\n",
        "    display(Audio(file_name, autoplay=True))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:55:24.442339Z",
          "iopub.execute_input": "2025-04-04T04:55:24.44311Z",
          "iopub.status.idle": "2025-04-04T04:55:34.015641Z",
          "shell.execute_reply.started": "2025-04-04T04:55:24.443067Z",
          "shell.execute_reply": "2025-04-04T04:55:34.014598Z"
        },
        "id": "h4AvKkCTbKj7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can try interacting in real time! As this example requires you to type your input live, it is commented out. Be sure to uncomment the last line to run the example!"
      ],
      "metadata": {
        "id": "aJzfL5QibKj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def start_chat_with_user_input():\n",
        "    async with live_client.aio.live.connect(model=MODEL, config=CONFIG) as session:\n",
        "\n",
        "      print('Starting text-in, audio-out chat! Type \"q\" to quit.')\n",
        "      while (message := input('> ')).lower()[0] != 'q':\n",
        "\n",
        "        # Send the message to the model.\n",
        "        await session.send(input=message, end_of_turn=True)\n",
        "\n",
        "        # Set up a temporary audio file to store the audio response.\n",
        "        with wave_file(file_name := \"audio_chat.wav\") as wav:\n",
        "\n",
        "          # Start receiving and handling the response\n",
        "          async for chunk in session.receive():\n",
        "            # Text responses.\n",
        "            if chunk.text is not None:\n",
        "              print(chunk.text, end='')\n",
        "\n",
        "            # Audio responses.\n",
        "            elif chunk.data is not None:\n",
        "              wav.writeframes(chunk.data)\n",
        "              print('.', end='')\n",
        "\n",
        "        display(Audio(file_name, autoplay=True))\n",
        "\n",
        "# Uncomment this to run the live chat. 'q' will end the conversation.\n",
        "# await start_chat_with_user_input()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:54:47.416267Z",
          "iopub.execute_input": "2025-04-04T04:54:47.41659Z",
          "iopub.status.idle": "2025-04-04T04:54:47.424435Z",
          "shell.execute_reply.started": "2025-04-04T04:54:47.416557Z",
          "shell.execute_reply": "2025-04-04T04:54:47.423263Z"
        },
        "id": "i7E5NZ-kbKkA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Live API can do more than just chat too, you can use code generation/execution, add tools like Google Search as well as bring your own custom tools, like you did on day 3.  For more examples check out the [tools](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb) and [plotting](https://github.com/google-gemini/cookbook/blob/main/examples/LiveAPI_plotting_and_mapping.ipynb) guides in the cookbook."
      ],
      "metadata": {
        "id": "ZfNZ-GjqbKkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image generation\n",
        "\n",
        "New in the Gemini API is the ability to generate images. Try out"
      ],
      "metadata": {
        "id": "cAyXxSR1bKkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Can you create a 3d rendered image of a pig with wings\n",
        "and a top hat flying over a happy futuristic scifi city with lots\n",
        "of greenery?\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    # Use the dedicated image generation model.\n",
        "    model=\"gemini-2.0-flash-exp-image-generation\",\n",
        "    contents=prompt,\n",
        "    # This model requires both text and image outputs.\n",
        "    config=types.GenerateContentConfig(\n",
        "      response_modalities=['text', 'image']\n",
        "    )\n",
        ")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "\n",
        "  if part.text:\n",
        "    print(part.text)\n",
        "\n",
        "  elif part.inline_data:\n",
        "    display(Image(part.inline_data.data))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:54:47.425873Z",
          "iopub.execute_input": "2025-04-04T04:54:47.426262Z",
          "iopub.status.idle": "2025-04-04T04:54:51.183358Z",
          "shell.execute_reply.started": "2025-04-04T04:54:47.426227Z",
          "shell.execute_reply": "2025-04-04T04:54:51.182223Z"
        },
        "id": "R4mvfUCsbKkA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image generation works in a chat conversation too, so you can ask the model follow-up questions and it can make edits to the image. You can also provide images as input, for the model to start from.\n",
        "\n",
        "You can try writing this code yourself, but the easiest way to explore the feature is through [the image generation model in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-exp-image-generation)."
      ],
      "metadata": {
        "id": "c1TZmyYybKkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context caching\n",
        "\n",
        "Context caching is a technique that allows you to cache part of a request, such that it does not need to be re-processed by the model each time you use it. This is useful, for example, for asking new questions of the same documents.\n",
        "\n",
        "Note that context caching typically charges per million tokens per hour of caching. If you are using a paid API key, be sure to set your cache expiry or delete the cached tokens after use. See the [billing page](https://ai.google.dev/pricing) for more info. The Flash 1.5 model also supports caching on the free tier.\n",
        "\n",
        "To ensure that the cache remains valid, caches are created by specifying versioned model names, so `gemini-1.5-flash-001`, where `-001` signifies the model version."
      ],
      "metadata": {
        "id": "uPHkuPHCbKkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the transcript\n",
        "!wget -nv -O apollo11.txt https://storage.googleapis.com/generativeai-downloads/data/a11.txt\n",
        "\n",
        "# Upload to the File API\n",
        "transcript_file = client.files.upload(file='apollo11.txt')\n",
        "\n",
        "# Create a cache\n",
        "apollo_cache = client.caches.create(\n",
        "    model='gemini-1.5-flash-001',\n",
        "    config=types.CreateCachedContentConfig(\n",
        "        system_instruction=\"You are a space history buff that enjoys discussing and explaining historical space events.\",\n",
        "        contents=[transcript_file],\n",
        "    ),\n",
        ")\n",
        "\n",
        "apollo_cache"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:54:51.184591Z",
          "iopub.execute_input": "2025-04-04T04:54:51.184902Z",
          "iopub.status.idle": "2025-04-04T04:54:54.754652Z",
          "shell.execute_reply.started": "2025-04-04T04:54:51.18487Z",
          "shell.execute_reply": "2025-04-04T04:54:54.753392Z"
        },
        "id": "-hWHG0U2bKkB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can use this cache when generating content."
      ],
      "metadata": {
        "id": "GaUrPvFVbKkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=apollo_cache.model,\n",
        "    config=types.GenerateContentConfig(\n",
        "        cached_content=apollo_cache.name,\n",
        "    ),\n",
        "    contents=\"Find a nice moment from this transcript\"\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:54:54.756534Z",
          "iopub.execute_input": "2025-04-04T04:54:54.757003Z",
          "iopub.status.idle": "2025-04-04T04:55:05.114669Z",
          "shell.execute_reply.started": "2025-04-04T04:54:54.756952Z",
          "shell.execute_reply": "2025-04-04T04:55:05.113622Z"
        },
        "id": "BiRsUHPobKkB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The response object includes information about the number of tokens that were cached and otherwise used in the prompt."
      ],
      "metadata": {
        "id": "vGtWYnjUbKkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response.usage_metadata.to_json_dict()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:55:05.115911Z",
          "iopub.execute_input": "2025-04-04T04:55:05.116248Z",
          "iopub.status.idle": "2025-04-04T04:55:05.122953Z",
          "shell.execute_reply.started": "2025-04-04T04:55:05.116216Z",
          "shell.execute_reply": "2025-04-04T04:55:05.121899Z"
        },
        "id": "IY1MXLenbKkC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And you can calculate how many non-cached tokens were used as input."
      ],
      "metadata": {
        "id": "KLtIgIswbKkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response.usage_metadata.total_token_count - response.usage_metadata.cached_content_token_count"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:55:05.124468Z",
          "iopub.execute_input": "2025-04-04T04:55:05.124884Z",
          "iopub.status.idle": "2025-04-04T04:55:05.137306Z",
          "shell.execute_reply.started": "2025-04-04T04:55:05.124837Z",
          "shell.execute_reply": "2025-04-04T04:55:05.136059Z"
        },
        "id": "4tpWv7sFbKkC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delete the cache\n",
        "\n",
        "To ensure you are not charged for any cached tokens you are not using, delete the cache. If you are on the free tier, you won't be charged, but it's good practice to clean up when you're done."
      ],
      "metadata": {
        "id": "2bSEfH4GbKkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(apollo_cache.name)\n",
        "client.caches.delete(name=apollo_cache.name)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T04:55:05.13891Z",
          "iopub.execute_input": "2025-04-04T04:55:05.13938Z",
          "iopub.status.idle": "2025-04-04T04:55:05.321639Z",
          "shell.execute_reply.started": "2025-04-04T04:55:05.139331Z",
          "shell.execute_reply": "2025-04-04T04:55:05.320469Z"
        },
        "id": "GA9iyl7qbKkC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further reading\n",
        "\n",
        "Take a look through the [Gemini API cookbook](https://github.com/google-gemini/cookbook) for more feature-based quickstarts and complex examples. And don't forget to explore [AI Studio](https://aistudio.google.com) to try out API features directly in the browser.\n",
        "\n",
        "If you enabled billing on your API key and are finished with the key, you can [turn it off](https://ai.google.dev/gemini-api/docs/billing) unless you plan on using it again.\n",
        "\n",
        "And **thank you** for coming with us on this 5-day learning journey!\n",
        "\n",
        "\\- [Mark McD](https://linktr.ee/markmcd)"
      ],
      "metadata": {
        "id": "4zPkvwlBbKkD"
      }
    }
  ]
}