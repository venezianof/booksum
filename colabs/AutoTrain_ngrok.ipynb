{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venezianof/booksum/blob/main/colabs/AutoTrain_ngrok.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "II6F7ThkI10I",
        "outputId": "1e824664-11b1-425a-cc0a-e44c266320d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-04-02 20:17:56\u001b[0m | \u001b[36mautotrain.cli.run_app\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m132\u001b[0m - \u001b[1mAutoTrain Public URL: NgrokTunnel: \"https://5032-34-125-26-254.ngrok-free.app\" -> \"http://localhost:7860\"\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-04-02 20:17:56\u001b[0m | \u001b[36mautotrain.cli.run_app\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1mPlease wait for the app to load...\u001b[0m\n",
            "INFO     | 2025-04-02 20:18:02 | autotrain.app.ui_routes:<module>:31 - Starting AutoTrain...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models?filter=transformers&filter=fill-mask&sort=downloads&direction=-1&limit=100\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/uvicorn\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1161, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1082, in main\n",
            "    rv = self.invoke(ctx)\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1443, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 788, in invoke\n",
            "    return __callback(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 412, in main\n",
            "    run(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 579, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 190, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n",
            "    return future.result()\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    await self._serve(sockets)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 77, in _serve\n",
            "    config.load()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/config.py\", line 435, in load\n",
            "    self.loaded_app = import_from_string(self.app)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/importer.py\", line 19, in import_from_string\n",
            "    module = importlib.import_module(module_str)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/autotrain/app/app.py\", line 10, in <module>\n",
            "    from autotrain.app.ui_routes import ui_router\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/autotrain/app/ui_routes.py\", line 39, in <module>\n",
            "    MODEL_CHOICE = fetch_models()\n",
            "                   ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/autotrain/app/models.py\", line 338, in fetch_models\n",
            "    _mc[\"text-classification\"] = _fetch_text_classification_models()\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/autotrain/app/models.py\", line 35, in _fetch_text_classification_models\n",
            "    hub_models1 = list(\n",
            "                  ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\", line 1915, in list_models\n",
            "    for item in items:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_pagination.py\", line 37, in paginate\n",
            "    hf_raise_for_status(r)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 477, in hf_raise_for_status\n",
            "    raise _format(HfHubHTTPError, str(e), response) from e\n",
            "huggingface_hub.errors.HfHubHTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models?filter=transformers&filter=fill-mask&sort=downloads&direction=-1&limit=100 (Request ID: Root=1-67ed9b7a-4f52b53d4cf65d7d32e36c55;c0f7a8eb-3420-467c-a207-fea65a73540d)\n",
            "\n",
            "Invalid credentials in Authorization header\n"
          ]
        }
      ],
      "source": [
        "#@title ðŸ¤— AutoTrain\n",
        "#@markdown In order to use this colab\n",
        "#@markdown - Enter your [Hugging Face Write Token](https://huggingface.co/settings/tokens)\n",
        "#@markdown - Enter your [ngrok auth token](https://dashboard.ngrok.com/get-started/your-authtoken)\n",
        "huggingface_token = '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 ' # @param {type:\"string\"}\n",
        "ngrok_token = \"2u9v0BA9aZJFyqeRmLGPjyyVOBa_PFyoMKxBsUrmB8quxAq2\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown\n",
        "#@markdown - Attach appropriate accelerator `Runtime > Change runtime type > Hardware accelerator`\n",
        "#@markdown - click `Runtime > Run all`\n",
        "#@markdown - Follow the link to access the UI\n",
        "#@markdown - Training happens inside this Google Colab\n",
        "#@markdown - report issues / feature requests [here](https://github.com/huggingface/autotrain-advanced/issues)\n",
        "\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = str(huggingface_token)\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = str(ngrok_token)\n",
        "os.environ[\"AUTOTRAIN_LOCAL\"] = \"1\"\n",
        "\n",
        "!pip install -U autotrain-advanced > install_logs.txt 2>&1\n",
        "!autotrain app --share"
      ]
    },
    {
      "source": [
        "!pip install smolagents"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SbS-YHMuE5wI",
        "outputId": "698d8b40-96c4-4b68-cb1c-2cc38ace842e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: smolagents in /usr/local/lib/python3.11/dist-packages (1.13.0)\n",
            "Collecting huggingface-hub>=0.28.0 (from smolagents)\n",
            "  Using cached huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from smolagents) (2.32.3)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.11/dist-packages (from smolagents) (13.9.4)\n",
            "Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from smolagents) (3.1.6)\n",
            "Requirement already satisfied: pillow<11.2.0,>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from smolagents) (11.0.0)\n",
            "Requirement already satisfied: markdownify>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from smolagents) (1.1.0)\n",
            "Requirement already satisfied: duckduckgo-search>=6.3.7 in /usr/local/lib/python3.11/dist-packages (from smolagents) (2025.4.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from smolagents) (1.1.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search>=6.3.7->smolagents) (8.1.8)\n",
            "Requirement already satisfied: primp>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search>=6.3.7->smolagents) (0.14.0)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search>=6.3.7->smolagents) (5.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.0->smolagents) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.0->smolagents) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.0->smolagents) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.0->smolagents) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.0->smolagents) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.0->smolagents) (4.13.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.1.4->smolagents) (3.0.2)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /usr/local/lib/python3.11/dist-packages (from markdownify>=0.14.1->smolagents) (4.13.3)\n",
            "Requirement already satisfied: six<2,>=1.15 in /usr/local/lib/python3.11/dist-packages (from markdownify>=0.14.1->smolagents) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->smolagents) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->smolagents) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->smolagents) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->smolagents) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->smolagents) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->smolagents) (2.18.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.9->markdownify>=0.14.1->smolagents) (2.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->smolagents) (0.1.2)\n",
            "Using cached huggingface_hub-0.30.1-py3-none-any.whl (481 kB)\n",
            "Installing collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.27.0\n",
            "    Uninstalling huggingface-hub-0.27.0:\n",
            "      Successfully uninstalled huggingface-hub-0.27.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "autotrain-advanced 0.8.36 requires huggingface-hub==0.27.0, but you have huggingface-hub 0.30.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.30.1\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from smolagents import DuckDuckGoSearchTool"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "yRoBcxrdE6gQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "source": [
        "search_tool = DuckDuckGoSearchTool()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UttkG-DeE7G8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "source": [
        "results = search_tool(\"Who's the current President of France?\")\n",
        "   print(results)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-ITtssdME7k9",
        "outputId": "1e9c5373-8063-4ef7-be09-80a20be7eeaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-5-0bd2e379901c>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-0bd2e379901c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print(results)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install llama-index huggingface_hub"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3XmH2ht5JJcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from llama_index.core.agent.workflow import AgentWorkflow\n",
        "   from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "\n",
        "   # Initialize the Hugging Face model\n",
        "   llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
        "\n",
        "   # Define your tools (search_tool, weather_info_tool, hub_stats_tool)\n",
        "   # ... (Refer to previous responses for definitions) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JJh5ihn1JKFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install langgraph langchain-huggingface"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-pbqCFBDJebO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from typing import TypedDict, Annotated\n",
        "   from langgraph.graph.message import add_messages\n",
        "   from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
        "   from langgraph.prebuilt import ToolNode\n",
        "   from langgraph.graph import START, StateGraph\n",
        "   from langgraph.prebuilt import tools_condition\n",
        "   from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "   # Generate the chat interface, including the tools\n",
        "   # Replace 'YOUR_HUGGINGFACEHUB_API_TOKEN' with your actual token\n",
        "   HUGGINGFACEHUB_API_TOKEN = 'YOUR_HUGGINGFACEHUB_API_TOKEN'\n",
        "   llm = HuggingFaceEndpoint(\n",
        "       repo_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
        "       huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
        "   )\n",
        "\n",
        "   chat = ChatHuggingFace(llm=llm, verbose=True)\n",
        "   tools = [search_tool, weather_info_tool, hub_stats_tool]  # Define your tools as before\n",
        "   chat_with_tools = chat.bind_tools(tools)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ofKzjohCJfgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Generate the AgentState and Agent graph\n",
        "   class AgentState(TypedDict):\n",
        "       messages: Annotated[list[AnyMessage], add_messages]\n",
        "\n",
        "   def assistant(state: AgentState):\n",
        "       return {\n",
        "           \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
        "       }\n",
        "\n",
        "   ## The graph\n",
        "   builder = StateGraph(AgentState)\n",
        "\n",
        "   # Define nodes: these do the work\n",
        "   builder.add_node(\"assistant\", assistant)\n",
        "   builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "   # Define edges: these determine how the control flow moves\n",
        "   builder.add_edge(START, \"assistant\")\n",
        "   builder.add_conditional_edges(\n",
        "       \"assistant\",\n",
        "       # If the latest message requires a tool, route to tools\n",
        "       # Otherwise, provide a direct response\n",
        "       tools_condition,\n",
        "   )\n",
        "   builder.add_edge(\"tools\", \"assistant\")\n",
        "   alfred = builder.compile()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QebSIPxWJhq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "messages = [HumanMessage(content=\"Who is Facebook and what's their most popular model?\")]\n",
        "   response = alfred.invoke({\"messages\": messages})\n",
        "\n",
        "   print(\"ðŸŽ© Alfred's Response:\")\n",
        "   print(response['messages'][-1].content)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OqEg137AJijX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# DuckDuckGoSearchTool\n",
        "   from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec\n",
        "   from llama_index.core.tools import FunctionTool\n",
        "\n",
        "   tool_spec = DuckDuckGoSearchToolSpec()\n",
        "   search_tool = FunctionTool.from_defaults(tool_spec.duckduckgo_full_search)\n",
        "\n",
        "   # WeatherInfoTool\n",
        "   import random\n",
        "   from llama_index.core.tools import FunctionTool\n",
        "\n",
        "   def get_weather_info(location: str) -> str:\n",
        "       \"\"\"Fetches dummy weather information for a given location.\"\"\"\n",
        "       weather_conditions = [\n",
        "           {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
        "           {\"condition\": \"Clear\", \"temp_c\": 25},\n",
        "           {\"condition\": \"Windy\", \"temp_c\": 20}\n",
        "       ]\n",
        "       data = random.choice(weather_conditions)\n",
        ""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Qg5CprizJL9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Create Alfred with all the tools\n",
        "   alfred = AgentWorkflow.from_tools_or_functions(\n",
        "       [search_tool, weather_info_tool, hub_stats_tool],\n",
        "       llm=llm\n",
        "   )"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wx1a-M-xJPUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Example query Alfred might receive during the gala\n",
        "   response = await alfred.arun(\"What is Facebook and what's their most popular model?\")  # Use arun for async\n",
        "\n",
        "   print(\"ðŸŽ© Alfred's Response:\")\n",
        "   print(response)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nlPuCgpBJPuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install smolagents huggingface_hub"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mg9VmZtdIzu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from smolagents import CodeAgent, HfApiModel, Tool\n",
        "   from huggingface_hub import list_models\n",
        "   import random\n",
        "\n",
        "   # Initialize the Hugging Face model\n",
        "   model = HfApiModel()\n",
        "\n",
        "   # Define and initialize your tools (search_tool, weather_info_tool, hub_stats_tool)\n",
        "   # You'll need to define and initialize these tools as you did in the previous examples.\n",
        "   # ... (Code for the tools as provided in earlier responses) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0sIH9_ADI0NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# DuckDuckGoSearchTool\n",
        "from smolagents import DuckDuckGoSearchTool\n",
        "search_tool = DuckDuckGoSearchTool()\n",
        "\n",
        "# WeatherInfoTool\n",
        "class WeatherInfoTool(Tool):\n",
        "    name = \"weather_info\"\n",
        "    description = \"Fetches dummy weather information for a given location.\"\n",
        "    inputs = {\n",
        "        \"location\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The location to get weather information for.\"\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def forward(self, location: str):\n",
        "        # Dummy weather data\n",
        "        weather_conditions = [\n",
        "            {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
        "            {\"condition\": \"Clear\", \"temp_c\": 25},\n",
        "            {\"condition\": \"Windy\", \"temp_c\": 20}\n",
        "        ]\n",
        "        # Randomly select a weather condition\n",
        "        data = random.choice(weather_conditions)\n",
        "        return f\"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C\"\n",
        "weather_info_tool = WeatherInfoTool()\n",
        "\n",
        "# HubStatsTool\n",
        "class HubStatsTool(Tool):\n",
        "    name = \"hub_stats\"\n",
        "    description = \""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mZBOUFgXI121"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Create Alfred with all the tools\n",
        "   alfred = CodeAgent(\n",
        "       tools=[search_tool, weather_info_tool, hub_stats_tool],\n",
        "       model=model\n",
        "   )"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Q9kB2ArWI5ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Example query Alfred might receive during the gala\n",
        "   response = alfred.run(\"What is Facebook and what's their most popular model?\")\n",
        "\n",
        "   print(\"ðŸŽ© Alfred's Response:\")\n",
        "   print(response)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qr43TN4DI6Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install smolagents huggingface_hub"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qcEHPo9xIJ7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install llama-index huggingface_hub"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "XDyvZaJuIR-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import random\n",
        "   from llama_index.core.tools import FunctionTool\n",
        "   from huggingface_hub import list_models\n",
        "\n",
        "   def get_hub_stats(author: str) -> str:\n",
        "       \"\"\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\"\"\n",
        "       try:\n",
        "           # List models from the specified author, sorted by downloads\n",
        "           models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n",
        "\n",
        "           if models:\n",
        "               model = models[0]\n",
        "               return f\"The most downloaded model by {author} is {model.id} with {model.downloads"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JxX2hl5DISZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Initialize the tool\n",
        "   hub_stats_tool = FunctionTool.from_defaults(get_hub_stats)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-LuvVAmfIT1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Example usage\n",
        "   print(hub_stats_tool.invoke(\"facebook\"))  # Example: Get the most downloaded model by Facebook"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-6NO1GWsIUNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install langchain huggingface_hub"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EdZw3Hb4Ieb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain.tools import Tool\n",
        "from huggingface_hub import list_models\n",
        "\n",
        "def get_hub_stats(author: str) -> str:\n",
        "    \"\"\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\"\"\n",
        "    try:\n",
        "        # List models from the specified author, sorted by downloads\n",
        "        models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n",
        "\n",
        "        if models:\n",
        "            model = models[0]\n",
        "            return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"\n",
        "        else:\n",
        "            return f\"No models found for author {author}.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching models for {author}: {str(e)}\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Q8-Zksk5IgDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Initialize the tool\n",
        "hub_stats_tool = Tool(\n",
        "    name=\"get_hub_stats\",\n",
        "    func=get_hub_stats,\n",
        "    description=\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "75BmB06uIhQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Example usage\n",
        "print(hub_stats_tool.run(\"facebook\")) # Example: Get the most downloaded model by Facebook"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "__NcarUmIiMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install langchain_community"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "l7ScQolvHFIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QOemmKuqHFc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "search_tool = DuckDuckGoSearchRun()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "US4ip4VkHFsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "results = search_tool.invoke(\"Who's the current President of France?\")\n",
        "   print(results)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "HAnqmuFSHF7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install smolagents"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bWxS1TtFHRbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from smolagents import Tool\n",
        "import random\n",
        "\n",
        "class WeatherInfoTool(Tool):\n",
        "    name = \"weather_info\"\n",
        "    description = \"Fetches dummy weather information for a given location.\"\n",
        "    inputs = {\n",
        "        \"location\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The location to get weather information for.\"\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def forward(self, location: str):\n",
        "        # Dummy weather data\n",
        "        weather_conditions = [\n",
        "            {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
        "            {\"condition\": \"Clear\", \"temp_c\": 25},\n",
        "            {\"condition\": \"Windy\","
      ],
      "cell_type": "code",
      "metadata": {
        "id": "THtMpf5MHR1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "weather_info_tool = WeatherInfoTool()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wwRmExeZHTDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "result = weather_info_tool.forward(\"Paris\")\n",
        "    print(result)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "S-0Yd5OKHTeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install smolagents"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IAHkG0LrHcrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from smolagents import Tool\n",
        "import random\n",
        "\n",
        "class WeatherInfoTool(Tool):\n",
        "    name = \"weather_info\"\n",
        "    description = \"Fetches dummy weather information for a given location.\"\n",
        "    inputs = {\n",
        "        \"location\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The location to get weather information for.\"\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def forward(self, location: str):\n",
        "        # Dummy weather data\n",
        "        weather_conditions = [\n",
        "            {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
        "            {\"condition\": \"Clear\", \"temp_c\": 25},\n",
        "            {\"condition\": \"Windy\", \"temp_c\": 20}\n",
        "        ]\n",
        "        # Randomly select a weather condition\n",
        "        data = random.choice(weather_conditions)\n",
        "        return f\"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "kXdM22rPHc-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "weather_info_tool = WeatherInfoTool()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BlXYhLguHdtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "result = weather_info_tool.forward(\"Paris\")\n",
        "    print(result)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ERu5lgSwHeDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import random\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def get_weather_info(location: str) -> str:\n",
        "    \"\"\"Fetches dummy weather information for a given location.\"\"\"\n",
        "    # Dummy weather data\n",
        "    weather_conditions = [\n",
        "        {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
        "        {\"condition\": \"Clear\", \"temp_c\": 25},\n",
        "        {\"condition\": \"Windy\", \"temp_c\": 20}\n",
        "    ]\n",
        "    # Randomly select a weather condition\n",
        "    data = random.choice(weather_conditions)\n",
        "    return f\"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-fmq7A1xHy9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "weather_info_tool = FunctionTool.from_defaults(get_weather_info)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "aVUQmucyH0Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Make a prediction using the tool\n",
        "weather_info_tool.invoke(\"Paris\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "T0ASbZ29H0dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install langchain"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3PoXu0VHH-pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import random\n",
        "   from langchain.tools import Tool\n",
        "\n",
        "   def get_weather_info(location: str) -> str:\n",
        "       \"\"\"Fetches dummy weather information for a given location.\"\"\"\n",
        "       # Dummy weather data\n",
        "       weather_conditions = [\n",
        "           {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
        "           {\"condition\": \"Clear\", \"temp_c\": 25},\n",
        "           {\"condition\": \"Windy\", \"temp_c\": 20}\n",
        "       ]\n",
        "       # Randomly select a weather condition\n",
        "       data = random.choice(weather_conditions)\n",
        "       return f\"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "A6XL9xXHH_Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Initialize the tool\n",
        "   weather_info_tool = Tool(\n",
        "       name=\"get_weather_info\",\n",
        "       func=get_weather_info,\n",
        "       description=\"Fetches dummy weather information for a given location.\"\n",
        "   )"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "6yk2Sm4cIAfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "result = weather_info_tool.run(\"Paris\")\n",
        "    print(result)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2tg0HRalIBQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import random\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def get_weather_info(location: str) -> str:\n",
        "    \"\"\"Fetches dummy weather information for a given location.\"\"\"\n",
        "    # Dummy weather data\n",
        "    weather_conditions = [\n",
        "        {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
        "        {\"condition\": \"Clear\", \"temp_c\": 25},\n",
        "        {\"condition\": \"Windy\", \"temp_c\": 20}\n",
        "    ]\n",
        "    # Randomly select a weather condition\n",
        "    data = random.choice(weather_conditions)\n",
        "    return f\"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gP-wro_jG0hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "weather_info_tool = FunctionTool.from_defaults(get_weather_info)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "phmDQboAG1PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Make a prediction using the tool\n",
        "weather_info_tool.invoke(\"Paris\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "b31Q5vueG1jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install langchain_community"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "iXl-VAx5GeTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fdizqAA7Ges9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "search_tool = DuckDuckGoSearchRun()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ovaiHQ69GfAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "results = search_tool.invoke(\"Who's the current President of France?\")\n",
        "   print(results)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jCNYtr3fGfTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install llama-index"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Z3__IsQiFqAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec\n",
        "   from llama_index.core.tools import FunctionTool"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NCbn0qTYFqp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "tool_spec = DuckDuckGoSearchToolSpec()\n",
        "   search_tool = FunctionTool.from_defaults(tool_spec.duckduckgo_full_search)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "B3JZVGwrFq_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "response = search_tool(\"Who's the current President of France?\")\n",
        "   print(response.raw_output[-1]['body'])  # Access the body of the last result"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "X4NfAbdTFrXQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}