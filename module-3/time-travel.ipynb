{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venezianof/booksum/blob/main/module-3/time-travel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9902a6a3",
      "metadata": {
        "id": "9902a6a3"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/time-travel.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239536-lesson-5-time-travel)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba98beac-d461-4d7d-878a-11beca03ea1c",
      "metadata": {
        "id": "ba98beac-d461-4d7d-878a-11beca03ea1c"
      },
      "source": [
        "# Time travel\n",
        "\n",
        "## Review\n",
        "\n",
        "We discussed motivations for human-in-the-loop:\n",
        "\n",
        "(1) `Approval` - We can interrupt our agent, surface state to a user, and allow the user to accept an action\n",
        "\n",
        "(2) `Debugging` - We can rewind the graph to reproduce or avoid issues\n",
        "\n",
        "(3) `Editing` - You can modify the state\n",
        "\n",
        "We showed how breakpoints can stop the graph at specific nodes or allow the graph to dynamically interrupt itself.\n",
        "\n",
        "Then we showed how to proceed with human approval or directly edit the graph state with human feedback.\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, let's show how LangGraph [supports debugging](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/time-travel/) by viewing, re-playing, and even forking from past states.\n",
        "\n",
        "We call this `time travel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "bd48aeb6-8478-4cb4-aef1-d524b80824d3",
      "metadata": {
        "id": "bd48aeb6-8478-4cb4-aef1-d524b80824d3"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain_openai langgraph_sdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "7d32093f",
      "metadata": {
        "id": "7d32093f"
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0497d316-832a-4668-b133-fd317ee81220",
      "metadata": {
        "id": "0497d316-832a-4668-b133-fd317ee81220"
      },
      "source": [
        "Let's build our agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "d64ab3a1-b39c-4176-88c7-791a0b80c725",
      "metadata": {
        "id": "d64ab3a1-b39c-4176-88c7-791a0b80c725"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "# This will be a tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a by b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a / b\n",
        "\n",
        "tools = [add, multiply, divide]\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "1d8622a9-57cd-44dc-8696-46c5ab32d0b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "1d8622a9-57cd-44dc-8696-46c5ab32d0b9",
        "outputId": "4be94ded-249f-4784-f220-e14488b65c5f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import MessagesState\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "from langgraph.prebuilt import tools_condition, ToolNode\n",
        "\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# System message\n",
        "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
        "\n",
        "# Node\n",
        "def assistant(state: MessagesState):\n",
        "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
        "\n",
        "# Graph\n",
        "builder = StateGraph(MessagesState)\n",
        "\n",
        "# Define nodes: these do the work\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define edges: these determine the control flow\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
        "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "\n",
        "memory = MemorySaver()\n",
        "graph = builder.compile(checkpointer=MemorySaver())\n",
        "\n",
        "# Show\n",
        "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fab18a04-1329-47ac-a25b-4e01bf756e2a",
      "metadata": {
        "id": "fab18a04-1329-47ac-a25b-4e01bf756e2a"
      },
      "source": [
        "Let's run it, as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "05b2ab62-82bc-4356-8d5b-2d4f49069fdd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "05b2ab62-82bc-4356-8d5b-2d4f49069fdd",
        "outputId": "92d6317a-46bb-44ef-d8ec-a4389bd74bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-lf-34******************************2672. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-149-7c6e960d6600>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Run the graph until the first interruption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'messages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2329\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2331\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2332\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2333\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    147\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m                 )\n\u001b[1;32m    605\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-148-8d92994b3080>\u001b[0m in \u001b[0;36massistant\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0massistant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMessagesState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mllm_with_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys_msg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5356\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5357\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5358\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5359\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5360\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m         return cast(\n\u001b[1;32m    306\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    308\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    842\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                 results.append(\n\u001b[0;32m--> 683\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    684\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    909\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    912\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    913\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-lf-34******************************2672. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
          ]
        }
      ],
      "source": [
        "# Input\n",
        "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
        "\n",
        "# Thread\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Run the graph until the first interruption\n",
        "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
        "    event['messages'][-1].pretty_print()"
      ]
    },
    {
      "source": [
        "!pip install tenacity"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Ghf3TauXIHoX"
      },
      "id": "Ghf3TauXIHoX",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install tenacity"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7rx_CNapILZw"
      },
      "id": "7rx_CNapILZw",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# ... (rest of your code) ...\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, max=10))\n",
        "def assistant(state: MessagesState):\n",
        "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
        "\n",
        "# ... (rest of your code) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BW6_K-wFITOC"
      },
      "id": "BW6_K-wFITOC",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install tenacity"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zcbjtQJhIOd0"
      },
      "id": "zcbjtQJhIOd0",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")  # Ensure this is called with your valid API key"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "RMWUNfgKG_mE"
      },
      "id": "RMWUNfgKG_mE",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")  # Set your OpenAI API key here"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "H8A2DBujGiNc"
      },
      "id": "H8A2DBujGiNc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "268cfa43-22d1-4d63-8d81-a3ce00f1f2c8",
      "metadata": {
        "id": "268cfa43-22d1-4d63-8d81-a3ce00f1f2c8"
      },
      "source": [
        "## Browsing History\n",
        "\n",
        "We can use `get_state` to look at the **current** state of our graph, given the `thread_id`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "161eb053-18f6-4c99-8674-8cbd11cae57e",
      "metadata": {
        "id": "161eb053-18f6-4c99-8674-8cbd11cae57e"
      },
      "outputs": [],
      "source": [
        "graph.get_state({'configurable': {'thread_id': '1'}})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d00869e-7b41-4d71-ad3c-cacf8f9c029f",
      "metadata": {
        "id": "8d00869e-7b41-4d71-ad3c-cacf8f9c029f"
      },
      "source": [
        "We can also browse the state history of our agent.\n",
        "\n",
        "`get_state_history` lets us get the state at all prior steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3010169c-3bfa-498c-a30c-7ba53744e4d5",
      "metadata": {
        "id": "3010169c-3bfa-498c-a30c-7ba53744e4d5"
      },
      "outputs": [],
      "source": [
        "all_states = [s for s in graph.get_state_history(thread)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4612ccf-59fc-4848-8845-0433fee2ca8e",
      "metadata": {
        "id": "c4612ccf-59fc-4848-8845-0433fee2ca8e"
      },
      "outputs": [],
      "source": [
        "len(all_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af30f269-1152-4fa1-a7c6-2947acad9a27",
      "metadata": {
        "id": "af30f269-1152-4fa1-a7c6-2947acad9a27"
      },
      "source": [
        "The first element is the current state, just as we got from `get_state`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e60b292-8efc-4cc3-b836-51f060fa608b",
      "metadata": {
        "id": "4e60b292-8efc-4cc3-b836-51f060fa608b"
      },
      "outputs": [],
      "source": [
        "all_states[-2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4148a710-ceed-413b-b93c-070c6c792fa2",
      "metadata": {
        "id": "4148a710-ceed-413b-b93c-070c6c792fa2"
      },
      "source": [
        "Everything above we can visualize here:\n",
        "\n",
        "![fig1.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb038211b544898570be3_time-travel1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5ad554a-faf3-489f-a9a9-774f4ec2a526",
      "metadata": {
        "id": "a5ad554a-faf3-489f-a9a9-774f4ec2a526"
      },
      "source": [
        "## Replaying\n",
        "\n",
        "We can re-run our agent from any of the prior steps.\n",
        "\n",
        "![fig2.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb038a0bd34b541c78fb8_time-travel2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e135d2db-d613-42da-877e-d429f21aaefd",
      "metadata": {
        "id": "e135d2db-d613-42da-877e-d429f21aaefd"
      },
      "source": [
        "Let's look back at the step that recieved human input!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3688e511-a440-4330-a450-e5ed889c3b30",
      "metadata": {
        "id": "3688e511-a440-4330-a450-e5ed889c3b30"
      },
      "outputs": [],
      "source": [
        "to_replay = all_states[-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72adf296-d519-4bdc-af03-3b29799e9534",
      "metadata": {
        "id": "72adf296-d519-4bdc-af03-3b29799e9534"
      },
      "outputs": [],
      "source": [
        "to_replay"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "571e7894-6546-48ff-9c25-fa6d120391b3",
      "metadata": {
        "id": "571e7894-6546-48ff-9c25-fa6d120391b3"
      },
      "source": [
        "Look at the state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe69428-f364-4330-bf5d-aa966c7f3b07",
      "metadata": {
        "id": "6fe69428-f364-4330-bf5d-aa966c7f3b07"
      },
      "outputs": [],
      "source": [
        "to_replay.values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff2df545-cc80-4962-a34a-faac7af8eb3d",
      "metadata": {
        "id": "ff2df545-cc80-4962-a34a-faac7af8eb3d"
      },
      "source": [
        "We can see the next node to call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f333f9-9b2b-46f6-ac3a-525f86b20f1b",
      "metadata": {
        "id": "d2f333f9-9b2b-46f6-ac3a-525f86b20f1b"
      },
      "outputs": [],
      "source": [
        "to_replay.next"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8938c18-5c22-47df-b71e-40afa73c87af",
      "metadata": {
        "id": "b8938c18-5c22-47df-b71e-40afa73c87af"
      },
      "source": [
        "We also get the config, which tells us the `checkpoint_id` as well as the `thread_id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1298786-afa5-4277-927e-708a8629231b",
      "metadata": {
        "id": "b1298786-afa5-4277-927e-708a8629231b"
      },
      "outputs": [],
      "source": [
        "to_replay.config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d93b5eb-f541-4f82-93b1-48f54bf5cf83",
      "metadata": {
        "id": "1d93b5eb-f541-4f82-93b1-48f54bf5cf83"
      },
      "source": [
        "To replay from here, we simply pass the config back to the agent!\n",
        "\n",
        "The graph knows that this checkpoint has aleady been executed.\n",
        "\n",
        "It just re-plays from this checkpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531b4cd1-54f6-44aa-9ffe-cf5403dad65d",
      "metadata": {
        "id": "531b4cd1-54f6-44aa-9ffe-cf5403dad65d"
      },
      "outputs": [],
      "source": [
        "for event in graph.stream(None, to_replay.config, stream_mode=\"values\"):\n",
        "    event['messages'][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d7a914e-63e6-4424-970f-15059ce9b4c3",
      "metadata": {
        "id": "7d7a914e-63e6-4424-970f-15059ce9b4c3"
      },
      "source": [
        "Now, we can see our current state after the agent re-ran."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a5a1f03-19f2-4d22-ba54-1c065ff08e85",
      "metadata": {
        "id": "5a5a1f03-19f2-4d22-ba54-1c065ff08e85"
      },
      "source": [
        "## Forking\n",
        "\n",
        "What if we want to run from that same step, but with a different input.\n",
        "\n",
        "This is forking.\n",
        "\n",
        "![fig3.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb038f89f2d847ee5c336_time-travel3.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdeb5bf2-1566-4d8c-8ea5-65894e3a7038",
      "metadata": {
        "id": "cdeb5bf2-1566-4d8c-8ea5-65894e3a7038"
      },
      "outputs": [],
      "source": [
        "to_fork = all_states[-2]\n",
        "to_fork.values[\"messages\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a15f6a6-6eaa-48d6-92bb-864ea3a31b6a",
      "metadata": {
        "id": "4a15f6a6-6eaa-48d6-92bb-864ea3a31b6a"
      },
      "source": [
        "Again, we have the config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1621b27-ee51-4dc3-81c4-1d05317280db",
      "metadata": {
        "id": "d1621b27-ee51-4dc3-81c4-1d05317280db"
      },
      "outputs": [],
      "source": [
        "to_fork.config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2102195-0583-4dbe-ad2f-02fac7915585",
      "metadata": {
        "id": "c2102195-0583-4dbe-ad2f-02fac7915585"
      },
      "source": [
        "Let's modify the state at this checkpoint.\n",
        "\n",
        "We can just run `update_state` with the `checkpoint_id` supplied.\n",
        "\n",
        "Remember how our reducer on `messages` works:\n",
        "\n",
        "* It will append, unless we supply a message ID.\n",
        "* We supply the message ID to overwrite the message, rather than appending to state!\n",
        "\n",
        "So, to overwrite the the message, we just supply the message ID, which we have `to_fork.values[\"messages\"].id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4a918d-858a-41ac-a5d4-e99260e2d6ec",
      "metadata": {
        "id": "0b4a918d-858a-41ac-a5d4-e99260e2d6ec"
      },
      "outputs": [],
      "source": [
        "fork_config = graph.update_state(\n",
        "    to_fork.config,\n",
        "    {\"messages\": [HumanMessage(content='Multiply 5 and 3',\n",
        "                               id=to_fork.values[\"messages\"][0].id)]},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ff4e9bb-8221-42d1-b7d0-b0cbd5dc374a",
      "metadata": {
        "id": "8ff4e9bb-8221-42d1-b7d0-b0cbd5dc374a"
      },
      "outputs": [],
      "source": [
        "fork_config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebfe6fd-c94b-4291-a125-ec6170e35bc5",
      "metadata": {
        "id": "bebfe6fd-c94b-4291-a125-ec6170e35bc5"
      },
      "source": [
        "This creates a new, forked checkpoint.\n",
        "\n",
        "But, the metadata - e.g., where to go next - is perserved!\n",
        "\n",
        "We can see the current state of our agent has been updated with our fork."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "586ce86c-1257-45e9-ba30-6287932b9484",
      "metadata": {
        "id": "586ce86c-1257-45e9-ba30-6287932b9484"
      },
      "outputs": [],
      "source": [
        "all_states = [state for state in graph.get_state_history(thread) ]\n",
        "all_states[0].values[\"messages\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12e19798-25d8-49e8-8542-13d2b3bdf58e",
      "metadata": {
        "id": "12e19798-25d8-49e8-8542-13d2b3bdf58e"
      },
      "outputs": [],
      "source": [
        "graph.get_state({'configurable': {'thread_id': '1'}})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78c641e2-b8e9-4461-b854-8725006a5eb6",
      "metadata": {
        "id": "78c641e2-b8e9-4461-b854-8725006a5eb6"
      },
      "source": [
        "Now, when we stream, the graph knows this checkpoint has never been executed.\n",
        "\n",
        "So, the graph runs, rather than simply re-playing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c49f2a8-b325-45e4-b36c-17fab1b37cc0",
      "metadata": {
        "id": "1c49f2a8-b325-45e4-b36c-17fab1b37cc0"
      },
      "outputs": [],
      "source": [
        "for event in graph.stream(None, fork_config, stream_mode=\"values\"):\n",
        "    event['messages'][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "428d7f80-ee60-4147-b51f-ee3b0cf5cbba",
      "metadata": {
        "id": "428d7f80-ee60-4147-b51f-ee3b0cf5cbba"
      },
      "source": [
        "Now, we can see the current state is the end of our agent run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "132ef840-64c7-479c-ad34-3f177f4b2524",
      "metadata": {
        "id": "132ef840-64c7-479c-ad34-3f177f4b2524"
      },
      "outputs": [],
      "source": [
        "graph.get_state({'configurable': {'thread_id': '1'}})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ceb5f31-97b0-466c-9b3b-ae4df7cd462a",
      "metadata": {
        "id": "2ceb5f31-97b0-466c-9b3b-ae4df7cd462a"
      },
      "source": [
        "### Time travel with LangGraph API\n",
        "\n",
        "**⚠️ DISCLAIMER**\n",
        "\n",
        "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
        "\n",
        "```\n",
        "langgraph dev\n",
        "```\n",
        "\n",
        "You should see the following output:\n",
        "```\n",
        "- 🚀 API: http://127.0.0.1:2024\n",
        "- 🎨 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
        "- 📚 API Docs: http://127.0.0.1:2024/docs\n",
        "```\n",
        "\n",
        "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.\n",
        "\n",
        "We connect to it via the SDK and show how the LangGraph API [supports time travel](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/#initial-invocation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "891defdb-746c-48e4-8efa-bb5f138dc4bd",
      "metadata": {
        "id": "891defdb-746c-48e4-8efa-bb5f138dc4bd"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a317925d-1788-4cfc-9c12-336b17b4d859",
      "metadata": {
        "id": "a317925d-1788-4cfc-9c12-336b17b4d859"
      },
      "outputs": [],
      "source": [
        "from langgraph_sdk import get_client\n",
        "client = get_client(url=\"http://127.0.0.1:2024\")"
      ]
    },
    {
      "source": [
        "import openai\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"] # assumes you have set the environment variable\n",
        "\n",
        "try:\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",  # or any other suitable model\n",
        "        prompt=\"Hello, world!\",\n",
        "        max_tokens=5\n",
        "    )\n",
        "    print(response)\n",
        "except openai.error.AuthenticationError as e:\n",
        "    print(f\"Authentication failed: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "kQPlAYcoIb-Y"
      },
      "id": "kQPlAYcoIb-Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "815d5e03-0ab8-4c7f-a1ee-f410b6aadc03",
      "metadata": {
        "id": "815d5e03-0ab8-4c7f-a1ee-f410b6aadc03"
      },
      "source": [
        "#### Re-playing\n",
        "\n",
        "Let's run our agent streaming `updates` to the state of the graph after each node is called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d4d01da-7b64-4c92-96b7-29ec93332d0b",
      "metadata": {
        "id": "9d4d01da-7b64-4c92-96b7-29ec93332d0b"
      },
      "outputs": [],
      "source": [
        "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
        "thread = await client.threads.create()\n",
        "async for chunk in client.runs.stream(\n",
        "    thread[\"thread_id\"],\n",
        "    assistant_id = \"agent\",\n",
        "    input=initial_input,\n",
        "    stream_mode=\"updates\",\n",
        "):\n",
        "    if chunk.data:\n",
        "        assisant_node = chunk.data.get('assistant', {}).get('messages', [])\n",
        "        tool_node = chunk.data.get('tools', {}).get('messages', [])\n",
        "        if assisant_node:\n",
        "            print(\"-\" * 20+\"Assistant Node\"+\"-\" * 20)\n",
        "            print(assisant_node[-1])\n",
        "        elif tool_node:\n",
        "            print(\"-\" * 20+\"Tools Node\"+\"-\" * 20)\n",
        "            print(tool_node[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cc3bab2",
      "metadata": {
        "id": "8cc3bab2"
      },
      "source": [
        "Now, let's look at **replaying** from a specified checkpoint.\n",
        "\n",
        "We simply need to pass the `checkpoint_id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ecc4fd",
      "metadata": {
        "id": "d8ecc4fd"
      },
      "outputs": [],
      "source": [
        "states = await client.threads.get_history(thread['thread_id'])\n",
        "to_replay = states[-2]\n",
        "to_replay"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e33f865a",
      "metadata": {
        "id": "e33f865a"
      },
      "source": [
        "Let's stream with `stream_mode=\"values\"` to see the full state at every node as we replay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "325e8272",
      "metadata": {
        "id": "325e8272"
      },
      "outputs": [],
      "source": [
        "async for chunk in client.runs.stream(\n",
        "    thread[\"thread_id\"],\n",
        "    assistant_id=\"agent\",\n",
        "    input=None,\n",
        "    stream_mode=\"values\",\n",
        "    checkpoint_id=to_replay['checkpoint_id']\n",
        "):\n",
        "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
        "    print(chunk.data)\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "source": [
        "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
        "# Re-initialize the thread variable to ensure it contains the 'thread_id' key\n",
        "thread = await client.threads.create()\n",
        "async for chunk in client.runs.stream(\n",
        "    thread[\"thread_id\"],\n",
        "    assistant_id=\"agent\",\n",
        "    input=initial_input,\n",
        "    stream_mode=\"updates\",\n",
        "):\n",
        "    if chunk.data:\n",
        "        assisant_node = chunk.data.get('assistant', {}).get('messages', [])\n",
        "        tool_node = chunk.data.get('tools', {}).get('messages', [])\n",
        "        if assisant_node:\n",
        "            print(\"-\" * 20 + \"Assistant Node\" + \"-\" * 20)\n",
        "            print(assisant_node[-1])\n",
        "        elif tool_node:\n",
        "            print(\"-\" * 20 + \"Tools Node\" + \"-\" * 20)\n",
        "            print(tool_node[-1])\n",
        "\n",
        "# ... (rest of the code)\n",
        "\n",
        "states = await client.threads.get_history(thread['thread_id'])\n",
        "to_replay = states[-2]\n",
        "\n",
        "async for chunk in client.runs.stream(\n",
        "    thread[\"thread_id\"],  # Accessing the 'thread_id' from the re-initialized thread\n",
        "    assistant_id=\"agent\",\n",
        "    input=None,\n",
        "    stream_mode=\"values\",\n",
        "    checkpoint_id=to_replay['checkpoint_id']\n",
        "):\n",
        "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
        "    print(chunk.data)\n",
        "    print(\"\\n\\n\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JCE9rMijIvbN"
      },
      "id": "JCE9rMijIvbN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "14c153b3",
      "metadata": {
        "id": "14c153b3"
      },
      "source": [
        "We can all view this as streaming only `updates` to state made by the nodes that we reply."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e608e93",
      "metadata": {
        "id": "9e608e93"
      },
      "outputs": [],
      "source": [
        "async for chunk in client.runs.stream(\n",
        "    thread[\"thread_id\"],\n",
        "    assistant_id=\"agent\",\n",
        "    input=None,\n",
        "    stream_mode=\"updates\",\n",
        "    checkpoint_id=to_replay['checkpoint_id']\n",
        "):\n",
        "    if chunk.data:\n",
        "        assisant_node = chunk.data.get('assistant', {}).get('messages', [])\n",
        "        tool_node = chunk.data.get('tools', {}).get('messages', [])\n",
        "        if assisant_node:\n",
        "            print(\"-\" * 20+\"Assistant Node\"+\"-\" * 20)\n",
        "            print(assisant_node[-1])\n",
        "        elif tool_node:\n",
        "            print(\"-\" * 20+\"Tools Node\"+\"-\" * 20)\n",
        "            print(tool_node[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e66e0e8",
      "metadata": {
        "id": "8e66e0e8"
      },
      "source": [
        "#### Forking\n",
        "\n",
        "Now, let's look at forking.\n",
        "\n",
        "Let's get the same step as we worked with above, the human input.\n",
        "\n",
        "Let's create a new thread with our agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01af5ed4",
      "metadata": {
        "id": "01af5ed4"
      },
      "outputs": [],
      "source": [
        "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n",
        "thread = await client.threads.create()\n",
        "async for chunk in client.runs.stream(\n",
        "    thread[\"thread_id\"],\n",
        "    assistant_id=\"agent\",\n",
        "    input=initial_input,\n",
        "    stream_mode=\"updates\",\n",
        "):\n",
        "    if chunk.data:\n",
        "        assisant_node = chunk.data.get('assistant', {}).get('messages', [])\n",
        "        tool_node = chunk.data.get('tools', {}).get('messages', [])\n",
        "        if assisant_node:\n",
        "            print(\"-\" * 20+\"Assistant Node\"+\"-\" * 20)\n",
        "            print(assisant_node[-1])\n",
        "        elif tool_node:\n",
        "            print(\"-\" * 20+\"Tools Node\"+\"-\" * 20)\n",
        "            print(tool_node[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dbc8795-c3f5-4559-a00e-dc410c0a927f",
      "metadata": {
        "id": "3dbc8795-c3f5-4559-a00e-dc410c0a927f"
      },
      "outputs": [],
      "source": [
        "states = await client.threads.get_history(thread['thread_id'])\n",
        "to_fork = states[-2]\n",
        "to_fork['values']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11e6cde1-0388-43ea-b994-1c4e9ca1199b",
      "metadata": {
        "id": "11e6cde1-0388-43ea-b994-1c4e9ca1199b"
      },
      "outputs": [],
      "source": [
        "to_fork['values']['messages'][0]['id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c1e2300-c8b2-4994-a96d-1be19c04b6a8",
      "metadata": {
        "id": "0c1e2300-c8b2-4994-a96d-1be19c04b6a8"
      },
      "outputs": [],
      "source": [
        "to_fork['next']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d31d5aa-524f-42f4-ba7e-713a029610d6",
      "metadata": {
        "id": "9d31d5aa-524f-42f4-ba7e-713a029610d6"
      },
      "outputs": [],
      "source": [
        "to_fork['checkpoint_id']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f11e1d9-9fe7-4243-a06f-9b07e38a12ad",
      "metadata": {
        "id": "8f11e1d9-9fe7-4243-a06f-9b07e38a12ad"
      },
      "source": [
        "Let's edit the state.\n",
        "\n",
        "Remember how our reducer on `messages` works:\n",
        "\n",
        "* It will append, unless we supply a message ID.\n",
        "* We supply the message ID to overwrite the message, rather than appending to state!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0198f1b8-2f57-4c6e-ac6a-c6fb80cce0bd",
      "metadata": {
        "id": "0198f1b8-2f57-4c6e-ac6a-c6fb80cce0bd"
      },
      "outputs": [],
      "source": [
        "forked_input = {\"messages\": HumanMessage(content=\"Multiply 3 and 3\",\n",
        "                                         id=to_fork['values']['messages'][0]['id'])}\n",
        "\n",
        "forked_config = await client.threads.update_state(\n",
        "    thread[\"thread_id\"],\n",
        "    forked_input,\n",
        "    checkpoint_id=to_fork['checkpoint_id']\n",
        ")"
      ]
    },
    {
      "source": [
        "# 1. Get the thread history\n",
        "states = await client.threads.get_history(thread['thread_id'])\n",
        "\n",
        "# 2. Select the specific state you want to fork from\n",
        "to_fork = states[-2]  # Assuming you want the second to last state\n",
        "\n",
        "# 3. Define the forked input based on to_fork\n",
        "forked_input = {\"messages\": HumanMessage(content=\"Multiply 3 and 3\",\n",
        "                                         id=to_fork['values']['messages'][0]['id'])}\n",
        "\n",
        "# 4. Update the thread state with the forked input\n",
        "forked_config = await client.threads.update_state(\n",
        "    thread[\"thread_id\"],\n",
        "    forked_input,\n",
        "    checkpoint_id=to_fork['checkpoint_id']\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "XnXDiRjdJUNk"
      },
      "id": "XnXDiRjdJUNk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dcd5b8e-6bb1-4967-84cf-4af710b8bf46",
      "metadata": {
        "id": "1dcd5b8e-6bb1-4967-84cf-4af710b8bf46"
      },
      "outputs": [],
      "source": [
        "forked_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "015ac68a-5cc1-4c42-90a2-5b2b4865a153",
      "metadata": {
        "id": "015ac68a-5cc1-4c42-90a2-5b2b4865a153"
      },
      "outputs": [],
      "source": [
        "states = await client.threads.get_history(thread['thread_id'])\n",
        "states[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3de80029-b987-49c5-890d-6cd70cbc8de7",
      "metadata": {
        "id": "3de80029-b987-49c5-890d-6cd70cbc8de7"
      },
      "source": [
        "To rerun, we pass in the `checkpoint_id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da005240-d3f0-4c89-9aca-dfcb5d410ceb",
      "metadata": {
        "id": "da005240-d3f0-4c89-9aca-dfcb5d410ceb"
      },
      "outputs": [],
      "source": [
        "async for chunk in client.runs.stream(\n",
        "    thread[\"thread_id\"],\n",
        "    assistant_id=\"agent\",\n",
        "    input=None,\n",
        "    stream_mode=\"updates\",\n",
        "    checkpoint_id=forked_config['checkpoint_id']\n",
        "):\n",
        "    if chunk.data:\n",
        "        assisant_node = chunk.data.get('assistant', {}).get('messages', [])\n",
        "        tool_node = chunk.data.get('tools', {}).get('messages', [])\n",
        "        if assisant_node:\n",
        "            print(\"-\" * 20+\"Assistant Node\"+\"-\" * 20)\n",
        "            print(assisant_node[-1])\n",
        "        elif tool_node:\n",
        "            print(\"-\" * 20+\"Tools Node\"+\"-\" * 20)\n",
        "            print(tool_node[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36956571-a2b8-4f1b-8e30-51f02f155a6f",
      "metadata": {
        "id": "36956571-a2b8-4f1b-8e30-51f02f155a6f"
      },
      "source": [
        "### LangGraph Studio\n",
        "\n",
        "Let's look at forking in the Studio UI w\n",
        "\n",
        "ith our `agent`, which uses `module-1/studio/agent.py` set in `module-1/studio/langgraph.json`.vai\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}