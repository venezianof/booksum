{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venezianof/booksum/blob/main/examples/Agents_Function_Calling_Barista_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Load the tools you want to use (e.g., Google Search).\n",
        "tools = load_tools([\"google-search\"])\n",
        "\n",
        "# Initialize the agent with OpenAI as the language model."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "x8EYpx4L5cm1",
        "outputId": "d9fc2f31-8e63-4b3b-b182-6e214d52e90f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "Module langchain_community.agent_toolkits.load_tools not found. Please install langchain-community to access this module. You can install it using `pip install -U langchain-community`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/_api/module_import.py\u001b[0m in \u001b[0;36mimport_by_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5d3e404c148c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_tools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minitialize_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgentType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;34mf\"Please update your import statement from: `{old_path}` to `{new_path}`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         )\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_import_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/_api/module_import.py\u001b[0m in \u001b[0;36mimport_by_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"langchain_community\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                     raise ModuleNotFoundError(\n\u001b[0m\u001b[1;32m     73\u001b[0m                         \u001b[0;34mf\"Module {new_module} not found. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                         \u001b[0;34m\"Please install langchain-community to access this module. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: Module langchain_community.agent_toolkits.load_tools not found. Please install langchain-community to access this module. You can install it using `pip install -U langchain-community`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Load the tools you want to use (e.g., Google Search).\n",
        "tools = load_tools([\"google-search\"])\n",
        "\n",
        "# Initialize the agent with OpenAI as the language model.\n",
        "agent = initialize_agent(tools, OpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Now you can start interacting with the agent.\n",
        "agent.run(\"What's the weather in San Francisco today?\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7X5AfrvQ6MZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok-j-Bih_C5S"
      },
      "source": [
        "# Gemini API: Agents and Automatic Function Calling with Barista Bot\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Agents_Function_Calling_Barista_Bot.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YXoI96g_PLL"
      },
      "source": [
        "This notebook shows a practical example of using automatic function calling with the Gemini API's Python SDK to build an agent. You will define some functions that comprise a café's ordering system, connect them to the Gemini API and write an agent loop that interacts with the user to order café drinks.\n",
        "\n",
        "The guide was inspired by the ReAct-style [Barista bot](https://aistudio.google.com/app/prompts/barista-bot) prompt available through AI Studio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IzLYKxmTHd5"
      },
      "outputs": [],
      "source": [
        "%pip install -qU \"google-genai>=1.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFjRBXVrAdYB"
      },
      "source": [
        "To run this notebook, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you are running in a different environment, you can store your key in an environment variable. See [Authentication](../quickstarts/Authentication.ipynb) to learn more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0gOuwcCUTNAO"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.colab import userdata\n",
        "\n",
        "client = genai.Client(api_key=userdata.get(\"GOOGLE_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F72jp1hzA8zU"
      },
      "source": [
        "## Define the API\n",
        "\n",
        "To emulate a café's ordering system, define functions for managing the customer's order: adding, editing, clearing, confirming and fulfilling.\n",
        "\n",
        "These functions track the customer's order using the global variables `order` (the in-progress order) and `placed_order` (the confirmed order sent to the kitchen). Each of the order-editing functions updates the `order`, and once placed, `order` is copied to `placed_order` and cleared.\n",
        "\n",
        "In the Python SDK you can pass functions directly to the model constructor, where the SDK will inspect the type signatures and docstrings to define the `tools`. For this reason it's important that you correctly type each of the parameters, give the functions sensible names and detailed docstrings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wMltPyUpTu3h"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "from random import randint\n",
        "\n",
        "order = []  # The in-progress order.\n",
        "placed_order = []  # The confirmed, completed order.\n",
        "\n",
        "\n",
        "def add_to_order(drink: str, modifiers: Optional[list[str]] = None) -> None:\n",
        "    \"\"\"Adds the specified drink to the customer's order, including any modifiers.\"\"\"\n",
        "    if modifiers is None:  # Ensures safe handling of None\n",
        "        modifiers = []\n",
        "    order.append((drink, modifiers))\n",
        "\n",
        "\n",
        "def get_order() -> list[tuple[str, list[str]]]:\n",
        "    \"\"\"Returns the customer's order.\"\"\"\n",
        "    return order\n",
        "\n",
        "\n",
        "def remove_item(n: int) -> str:\n",
        "    \"\"\"Removes the nth (one-based) item from the order.\n",
        "\n",
        "    Returns:\n",
        "        The item that was removed.\n",
        "    \"\"\"\n",
        "    item, _ = order.pop(n - 1)\n",
        "    return item\n",
        "\n",
        "\n",
        "def clear_order() -> None:\n",
        "    \"\"\"Removes all items from the customer's order.\"\"\"\n",
        "    order.clear()\n",
        "\n",
        "\n",
        "def confirm_order() -> str:\n",
        "    \"\"\"Asks the customer if the order is correct.\n",
        "\n",
        "    Returns:\n",
        "        The user's free-text response.\n",
        "    \"\"\"\n",
        "    print(\"Your order:\")\n",
        "    if not order:\n",
        "        print(\"  (no items)\")\n",
        "\n",
        "    for drink, modifiers in order:\n",
        "        print(f\"  {drink}\")\n",
        "        if modifiers:\n",
        "            print(f'   - {\", \".join(modifiers)}')\n",
        "\n",
        "    return input(\"Is this correct? \")\n",
        "\n",
        "\n",
        "def place_order() -> int:\n",
        "    \"\"\"Submit the order to the kitchen.\n",
        "\n",
        "    Returns:\n",
        "        The estimated number of minutes until the order is ready.\n",
        "    \"\"\"\n",
        "    placed_order[:] = order.copy()\n",
        "    clear_order()\n",
        "\n",
        "    # TODO: Implement coffee fulfillment.\n",
        "    return randint(1, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7l3_vBtC0oF"
      },
      "source": [
        "## Test the API\n",
        "\n",
        "With the functions written, test that they work as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jg1LjYNUWnsC",
        "outputId": "a99778fa-e92d-4a94-abac-3ff377f4eb60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your order:\n",
            "  Latte\n",
            "   - Extra shot\n",
            "  Tea\n",
            "   - Earl Grey, hot\n",
            "Is this correct? AIzaSyA8SWIs6gk2nLWGfy7i5TO-5ZBQu5_NBhg\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyA8SWIs6gk2nLWGfy7i5TO-5ZBQu5_NBhg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Test it out!\n",
        "\n",
        "clear_order()\n",
        "add_to_order(\"Latte\", [\"Extra shot\"])\n",
        "add_to_order(\"Tea\")\n",
        "remove_item(2)\n",
        "add_to_order(\"Tea\", [\"Earl Grey\", \"hot\"])\n",
        "confirm_order()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOhiADoCC811"
      },
      "source": [
        "## Define the prompt\n",
        "\n",
        "Here you define the full Barista-bot prompt. This prompt contains the café's menu items and modifiers and some instructions.\n",
        "\n",
        "The instructions include guidance on how functions should be called (e.g. \"Always `confirm_order` with the user before calling `place_order`\"). You can modify this to add your own interaction style to the bot, for example if you wanted to have the bot repeat every request back before adding to the order, you could provide that instruction here.\n",
        "\n",
        "The end of the prompt includes some jargon the bot might encounter, and instructions _du jour_ - in this case it notes that the café has run out of soy milk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IoBvZ1JYXgn5"
      },
      "outputs": [],
      "source": [
        "COFFEE_BOT_PROMPT = \"\"\"\\You are a coffee order taking system and you are restricted to talk only about drinks on the MENU. Do not talk about anything but ordering MENU drinks for the customer, ever.\n",
        "Your goal is to do place_order after understanding the menu items and any modifiers the customer wants.\n",
        "Add items to the customer's order with add_to_order, remove specific items with remove_item, and reset the order with clear_order.\n",
        "To see the contents of the order so far, call get_order (by default this is shown to you, not the user)\n",
        "Always confirm_order with the user (double-check) before calling place_order. Calling confirm_order will display the order items to the user and returns their response to seeing the list. Their response may contain modifications.\n",
        "Always verify and respond with drink and modifier names from the MENU before adding them to the order.\n",
        "If you are unsure a drink or modifier matches those on the MENU, ask a question to clarify or redirect.\n",
        "You only have the modifiers listed on the menu below: Milk options, espresso shots, caffeine, sweeteners, special requests.\n",
        "Once the customer has finished ordering items, confirm_order and then place_order.\n",
        "\n",
        "Hours: Tues, Wed, Thurs, 10am to 2pm\n",
        "Prices: All drinks are free.\n",
        "\n",
        "MENU:\n",
        "Coffee Drinks:\n",
        "Espresso\n",
        "Americano\n",
        "Cold Brew\n",
        "\n",
        "Coffee Drinks with Milk:\n",
        "Latte\n",
        "Cappuccino\n",
        "Cortado\n",
        "Macchiato\n",
        "Mocha\n",
        "Flat White\n",
        "\n",
        "Tea Drinks:\n",
        "English Breakfast Tea\n",
        "Green Tea\n",
        "Earl Grey\n",
        "\n",
        "Tea Drinks with Milk:\n",
        "Chai Latte\n",
        "Matcha Latte\n",
        "London Fog\n",
        "\n",
        "Other Drinks:\n",
        "Steamer\n",
        "Hot Chocolate\n",
        "\n",
        "Modifiers:\n",
        "Milk options: Whole, 2%, Oat, Almond, 2% Lactose Free; Default option: whole\n",
        "Espresso shots: Single, Double, Triple, Quadruple; default: Double\n",
        "Caffeine: Decaf, Regular; default: Regular\n",
        "Hot-Iced: Hot, Iced; Default: Hot\n",
        "Sweeteners (option to add one or more): vanilla sweetener, hazelnut sweetener, caramel sauce, chocolate sauce, sugar free vanilla sweetener\n",
        "Special requests: any reasonable modification that does not involve items not on the menu, for example: 'extra hot', 'one pump', 'half caff', 'extra foam', etc.\n",
        "\n",
        "\"dirty\" means add a shot of espresso to a drink that doesn't usually have it, like \"Dirty Chai Latte\".\n",
        "\"Regular milk\" is the same as 'whole milk'.\n",
        "\"Sweetened\" means add some regular sugar, not a sweetener.\n",
        "\n",
        "Soy milk has run out of stock today, so soy is not available.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_ybYQ-sU7rn"
      },
      "source": [
        "## Set up the model\n",
        "\n",
        "In this step you collate the functions into a \"system\" that is passed as `tools`, instantiate the model and start the chat session.\n",
        "\n",
        "A retriable `send_message` function is also defined to help with low-quota conversations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8vmtzAlPaQH-"
      },
      "outputs": [],
      "source": [
        "from google.genai import types\n",
        "from google.api_core import retry\n",
        "\n",
        "ordering_system = [\n",
        "    add_to_order,\n",
        "    get_order,\n",
        "    remove_item,\n",
        "    clear_order,\n",
        "    confirm_order,\n",
        "    place_order,\n",
        "]\n",
        "model_name = \"gemini-2.0-flash\"  # @param [\"gemini-2.0-flash-lite\",\"gemini-2.0-flash\",\"gemini-2.5-pro-exp-03-25\"] {\"allow-input\":true}\n",
        "\n",
        "chat = client.chats.create(\n",
        "    model=model_name,\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=ordering_system,\n",
        "        system_instruction=COFFEE_BOT_PROMPT,\n",
        "    ),\n",
        ")\n",
        "\n",
        "placed_order = []\n",
        "order = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "624ai2Q5WMAF"
      },
      "source": [
        "## Chat with Barista Bot\n",
        "\n",
        "With the model defined and chat created, all that's left is to connect the user input to the model and display the output, in a loop. This loop continues until an order is placed.\n",
        "\n",
        "When run in Colab, any fixed-width text originates from your Python code (e.g. `print` calls in the ordering system), regular text comes the Gemini API, and the outlined boxes allow for user input that is rendered with a leading `>`.\n",
        "\n",
        "Try it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38SAyrNNVhvE",
        "outputId": "59b371f7-8ebd-48e5-f751-0ad140a29258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to Barista bot!\n",
            "\n",
            "\n",
            "> AIzaSyA8SWIs6gk2nLWGfy7i5TO-5ZBQu5_NBhg\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "OK. I am ready to take your order. What would you like to order from the menu today?\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "print(\"Welcome to Barista bot!\\n\\n\")\n",
        "\n",
        "while not placed_order:\n",
        "    response = chat.send_message(input(\"> \"))\n",
        "    display(Markdown(response.text))\n",
        "\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"[barista bot session over]\")\n",
        "print()\n",
        "print(\"Your order:\")\n",
        "print(f\"  {placed_order}\\n\")\n",
        "print(\"- Thanks for using Barista Bot!\")"
      ]
    },
    {
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Load the tool.\n",
        "tools = load_tools([\"python_repl\"])\n",
        "\n",
        "# Initialize the agent.\n",
        "agent = initialize_agent(tools, OpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Run the agent.\n",
        "agent.run(\"What is the sum of 3 and 5?\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2Fc23Iwn6vkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Load the tool.\n",
        "tools = load_tools([\"python_repl\"])\n",
        "\n",
        "# Initialize the agent.\n",
        "agent = initialize_agent(tools, OpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Run the agent.\n",
        "agent.run(\"What is the sum of 3 and 5?\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "KXRMdNt_7I4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from typing import Type\n",
        "from langchain.tools import BaseTool\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "class CustomTool(BaseTool):\n",
        "    name = \"CustomTool\"\n",
        "    description = \"A custom tool that does something specific.\"\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        # Implement your tool's logic here.\n",
        "        # Process the query and return the result.\n",
        "        result = f\"Custom Tool processed: {query}\"\n",
        "        return result\n",
        "\n",
        "    def _arun(self, query: str) -> str:\n",
        "        raise NotImplementedError(\"This tool does not support async\")\n",
        "\n",
        "# Create an instance of your custom tool.\n",
        "custom_tool = CustomTool()\n",
        "\n",
        "# Create a LangChain Tool object from your custom tool.\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=custom_tool.name,\n",
        "        func=custom_tool.run,\n",
        "        description=custom_tool.description,\n",
        "    )\n",
        "]\n",
        "\n",
        "# Initialize the agent with your custom tool.\n",
        "agent = initialize_agent(tools, OpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Now you can run the agent.\n",
        "agent.run(\"Use CustomTool to process this: Hello world!\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "t8PlLj5j7NNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain.agents import load_tools, initialize_agent, AgentType\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.tools import BaseTool\n",
        "from typing import Optional, Type\n",
        "\n",
        "class SearchTool(BaseTool):\n",
        "    name = \"Intermediate Answer\"\n",
        "    description = \"Use this when you need to search to find an intermediate answer.\"\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        \"\"\"Run the tool.\"\"\"\n",
        "        # This is where you would typically perform an action,\n",
        "        # such as searching with Google or another API.\n",
        "        # Since I cannot access external websites,\n",
        "        # I will simply return a placeholder response here.\n",
        "\n",
        "        return f\"Placeholder search results for: {query}\"\n",
        "\n",
        "    async def _arun(self, query: str) -> str:\n",
        "        \"\"\"Run the tool asynchronously.\"\"\"\n",
        "        raise NotImplementedError(\"IntermediateAnswer does not support async\")\n",
        "\n",
        "# 1. Load the tool:\n",
        "tools = [\n",
        "    SearchTool(),  # Your custom tool\n",
        "    # Other tools...\n",
        "]\n",
        "\n",
        "# 2. Initialize the agent:\n",
        "llm = OpenAI(temperature=0)\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# 3. Run the agent:\n",
        "agent.run(\"What is the population of France?\")  # Example query"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QSecnxNa7jl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from typing import Optional, List, Dict\n",
        "from langchain.agents import load_tools, initialize_agent, AgentType\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.tools import BaseTool\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Define custom function\n",
        "def custom_function(param1: str, param2: Optional[int] = None) -> str:\n",
        "    \"\"\"\n",
        "    My custom function does something.\n",
        "\n",
        "    Args:\n",
        "        param1: String parameter\n",
        "        param2: Optional integer parameter\n",
        "\n",
        "    Returns:\n",
        "        A string representing the result.\n",
        "    \"\"\"\n",
        "    # Your custom logic here\n",
        "    result = f\"Processed with param1: '{param1}' and param2: {param2}\"\n",
        "    return result\n",
        "\n",
        "class CustomToolInput(BaseModel):\n",
        "    param1: str = Field(..., description=\"String parameter\")\n",
        "    param2: Optional[int] = Field(None, description=\"Optional integer parameter\")\n",
        "\n",
        "class CustomTool(BaseTool):\n",
        "    name = \"custom_tool\"\n",
        "    description = \"A tool that executes custom_function\"\n",
        "    args_schema: Type[BaseModel] = CustomToolInput\n",
        "\n",
        "    def _run(self, param1: str, param2: Optional[int] = None) -> str:\n",
        "        return custom_function(param1=param1, param2=param2)\n",
        "\n",
        "    async def _arun(self, param1: str, param2: Optional[int] = None) -> str:\n",
        "        raise NotImplementedError(\"This tool does not support async\")\n",
        "\n",
        "\n",
        "# Initialize the agent with your custom tool\n",
        "tools = [CustomTool()]\n",
        "llm = OpenAI(temperature=0)\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose="
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bAB2xZc78VWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assume this code is from a previous cell defining the custom tool:\n",
        "from langchain.tools import BaseTool\n",
        "\n",
        "class MyCustomTool(BaseTool):\n",
        "    name = \"MyTool\"\n",
        "    description = \"This tool does something special.\"\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        # Your custom logic here, processing 'query'\n",
        "        return f\"MyTool processed: {query}\"\n",
        "\n",
        "    async def _arun(self, query: str) -> str:\n",
        "        raise NotImplementedError(\"MyTool does not support async\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5cFya9O38pjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain.agents import load_tools, initialize_agent, AgentType\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# 1. Load the tool:\n",
        "tools = [\n",
        "    MyCustomTool(),  # Your custom tool\n",
        "    # other tools if needed\n",
        "]\n",
        "\n",
        "# 2. Initialize the agent:\n",
        "llm = OpenAI(temperature=0)\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# 3. Use the tool in a query:\n",
        "agent.run(\"Use MyTool to process the following: 'Hello, world!'\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "uRVSqGiv8s0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# ... (define tools and custom tools if needed)\n",
        "\n",
        "# Initialize the first agent\n",
        "tools_agent1 = [...]  # Load the tools for the first agent\n",
        "agent1 = initialize_agent(tools_agent1, OpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Initialize the refinement agent\n",
        "tools_agent2 = [...]  # Load the tools for the refinement agent (could include custom tools)\n",
        "agent2 = initialize_agent(tools_agent2, OpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Run the first agent\n",
        "result1 = agent1.run(query)  # Initial query\n",
        "\n",
        "# Run the refinement agent\n",
        "final_result = agent2.run(f\"Refine this response: {result1}\")  # Refine the output of agent1"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pMGxtls986tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ... (define tools and custom refinement tool)\n",
        "\n",
        "# Initialize the agent (including the refinement tool)\n",
        "tools = [...]  # Load tools, including the custom refinement tool\n",
        "agent = initialize_agent(tools, OpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Run the agent (it might automatically use the refinement tool if needed)\n",
        "final_result = agent.run(query)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ANZrl4Mt8-VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain.agents import load_tools, initialize_agent, AgentType\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.tools import BaseTool\n",
        "from typing import Optional\n",
        "\n",
        "class MathTool(BaseTool):\n",
        "    name = \"Calculator\"\n",
        "    description = \"Useful for when you need to answer questions about math.\"\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        \"\"\"Use the tool.\"\"\"\n",
        "        try:\n",
        "            # This is where you would typically perform an action,\n",
        "            # such as searching with Google"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "905EC47L9JaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Assume 'agent' has been defined and initialized with tools in previous cells\n",
        "\n",
        "user_query = input(\"Enter your query: \")  # Get user input\n",
        "\n",
        "# Execute the agent with the user query\n",
        "agent_response = agent.run(user_query)\n",
        "\n",
        "# Print the agent's response\n",
        "print(agent_response)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "n_dlUiiD9jEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\"\"\"\n",
        "Some things to try:\n",
        "* \"What's the weather in London?\"\n",
        "* \"Who is the current president of France?\"\n",
        "* \"How tall is Mount Everest?\"\n",
        "* \"What are the latest news headlines?\"\n",
        "\"\"\"\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9rhVdye998vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\"\"\"\n",
        "Some things to try:\n",
        "* Ask about the menu (e.g. \"what coffee drinks are available?\")\n",
        "* Use terms that are not specified in the prompt (e.g. \"a strong latte\" or \"an EB tea\")\n",
        "* Change your mind part way through (\"uhh cancel the latte sorry\")\n",
        "* Go off-menu (\"a babycino\")\n",
        "\"\"\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nGQOuzSG-J7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\"\"\"\n",
        "## See also\n",
        "\n",
        "This sample app showed you how to integrate a traditional software system (the coffee ordering functions) and an AI agent powered by the Gemini API. This is a simple, practical way to use LLMs that allows for open-ended human language input and output that feels natural, but still keeps a human in the loop to ensure correct operation.\n",
        "\n",
        "To learn more about how Barista Bot works, check out:\n",
        "\n",
        "* The [Barista Bot](https://aistudio.google.com/app/prompts/barista-bot) prompt\n",
        "* [System instructions](../quickstarts/System_instructions.ipynb)\n",
        "* [Automatic function calling](../quickstarts/Function_calling.ipynb)\n",
        "\"\"\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "MbUE0y1D-LY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\"\"\"\n",
        "Some things to try:\n",
        "* Ask about the menu (e.g. \"what coffee drinks are available?\")\n",
        "* Use terms that are not specified in the prompt (e.g. \"a strong latte\" or \"an EB tea\")\n",
        "* Change your mind part way through (\"uhh cancel the latte sorry\")\n",
        "* Go off-menu (\"a babycino\")\n",
        "\"\"\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ylGbGvGL-qoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\"\"\"\n",
        "## See also\n",
        "\n",
        "This sample app showed you how to integrate a traditional software system (the coffee ordering functions) and an AI agent powered by the Gemini API. This is a simple, practical way to use LLMs that allows for open-ended human language input and output that feels natural, but still keeps a human in the loop to ensure correct operation.\n",
        "\n",
        "To learn more about how Barista Bot works, check out:\n",
        "\n",
        "* The [Barista Bot](https://aistudio.google.com/app/prompts/barista-bot) prompt\n",
        "* [System instructions](../quickstarts/System_instructions.ipynb)\n",
        "* [Automatic function calling](../quickstarts/Function_calling.ipynb)\n",
        "\"\"\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Hi1WZ9Ww-sev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "Some things to try:\n",
        "* \"What's the weather like in London today?\"\n",
        "* \"Write a short poem about the beauty of nature.\"\n",
        "* \"What are the top 5 tourist attractions in Paris?\"\n",
        "* \"Summarize the latest news on artificial intelligence.\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1gTKYPZC-44I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Some things to try:\n",
        "* Ask about the menu (e.g. \"what coffee drinks are available?\")\n",
        "* Use terms that are not specified in the prompt (e.g. \"a strong latte\" or \"an EB tea\")\n",
        "* Change your mind part way through (\"uhh cancel the latte sorry\")\n",
        "* Go off-menu (\"a babycino\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BbWgahum_Tqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Some things to try:\n",
        "* Ask about the menu (e.g. \"what coffee drinks are available?\")\n",
        "* Use terms that are not specified in the prompt (e.g. \"a strong latte\" or \"an EB tea\")\n",
        "* Change your mind part way through (\"uhh cancel the latte sorry\")\n",
        "* Go off-menu (\"a babycino\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UoXWJvPh_XKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr0xv8BIdXCQ"
      },
      "source": [
        "Some things to try:\n",
        "* Ask about the menu (e.g. \"what coffee drinks are available?\")\n",
        "* Use terms that are not specified in the prompt (e.g. \"a strong latte\" or \"an EB tea\")\n",
        "* Change your mind part way through (\"uhh cancel the latte sorry\")\n",
        "* Go off-menu (\"a babycino\")"
      ]
    },
    {
      "source": [
        "# Some things to try:\n",
        "* Ask about the menu (e.g. \"what coffee drinks are available?\")\n",
        "* Use terms that are not specified in the prompt (e.g. \"a strong latte\" or \"an EB tea\")\n",
        "* Change your mind part way through (\"uhh cancel the latte sorry\")\n",
        "* Go off-menu (\"a babycino\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qRJAjVlb_nA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\"\"\"\n",
        "## See also\n",
        "\n",
        "This sample app showed you how to integrate a traditional software system (the coffee ordering functions) and an AI agent powered by the Gemini API. This is a simple, practical way to use LLMs that allows for open-ended human language input and output that feels natural, but still keeps a human in the loop to ensure correct operation.\n",
        "\n",
        "To learn more about how Barista Bot works, check out:\n",
        "\n",
        "* The [Barista Bot](https://aistudio.google.com/app/prompts/barista-bot) prompt\n",
        "* [System instructions](../quickstarts/System_instructions.ipynb)\n",
        "* [Automatic function calling](../quickstarts/Function_calling.ipynb)\n",
        "\"\"\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QhvFw7zyAIea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\"\"\"\n",
        "## See also\n",
        "\n",
        "This sample app showed you how to integrate a traditional software system (the coffee ordering functions) and an AI agent powered by the Gemini API. This is a simple, practical way to use LLMs that allows for open-ended human language input and output that feels natural, but still keeps a human in the loop to ensure correct operation.\n",
        "\n",
        "To learn more about how Barista Bot works, check out:\n",
        "\n",
        "* The [Barista Bot](https://aistudio.google.com/app/prompts/barista-bot) prompt\n",
        "* [System instructions](../quickstarts/System_instructions.ipynb)\n",
        "* [Automatic function calling](../quickstarts/Function_calling.ipynb)\n",
        "\"\"\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "oxxRWZWZAWi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\"\"\"\n",
        "Some things to try:\n",
        "* \"What's the weather like in London today?\"\n",
        "* \"Write a short poem about the beauty of nature.\"\n",
        "* \"What are the top 5 tourist attractions in Paris?\"\n",
        "* \"Summarize the latest news on artificial intelligence.\"\n",
        "\"\"\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nfPGiyhyAkHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W438QHRGbLcB"
      },
      "source": [
        "## See also\n",
        "\n",
        "This sample app showed you how to integrate a traditional software system (the coffee ordering functions) and an AI agent powered by the Gemini API. This is a simple, practical way to use LLMs that allows for open-ended human language input and output that feels natural, but still keeps a human in the loop to ensure correct operation.\n",
        "\n",
        "To learn more about how Barista Bot works, check out:\n",
        "\n",
        "* The [Barista Bot](https://aistudio.google.com/app/prompts/barista-bot) prompt\n",
        "* [System instructions](../quickstarts/System_instructions.ipynb)\n",
        "* [Automatic function calling](../quickstarts/Function_calling.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nuova sezione"
      ],
      "metadata": {
        "id": "nVcg_cw1C6Me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nuova sezione"
      ],
      "metadata": {
        "id": "GSLYKgAkOObj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nuova sezione"
      ],
      "metadata": {
        "id": "Ic48GBsCOPBD"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Agents_Function_Calling_Barista_Bot.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}